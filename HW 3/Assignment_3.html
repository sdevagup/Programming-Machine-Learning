<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html><head></head><body>





















    
    
    
    

  <div class="border-box-sizing">
    <div class="container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Linear Regression [Dataset: communities.zip]<a rel="noopener" class="anchor-link" href="#Linear-Regression-[Dataset:-communities.zip]">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this problem you will experiment with multiple linear regression models to make predictions with numerical data. You will also explore more systematic methods for feature selection and for optimizing model parameters (model selection). The data set you will use is a subset of the &quot;Communities and Crime&quot; data set that combines information from the 1990 census data as well as FBI crime data from 1995. Please read the full description of the data, including the description and statistics on different variables. The target attribute for regression purposes is &quot;ViolentCrimesPerPop&quot;. The two identifier attributes &quot;state&quot; and &quot;community name&quot; should be excluded for the regression task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[68]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[69]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./communities.csv&quot;</span><span class="p">,</span> <span class="n">na_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;?&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[69]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>state</th>
      <th>communityname</th>
      <th>population</th>
      <th>householdsize</th>
      <th>racepctblack</th>
      <th>racePctWhite</th>
      <th>racePctAsian</th>
      <th>racePctHisp</th>
      <th>agePct12t21</th>
      <th>agePct12t29</th>
      <th>agePct16t24</th>
      <th>agePct65up</th>
      <th>numbUrban</th>
      <th>pctUrban</th>
      <th>medIncome</th>
      <th>pctWWage</th>
      <th>pctWFarmSelf</th>
      <th>pctWInvInc</th>
      <th>pctWSocSec</th>
      <th>pctWPubAsst</th>
      <th>pctWRetire</th>
      <th>medFamInc</th>
      <th>perCapInc</th>
      <th>whitePerCap</th>
      <th>blackPerCap</th>
      <th>indianPerCap</th>
      <th>AsianPerCap</th>
      <th>OtherPerCap</th>
      <th>HispPerCap</th>
      <th>NumUnderPov</th>
      <th>PctPopUnderPov</th>
      <th>PctLess9thGrade</th>
      <th>PctNotHSGrad</th>
      <th>PctBSorMore</th>
      <th>PctUnemployed</th>
      <th>PctEmploy</th>
      <th>PctEmplManu</th>
      <th>PctEmplProfServ</th>
      <th>MalePctDivorce</th>
      <th>MalePctNevMarr</th>
      <th>FemalePctDiv</th>
      <th>TotalPctDiv</th>
      <th>PersPerFam</th>
      <th>PctFam2Par</th>
      <th>PctKids2Par</th>
      <th>PctYoungKids2Par</th>
      <th>PctTeen2Par</th>
      <th>PctWorkMomYoungKids</th>
      <th>PctWorkMom</th>
      <th>NumIlleg</th>
      <th>PctIlleg</th>
      <th>NumImmig</th>
      <th>PctImmigRecent</th>
      <th>PctImmigRec5</th>
      <th>PctImmigRec8</th>
      <th>PctImmigRec10</th>
      <th>PctRecentImmig</th>
      <th>PctRecImmig5</th>
      <th>PctRecImmig8</th>
      <th>PctRecImmig10</th>
      <th>PctSpeakEnglOnly</th>
      <th>PctNotSpeakEnglWell</th>
      <th>PctLargHouseFam</th>
      <th>PctLargHouseOccup</th>
      <th>PersPerOccupHous</th>
      <th>PersPerOwnOccHous</th>
      <th>PersPerRentOccHous</th>
      <th>PctPersOwnOccup</th>
      <th>PctPersDenseHous</th>
      <th>PctHousLess3BR</th>
      <th>MedNumBR</th>
      <th>HousVacant</th>
      <th>PctHousOccup</th>
      <th>PctHousOwnOcc</th>
      <th>PctVacantBoarded</th>
      <th>PctVacMore6Mos</th>
      <th>MedYrHousBuilt</th>
      <th>PctHousNoPhone</th>
      <th>PctWOFullPlumb</th>
      <th>OwnOccLowQuart</th>
      <th>OwnOccMedVal</th>
      <th>OwnOccHiQuart</th>
      <th>RentLowQ</th>
      <th>RentMedian</th>
      <th>RentHighQ</th>
      <th>MedRent</th>
      <th>MedRentPctHousInc</th>
      <th>MedOwnCostPctInc</th>
      <th>MedOwnCostPctIncNoMtg</th>
      <th>NumInShelters</th>
      <th>NumStreet</th>
      <th>PctForeignBorn</th>
      <th>PctBornSameState</th>
      <th>PctSameHouse85</th>
      <th>PctSameCity85</th>
      <th>PctSameState85</th>
      <th>LandArea</th>
      <th>PopDens</th>
      <th>PctUsePubTrans</th>
      <th>ViolentCrimesPerPop</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8</td>
      <td>Lakewoodcity</td>
      <td>0.19</td>
      <td>0.33</td>
      <td>0.02</td>
      <td>0.90</td>
      <td>0.12</td>
      <td>0.17</td>
      <td>0.34</td>
      <td>0.47</td>
      <td>0.29</td>
      <td>0.32</td>
      <td>0.20</td>
      <td>1.0</td>
      <td>0.37</td>
      <td>0.72</td>
      <td>0.34</td>
      <td>0.60</td>
      <td>0.29</td>
      <td>0.15</td>
      <td>0.43</td>
      <td>0.39</td>
      <td>0.40</td>
      <td>0.39</td>
      <td>0.32</td>
      <td>0.27</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>0.41</td>
      <td>0.08</td>
      <td>0.19</td>
      <td>0.10</td>
      <td>0.18</td>
      <td>0.48</td>
      <td>0.27</td>
      <td>0.68</td>
      <td>0.23</td>
      <td>0.41</td>
      <td>0.68</td>
      <td>0.40</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.35</td>
      <td>0.55</td>
      <td>0.59</td>
      <td>0.61</td>
      <td>0.56</td>
      <td>0.74</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>0.14</td>
      <td>0.03</td>
      <td>0.24</td>
      <td>0.27</td>
      <td>0.37</td>
      <td>0.39</td>
      <td>0.07</td>
      <td>0.07</td>
      <td>0.08</td>
      <td>0.08</td>
      <td>0.89</td>
      <td>0.06</td>
      <td>0.14</td>
      <td>0.13</td>
      <td>0.33</td>
      <td>0.39</td>
      <td>0.28</td>
      <td>0.55</td>
      <td>0.09</td>
      <td>0.51</td>
      <td>0.5</td>
      <td>0.21</td>
      <td>0.71</td>
      <td>0.52</td>
      <td>0.05</td>
      <td>0.26</td>
      <td>0.65</td>
      <td>0.14</td>
      <td>0.06</td>
      <td>0.22</td>
      <td>0.19</td>
      <td>0.18</td>
      <td>0.36</td>
      <td>0.35</td>
      <td>0.38</td>
      <td>0.34</td>
      <td>0.38</td>
      <td>0.46</td>
      <td>0.25</td>
      <td>0.04</td>
      <td>0.0</td>
      <td>0.12</td>
      <td>0.42</td>
      <td>0.50</td>
      <td>0.51</td>
      <td>0.64</td>
      <td>0.12</td>
      <td>0.26</td>
      <td>0.20</td>
      <td>0.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>53</td>
      <td>Tukwilacity</td>
      <td>0.00</td>
      <td>0.16</td>
      <td>0.12</td>
      <td>0.74</td>
      <td>0.45</td>
      <td>0.07</td>
      <td>0.26</td>
      <td>0.59</td>
      <td>0.35</td>
      <td>0.27</td>
      <td>0.02</td>
      <td>1.0</td>
      <td>0.31</td>
      <td>0.72</td>
      <td>0.11</td>
      <td>0.45</td>
      <td>0.25</td>
      <td>0.29</td>
      <td>0.39</td>
      <td>0.29</td>
      <td>0.37</td>
      <td>0.38</td>
      <td>0.33</td>
      <td>0.16</td>
      <td>0.30</td>
      <td>0.22</td>
      <td>0.35</td>
      <td>0.01</td>
      <td>0.24</td>
      <td>0.14</td>
      <td>0.24</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>0.73</td>
      <td>0.57</td>
      <td>0.15</td>
      <td>1.00</td>
      <td>0.63</td>
      <td>0.91</td>
      <td>1.00</td>
      <td>0.29</td>
      <td>0.43</td>
      <td>0.47</td>
      <td>0.60</td>
      <td>0.39</td>
      <td>0.46</td>
      <td>0.53</td>
      <td>0.00</td>
      <td>0.24</td>
      <td>0.01</td>
      <td>0.52</td>
      <td>0.62</td>
      <td>0.64</td>
      <td>0.63</td>
      <td>0.25</td>
      <td>0.27</td>
      <td>0.25</td>
      <td>0.23</td>
      <td>0.84</td>
      <td>0.10</td>
      <td>0.16</td>
      <td>0.10</td>
      <td>0.17</td>
      <td>0.29</td>
      <td>0.17</td>
      <td>0.26</td>
      <td>0.20</td>
      <td>0.82</td>
      <td>0.0</td>
      <td>0.02</td>
      <td>0.79</td>
      <td>0.24</td>
      <td>0.02</td>
      <td>0.25</td>
      <td>0.65</td>
      <td>0.16</td>
      <td>0.00</td>
      <td>0.21</td>
      <td>0.20</td>
      <td>0.21</td>
      <td>0.42</td>
      <td>0.38</td>
      <td>0.40</td>
      <td>0.37</td>
      <td>0.29</td>
      <td>0.32</td>
      <td>0.18</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.21</td>
      <td>0.50</td>
      <td>0.34</td>
      <td>0.60</td>
      <td>0.52</td>
      <td>0.02</td>
      <td>0.12</td>
      <td>0.45</td>
      <td>0.67</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24</td>
      <td>Aberdeentown</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.49</td>
      <td>0.56</td>
      <td>0.17</td>
      <td>0.04</td>
      <td>0.39</td>
      <td>0.47</td>
      <td>0.28</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.30</td>
      <td>0.58</td>
      <td>0.19</td>
      <td>0.39</td>
      <td>0.38</td>
      <td>0.40</td>
      <td>0.84</td>
      <td>0.28</td>
      <td>0.27</td>
      <td>0.29</td>
      <td>0.27</td>
      <td>0.07</td>
      <td>0.29</td>
      <td>0.28</td>
      <td>0.39</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.27</td>
      <td>0.43</td>
      <td>0.19</td>
      <td>0.36</td>
      <td>0.58</td>
      <td>0.32</td>
      <td>0.29</td>
      <td>0.63</td>
      <td>0.41</td>
      <td>0.71</td>
      <td>0.70</td>
      <td>0.45</td>
      <td>0.42</td>
      <td>0.44</td>
      <td>0.43</td>
      <td>0.43</td>
      <td>0.71</td>
      <td>0.67</td>
      <td>0.01</td>
      <td>0.46</td>
      <td>0.00</td>
      <td>0.07</td>
      <td>0.06</td>
      <td>0.15</td>
      <td>0.19</td>
      <td>0.02</td>
      <td>0.02</td>
      <td>0.04</td>
      <td>0.05</td>
      <td>0.88</td>
      <td>0.04</td>
      <td>0.20</td>
      <td>0.20</td>
      <td>0.46</td>
      <td>0.52</td>
      <td>0.43</td>
      <td>0.42</td>
      <td>0.15</td>
      <td>0.51</td>
      <td>0.5</td>
      <td>0.01</td>
      <td>0.86</td>
      <td>0.41</td>
      <td>0.29</td>
      <td>0.30</td>
      <td>0.52</td>
      <td>0.47</td>
      <td>0.45</td>
      <td>0.18</td>
      <td>0.17</td>
      <td>0.16</td>
      <td>0.27</td>
      <td>0.29</td>
      <td>0.27</td>
      <td>0.31</td>
      <td>0.48</td>
      <td>0.39</td>
      <td>0.28</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.14</td>
      <td>0.49</td>
      <td>0.54</td>
      <td>0.67</td>
      <td>0.56</td>
      <td>0.01</td>
      <td>0.21</td>
      <td>0.02</td>
      <td>0.43</td>
    </tr>
    <tr>
      <th>3</th>
      <td>34</td>
      <td>Willingborotownship</td>
      <td>0.04</td>
      <td>0.77</td>
      <td>1.00</td>
      <td>0.08</td>
      <td>0.12</td>
      <td>0.10</td>
      <td>0.51</td>
      <td>0.50</td>
      <td>0.34</td>
      <td>0.21</td>
      <td>0.06</td>
      <td>1.0</td>
      <td>0.58</td>
      <td>0.89</td>
      <td>0.21</td>
      <td>0.43</td>
      <td>0.36</td>
      <td>0.20</td>
      <td>0.82</td>
      <td>0.51</td>
      <td>0.36</td>
      <td>0.40</td>
      <td>0.39</td>
      <td>0.16</td>
      <td>0.25</td>
      <td>0.36</td>
      <td>0.44</td>
      <td>0.01</td>
      <td>0.10</td>
      <td>0.09</td>
      <td>0.25</td>
      <td>0.31</td>
      <td>0.33</td>
      <td>0.71</td>
      <td>0.36</td>
      <td>0.45</td>
      <td>0.34</td>
      <td>0.45</td>
      <td>0.49</td>
      <td>0.44</td>
      <td>0.75</td>
      <td>0.65</td>
      <td>0.54</td>
      <td>0.83</td>
      <td>0.65</td>
      <td>0.85</td>
      <td>0.86</td>
      <td>0.03</td>
      <td>0.33</td>
      <td>0.02</td>
      <td>0.11</td>
      <td>0.20</td>
      <td>0.30</td>
      <td>0.31</td>
      <td>0.05</td>
      <td>0.08</td>
      <td>0.11</td>
      <td>0.11</td>
      <td>0.81</td>
      <td>0.08</td>
      <td>0.56</td>
      <td>0.62</td>
      <td>0.85</td>
      <td>0.77</td>
      <td>1.00</td>
      <td>0.94</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.5</td>
      <td>0.01</td>
      <td>0.97</td>
      <td>0.96</td>
      <td>0.60</td>
      <td>0.47</td>
      <td>0.52</td>
      <td>0.11</td>
      <td>0.11</td>
      <td>0.24</td>
      <td>0.21</td>
      <td>0.19</td>
      <td>0.75</td>
      <td>0.70</td>
      <td>0.77</td>
      <td>0.89</td>
      <td>0.63</td>
      <td>0.51</td>
      <td>0.47</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.19</td>
      <td>0.30</td>
      <td>0.73</td>
      <td>0.64</td>
      <td>0.65</td>
      <td>0.02</td>
      <td>0.39</td>
      <td>0.28</td>
      <td>0.12</td>
    </tr>
    <tr>
      <th>4</th>
      <td>42</td>
      <td>Bethlehemtownship</td>
      <td>0.01</td>
      <td>0.55</td>
      <td>0.02</td>
      <td>0.95</td>
      <td>0.09</td>
      <td>0.05</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.23</td>
      <td>0.36</td>
      <td>0.02</td>
      <td>0.9</td>
      <td>0.50</td>
      <td>0.72</td>
      <td>0.16</td>
      <td>0.68</td>
      <td>0.44</td>
      <td>0.11</td>
      <td>0.71</td>
      <td>0.46</td>
      <td>0.43</td>
      <td>0.41</td>
      <td>0.28</td>
      <td>0.00</td>
      <td>0.74</td>
      <td>0.51</td>
      <td>0.48</td>
      <td>0.00</td>
      <td>0.06</td>
      <td>0.25</td>
      <td>0.30</td>
      <td>0.33</td>
      <td>0.12</td>
      <td>0.65</td>
      <td>0.67</td>
      <td>0.38</td>
      <td>0.22</td>
      <td>0.27</td>
      <td>0.20</td>
      <td>0.21</td>
      <td>0.51</td>
      <td>0.91</td>
      <td>0.91</td>
      <td>0.89</td>
      <td>0.85</td>
      <td>0.40</td>
      <td>0.60</td>
      <td>0.00</td>
      <td>0.06</td>
      <td>0.00</td>
      <td>0.03</td>
      <td>0.07</td>
      <td>0.20</td>
      <td>0.27</td>
      <td>0.01</td>
      <td>0.02</td>
      <td>0.04</td>
      <td>0.05</td>
      <td>0.88</td>
      <td>0.05</td>
      <td>0.16</td>
      <td>0.19</td>
      <td>0.59</td>
      <td>0.60</td>
      <td>0.37</td>
      <td>0.89</td>
      <td>0.02</td>
      <td>0.19</td>
      <td>0.5</td>
      <td>0.01</td>
      <td>0.89</td>
      <td>0.87</td>
      <td>0.04</td>
      <td>0.55</td>
      <td>0.73</td>
      <td>0.05</td>
      <td>0.14</td>
      <td>0.31</td>
      <td>0.31</td>
      <td>0.30</td>
      <td>0.40</td>
      <td>0.36</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.22</td>
      <td>0.51</td>
      <td>0.21</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.11</td>
      <td>0.72</td>
      <td>0.64</td>
      <td>0.61</td>
      <td>0.53</td>
      <td>0.04</td>
      <td>0.09</td>
      <td>0.02</td>
      <td>0.03</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>a) Load and preprocess the data using Pandas or Numpy and, if necessary, preprocessing functions from scikit-learn. The provided data is already normalized (see description), so there is no need for additional normalization. Compute and display basic statistics (mean, standard deviation, min, max, etc.) for each of the variables in the data set. Separate the target attribute for regression. Create a 20%-80% randomized split of the data. Set aside the 20% test portion; the 80% training data partition will be used for cross-validation on various tasks specified below.<a rel="noopener" class="anchor-link" href="#a)-Load-and-preprocess-the-data-using-Pandas-or-Numpy-and,-if-necessary,-preprocessing-functions-from-scikit-learn.-The-provided-data-is-already-normalized-(see-description),-so-there-is-no-need-for-additional-normalization.-Compute-and-display-basic-statistics-(mean,-standard-deviation,-min,-max,-etc.)-for-each-of-the-variables-in-the-data-set.-Separate-the-target-attribute-for-regression.-Create-a-20%-80%-randomized-split-of-the-data.-Set-aside-the-20%-test-portion;-the-80%-training-data-partition-will-be-used-for-cross-validation-on-various-tasks-specified-below.">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[70]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[70]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(1994, 100)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[71]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1994 entries, 0 to 1993
Data columns (total 100 columns):
 #   Column                 Non-Null Count  Dtype  
---  ------                 --------------  -----  
 0   state                  1994 non-null   int64  
 1   communityname          1994 non-null   object 
 2   population             1994 non-null   float64
 3   householdsize          1994 non-null   float64
 4   racepctblack           1994 non-null   float64
 5   racePctWhite           1994 non-null   float64
 6   racePctAsian           1994 non-null   float64
 7   racePctHisp            1994 non-null   float64
 8   agePct12t21            1994 non-null   float64
 9   agePct12t29            1994 non-null   float64
 10  agePct16t24            1994 non-null   float64
 11  agePct65up             1994 non-null   float64
 12  numbUrban              1994 non-null   float64
 13  pctUrban               1994 non-null   float64
 14  medIncome              1994 non-null   float64
 15  pctWWage               1994 non-null   float64
 16  pctWFarmSelf           1994 non-null   float64
 17  pctWInvInc             1994 non-null   float64
 18  pctWSocSec             1994 non-null   float64
 19  pctWPubAsst            1994 non-null   float64
 20  pctWRetire             1994 non-null   float64
 21  medFamInc              1994 non-null   float64
 22  perCapInc              1994 non-null   float64
 23  whitePerCap            1994 non-null   float64
 24  blackPerCap            1994 non-null   float64
 25  indianPerCap           1994 non-null   float64
 26  AsianPerCap            1994 non-null   float64
 27  OtherPerCap            1993 non-null   float64
 28  HispPerCap             1994 non-null   float64
 29  NumUnderPov            1994 non-null   float64
 30  PctPopUnderPov         1994 non-null   float64
 31  PctLess9thGrade        1994 non-null   float64
 32  PctNotHSGrad           1994 non-null   float64
 33  PctBSorMore            1994 non-null   float64
 34  PctUnemployed          1994 non-null   float64
 35  PctEmploy              1994 non-null   float64
 36  PctEmplManu            1994 non-null   float64
 37  PctEmplProfServ        1994 non-null   float64
 38  MalePctDivorce         1994 non-null   float64
 39  MalePctNevMarr         1994 non-null   float64
 40  FemalePctDiv           1994 non-null   float64
 41  TotalPctDiv            1994 non-null   float64
 42  PersPerFam             1994 non-null   float64
 43  PctFam2Par             1994 non-null   float64
 44  PctKids2Par            1994 non-null   float64
 45  PctYoungKids2Par       1994 non-null   float64
 46  PctTeen2Par            1994 non-null   float64
 47  PctWorkMomYoungKids    1994 non-null   float64
 48  PctWorkMom             1994 non-null   float64
 49  NumIlleg               1994 non-null   float64
 50  PctIlleg               1994 non-null   float64
 51  NumImmig               1994 non-null   float64
 52  PctImmigRecent         1994 non-null   float64
 53  PctImmigRec5           1994 non-null   float64
 54  PctImmigRec8           1994 non-null   float64
 55  PctImmigRec10          1994 non-null   float64
 56  PctRecentImmig         1994 non-null   float64
 57  PctRecImmig5           1994 non-null   float64
 58  PctRecImmig8           1994 non-null   float64
 59  PctRecImmig10          1994 non-null   float64
 60  PctSpeakEnglOnly       1994 non-null   float64
 61  PctNotSpeakEnglWell    1994 non-null   float64
 62  PctLargHouseFam        1994 non-null   float64
 63  PctLargHouseOccup      1994 non-null   float64
 64  PersPerOccupHous       1994 non-null   float64
 65  PersPerOwnOccHous      1994 non-null   float64
 66  PersPerRentOccHous     1994 non-null   float64
 67  PctPersOwnOccup        1994 non-null   float64
 68  PctPersDenseHous       1994 non-null   float64
 69  PctHousLess3BR         1994 non-null   float64
 70  MedNumBR               1994 non-null   float64
 71  HousVacant             1994 non-null   float64
 72  PctHousOccup           1994 non-null   float64
 73  PctHousOwnOcc          1994 non-null   float64
 74  PctVacantBoarded       1994 non-null   float64
 75  PctVacMore6Mos         1994 non-null   float64
 76  MedYrHousBuilt         1994 non-null   float64
 77  PctHousNoPhone         1994 non-null   float64
 78  PctWOFullPlumb         1994 non-null   float64
 79  OwnOccLowQuart         1994 non-null   float64
 80  OwnOccMedVal           1994 non-null   float64
 81  OwnOccHiQuart          1994 non-null   float64
 82  RentLowQ               1994 non-null   float64
 83  RentMedian             1994 non-null   float64
 84  RentHighQ              1994 non-null   float64
 85  MedRent                1994 non-null   float64
 86  MedRentPctHousInc      1994 non-null   float64
 87  MedOwnCostPctInc       1994 non-null   float64
 88  MedOwnCostPctIncNoMtg  1994 non-null   float64
 89  NumInShelters          1994 non-null   float64
 90  NumStreet              1994 non-null   float64
 91  PctForeignBorn         1994 non-null   float64
 92  PctBornSameState       1994 non-null   float64
 93  PctSameHouse85         1994 non-null   float64
 94  PctSameCity85          1994 non-null   float64
 95  PctSameState85         1994 non-null   float64
 96  LandArea               1994 non-null   float64
 97  PopDens                1994 non-null   float64
 98  PctUsePubTrans         1994 non-null   float64
 99  ViolentCrimesPerPop    1994 non-null   float64
dtypes: float64(98), int64(1), object(1)
memory usage: 1.5+ MB
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[72]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;OtherPerCap&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">OtherPerCap</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[73]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Compute and display basic statistics (mean, standard deviation, min, max, etc.) </span>
<span class="c1">#for each of the variables in the data set.</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_columns&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[73]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>state</th>
      <th>population</th>
      <th>householdsize</th>
      <th>racepctblack</th>
      <th>racePctWhite</th>
      <th>racePctAsian</th>
      <th>racePctHisp</th>
      <th>agePct12t21</th>
      <th>agePct12t29</th>
      <th>agePct16t24</th>
      <th>agePct65up</th>
      <th>numbUrban</th>
      <th>pctUrban</th>
      <th>medIncome</th>
      <th>pctWWage</th>
      <th>pctWFarmSelf</th>
      <th>pctWInvInc</th>
      <th>pctWSocSec</th>
      <th>pctWPubAsst</th>
      <th>pctWRetire</th>
      <th>medFamInc</th>
      <th>perCapInc</th>
      <th>whitePerCap</th>
      <th>blackPerCap</th>
      <th>indianPerCap</th>
      <th>AsianPerCap</th>
      <th>OtherPerCap</th>
      <th>HispPerCap</th>
      <th>NumUnderPov</th>
      <th>PctPopUnderPov</th>
      <th>PctLess9thGrade</th>
      <th>PctNotHSGrad</th>
      <th>PctBSorMore</th>
      <th>PctUnemployed</th>
      <th>PctEmploy</th>
      <th>PctEmplManu</th>
      <th>PctEmplProfServ</th>
      <th>MalePctDivorce</th>
      <th>MalePctNevMarr</th>
      <th>FemalePctDiv</th>
      <th>TotalPctDiv</th>
      <th>PersPerFam</th>
      <th>PctFam2Par</th>
      <th>PctKids2Par</th>
      <th>PctYoungKids2Par</th>
      <th>PctTeen2Par</th>
      <th>PctWorkMomYoungKids</th>
      <th>PctWorkMom</th>
      <th>NumIlleg</th>
      <th>PctIlleg</th>
      <th>NumImmig</th>
      <th>PctImmigRecent</th>
      <th>PctImmigRec5</th>
      <th>PctImmigRec8</th>
      <th>PctImmigRec10</th>
      <th>PctRecentImmig</th>
      <th>PctRecImmig5</th>
      <th>PctRecImmig8</th>
      <th>PctRecImmig10</th>
      <th>PctSpeakEnglOnly</th>
      <th>PctNotSpeakEnglWell</th>
      <th>PctLargHouseFam</th>
      <th>PctLargHouseOccup</th>
      <th>PersPerOccupHous</th>
      <th>PersPerOwnOccHous</th>
      <th>PersPerRentOccHous</th>
      <th>PctPersOwnOccup</th>
      <th>PctPersDenseHous</th>
      <th>PctHousLess3BR</th>
      <th>MedNumBR</th>
      <th>HousVacant</th>
      <th>PctHousOccup</th>
      <th>PctHousOwnOcc</th>
      <th>PctVacantBoarded</th>
      <th>PctVacMore6Mos</th>
      <th>MedYrHousBuilt</th>
      <th>PctHousNoPhone</th>
      <th>PctWOFullPlumb</th>
      <th>OwnOccLowQuart</th>
      <th>OwnOccMedVal</th>
      <th>OwnOccHiQuart</th>
      <th>RentLowQ</th>
      <th>RentMedian</th>
      <th>RentHighQ</th>
      <th>MedRent</th>
      <th>MedRentPctHousInc</th>
      <th>MedOwnCostPctInc</th>
      <th>MedOwnCostPctIncNoMtg</th>
      <th>NumInShelters</th>
      <th>NumStreet</th>
      <th>PctForeignBorn</th>
      <th>PctBornSameState</th>
      <th>PctSameHouse85</th>
      <th>PctSameCity85</th>
      <th>PctSameState85</th>
      <th>LandArea</th>
      <th>PopDens</th>
      <th>PctUsePubTrans</th>
      <th>ViolentCrimesPerPop</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
      <td>1993.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>28.683894</td>
      <td>0.057612</td>
      <td>0.463437</td>
      <td>0.179227</td>
      <td>0.753984</td>
      <td>0.153753</td>
      <td>0.144089</td>
      <td>0.424210</td>
      <td>0.493914</td>
      <td>0.336297</td>
      <td>0.423086</td>
      <td>0.064104</td>
      <td>0.696618</td>
      <td>0.361259</td>
      <td>0.558314</td>
      <td>0.291540</td>
      <td>0.495780</td>
      <td>0.471044</td>
      <td>0.317546</td>
      <td>0.479242</td>
      <td>0.375805</td>
      <td>0.350336</td>
      <td>0.368073</td>
      <td>0.291169</td>
      <td>0.203567</td>
      <td>0.322333</td>
      <td>0.284742</td>
      <td>0.386157</td>
      <td>0.055509</td>
      <td>0.302750</td>
      <td>0.315695</td>
      <td>0.383251</td>
      <td>0.361711</td>
      <td>0.363281</td>
      <td>0.501229</td>
      <td>0.396427</td>
      <td>0.440552</td>
      <td>0.461164</td>
      <td>0.434451</td>
      <td>0.487501</td>
      <td>0.494195</td>
      <td>0.487747</td>
      <td>0.611124</td>
      <td>0.620868</td>
      <td>0.664300</td>
      <td>0.583081</td>
      <td>0.501405</td>
      <td>0.526703</td>
      <td>0.036292</td>
      <td>0.249694</td>
      <td>0.030075</td>
      <td>0.320261</td>
      <td>0.360723</td>
      <td>0.399212</td>
      <td>0.428038</td>
      <td>0.181450</td>
      <td>0.182218</td>
      <td>0.184867</td>
      <td>0.182970</td>
      <td>0.785805</td>
      <td>0.150652</td>
      <td>0.267602</td>
      <td>0.251897</td>
      <td>0.462132</td>
      <td>0.494496</td>
      <td>0.404064</td>
      <td>0.562619</td>
      <td>0.186272</td>
      <td>0.495203</td>
      <td>0.314601</td>
      <td>0.076829</td>
      <td>0.719649</td>
      <td>0.548685</td>
      <td>0.204476</td>
      <td>0.433211</td>
      <td>0.494235</td>
      <td>0.264355</td>
      <td>0.242905</td>
      <td>0.264792</td>
      <td>0.263593</td>
      <td>0.269037</td>
      <td>0.346553</td>
      <td>0.372614</td>
      <td>0.423121</td>
      <td>0.384240</td>
      <td>0.490070</td>
      <td>0.449759</td>
      <td>0.403638</td>
      <td>0.029453</td>
      <td>0.022790</td>
      <td>0.215655</td>
      <td>0.608776</td>
      <td>0.534967</td>
      <td>0.626322</td>
      <td>0.651470</td>
      <td>0.065243</td>
      <td>0.232910</td>
      <td>0.161741</td>
      <td>0.237983</td>
    </tr>
    <tr>
      <th>std</th>
      <td>16.401661</td>
      <td>0.126935</td>
      <td>0.163747</td>
      <td>0.252870</td>
      <td>0.243807</td>
      <td>0.208905</td>
      <td>0.232531</td>
      <td>0.155234</td>
      <td>0.143584</td>
      <td>0.166540</td>
      <td>0.179196</td>
      <td>0.128280</td>
      <td>0.444648</td>
      <td>0.209327</td>
      <td>0.182820</td>
      <td>0.204155</td>
      <td>0.178067</td>
      <td>0.173616</td>
      <td>0.221951</td>
      <td>0.167606</td>
      <td>0.198224</td>
      <td>0.191118</td>
      <td>0.186848</td>
      <td>0.171607</td>
      <td>0.164793</td>
      <td>0.195457</td>
      <td>0.191008</td>
      <td>0.183045</td>
      <td>0.127973</td>
      <td>0.228202</td>
      <td>0.213354</td>
      <td>0.202528</td>
      <td>0.209239</td>
      <td>0.201916</td>
      <td>0.173940</td>
      <td>0.202427</td>
      <td>0.175490</td>
      <td>0.182471</td>
      <td>0.175481</td>
      <td>0.175189</td>
      <td>0.183620</td>
      <td>0.154633</td>
      <td>0.201817</td>
      <td>0.206190</td>
      <td>0.218477</td>
      <td>0.191353</td>
      <td>0.168642</td>
      <td>0.175284</td>
      <td>0.108698</td>
      <td>0.229610</td>
      <td>0.087209</td>
      <td>0.219132</td>
      <td>0.210929</td>
      <td>0.201458</td>
      <td>0.194889</td>
      <td>0.235819</td>
      <td>0.236357</td>
      <td>0.236762</td>
      <td>0.234846</td>
      <td>0.226884</td>
      <td>0.219752</td>
      <td>0.196616</td>
      <td>0.190756</td>
      <td>0.169588</td>
      <td>0.157935</td>
      <td>0.189343</td>
      <td>0.197134</td>
      <td>0.210009</td>
      <td>0.172550</td>
      <td>0.255212</td>
      <td>0.150501</td>
      <td>0.194021</td>
      <td>0.185251</td>
      <td>0.217812</td>
      <td>0.188952</td>
      <td>0.232511</td>
      <td>0.242846</td>
      <td>0.206232</td>
      <td>0.224434</td>
      <td>0.231555</td>
      <td>0.235273</td>
      <td>0.219240</td>
      <td>0.209213</td>
      <td>0.248249</td>
      <td>0.213369</td>
      <td>0.169524</td>
      <td>0.187321</td>
      <td>0.192476</td>
      <td>0.102630</td>
      <td>0.100424</td>
      <td>0.231146</td>
      <td>0.204314</td>
      <td>0.181360</td>
      <td>0.200520</td>
      <td>0.198253</td>
      <td>0.109485</td>
      <td>0.203127</td>
      <td>0.229099</td>
      <td>0.233043</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>12.000000</td>
      <td>0.010000</td>
      <td>0.350000</td>
      <td>0.020000</td>
      <td>0.630000</td>
      <td>0.040000</td>
      <td>0.010000</td>
      <td>0.340000</td>
      <td>0.410000</td>
      <td>0.250000</td>
      <td>0.300000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.200000</td>
      <td>0.440000</td>
      <td>0.160000</td>
      <td>0.370000</td>
      <td>0.350000</td>
      <td>0.140000</td>
      <td>0.360000</td>
      <td>0.230000</td>
      <td>0.220000</td>
      <td>0.240000</td>
      <td>0.180000</td>
      <td>0.110000</td>
      <td>0.190000</td>
      <td>0.170000</td>
      <td>0.260000</td>
      <td>0.010000</td>
      <td>0.110000</td>
      <td>0.160000</td>
      <td>0.230000</td>
      <td>0.210000</td>
      <td>0.220000</td>
      <td>0.380000</td>
      <td>0.250000</td>
      <td>0.320000</td>
      <td>0.330000</td>
      <td>0.310000</td>
      <td>0.360000</td>
      <td>0.360000</td>
      <td>0.400000</td>
      <td>0.490000</td>
      <td>0.490000</td>
      <td>0.530000</td>
      <td>0.480000</td>
      <td>0.390000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.090000</td>
      <td>0.000000</td>
      <td>0.160000</td>
      <td>0.200000</td>
      <td>0.250000</td>
      <td>0.280000</td>
      <td>0.030000</td>
      <td>0.030000</td>
      <td>0.030000</td>
      <td>0.030000</td>
      <td>0.730000</td>
      <td>0.030000</td>
      <td>0.150000</td>
      <td>0.140000</td>
      <td>0.340000</td>
      <td>0.390000</td>
      <td>0.270000</td>
      <td>0.440000</td>
      <td>0.060000</td>
      <td>0.400000</td>
      <td>0.000000</td>
      <td>0.010000</td>
      <td>0.630000</td>
      <td>0.430000</td>
      <td>0.060000</td>
      <td>0.290000</td>
      <td>0.350000</td>
      <td>0.060000</td>
      <td>0.100000</td>
      <td>0.090000</td>
      <td>0.090000</td>
      <td>0.090000</td>
      <td>0.170000</td>
      <td>0.200000</td>
      <td>0.220000</td>
      <td>0.210000</td>
      <td>0.370000</td>
      <td>0.320000</td>
      <td>0.250000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.060000</td>
      <td>0.470000</td>
      <td>0.420000</td>
      <td>0.520000</td>
      <td>0.560000</td>
      <td>0.020000</td>
      <td>0.100000</td>
      <td>0.020000</td>
      <td>0.070000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.000000</td>
      <td>0.020000</td>
      <td>0.440000</td>
      <td>0.060000</td>
      <td>0.850000</td>
      <td>0.070000</td>
      <td>0.040000</td>
      <td>0.400000</td>
      <td>0.480000</td>
      <td>0.290000</td>
      <td>0.420000</td>
      <td>0.030000</td>
      <td>1.000000</td>
      <td>0.320000</td>
      <td>0.560000</td>
      <td>0.230000</td>
      <td>0.480000</td>
      <td>0.470000</td>
      <td>0.260000</td>
      <td>0.470000</td>
      <td>0.330000</td>
      <td>0.300000</td>
      <td>0.320000</td>
      <td>0.250000</td>
      <td>0.170000</td>
      <td>0.280000</td>
      <td>0.250000</td>
      <td>0.340000</td>
      <td>0.020000</td>
      <td>0.250000</td>
      <td>0.270000</td>
      <td>0.360000</td>
      <td>0.310000</td>
      <td>0.320000</td>
      <td>0.510000</td>
      <td>0.370000</td>
      <td>0.410000</td>
      <td>0.470000</td>
      <td>0.400000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.470000</td>
      <td>0.630000</td>
      <td>0.640000</td>
      <td>0.700000</td>
      <td>0.610000</td>
      <td>0.510000</td>
      <td>0.540000</td>
      <td>0.010000</td>
      <td>0.170000</td>
      <td>0.010000</td>
      <td>0.290000</td>
      <td>0.340000</td>
      <td>0.390000</td>
      <td>0.430000</td>
      <td>0.090000</td>
      <td>0.080000</td>
      <td>0.090000</td>
      <td>0.090000</td>
      <td>0.870000</td>
      <td>0.060000</td>
      <td>0.200000</td>
      <td>0.190000</td>
      <td>0.440000</td>
      <td>0.480000</td>
      <td>0.360000</td>
      <td>0.560000</td>
      <td>0.110000</td>
      <td>0.510000</td>
      <td>0.500000</td>
      <td>0.030000</td>
      <td>0.770000</td>
      <td>0.540000</td>
      <td>0.130000</td>
      <td>0.420000</td>
      <td>0.520000</td>
      <td>0.180000</td>
      <td>0.190000</td>
      <td>0.180000</td>
      <td>0.170000</td>
      <td>0.180000</td>
      <td>0.310000</td>
      <td>0.330000</td>
      <td>0.370000</td>
      <td>0.340000</td>
      <td>0.480000</td>
      <td>0.450000</td>
      <td>0.370000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.130000</td>
      <td>0.630000</td>
      <td>0.540000</td>
      <td>0.670000</td>
      <td>0.700000</td>
      <td>0.040000</td>
      <td>0.170000</td>
      <td>0.070000</td>
      <td>0.150000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>42.000000</td>
      <td>0.050000</td>
      <td>0.540000</td>
      <td>0.230000</td>
      <td>0.940000</td>
      <td>0.170000</td>
      <td>0.160000</td>
      <td>0.470000</td>
      <td>0.540000</td>
      <td>0.360000</td>
      <td>0.530000</td>
      <td>0.070000</td>
      <td>1.000000</td>
      <td>0.490000</td>
      <td>0.690000</td>
      <td>0.370000</td>
      <td>0.620000</td>
      <td>0.580000</td>
      <td>0.440000</td>
      <td>0.580000</td>
      <td>0.480000</td>
      <td>0.430000</td>
      <td>0.440000</td>
      <td>0.380000</td>
      <td>0.250000</td>
      <td>0.400000</td>
      <td>0.360000</td>
      <td>0.480000</td>
      <td>0.050000</td>
      <td>0.450000</td>
      <td>0.420000</td>
      <td>0.510000</td>
      <td>0.460000</td>
      <td>0.480000</td>
      <td>0.630000</td>
      <td>0.520000</td>
      <td>0.530000</td>
      <td>0.590000</td>
      <td>0.500000</td>
      <td>0.620000</td>
      <td>0.630000</td>
      <td>0.560000</td>
      <td>0.760000</td>
      <td>0.780000</td>
      <td>0.840000</td>
      <td>0.720000</td>
      <td>0.620000</td>
      <td>0.650000</td>
      <td>0.020000</td>
      <td>0.320000</td>
      <td>0.020000</td>
      <td>0.430000</td>
      <td>0.480000</td>
      <td>0.530000</td>
      <td>0.560000</td>
      <td>0.230000</td>
      <td>0.230000</td>
      <td>0.230000</td>
      <td>0.230000</td>
      <td>0.940000</td>
      <td>0.160000</td>
      <td>0.310000</td>
      <td>0.290000</td>
      <td>0.550000</td>
      <td>0.580000</td>
      <td>0.490000</td>
      <td>0.700000</td>
      <td>0.220000</td>
      <td>0.600000</td>
      <td>0.500000</td>
      <td>0.070000</td>
      <td>0.860000</td>
      <td>0.670000</td>
      <td>0.270000</td>
      <td>0.560000</td>
      <td>0.670000</td>
      <td>0.420000</td>
      <td>0.330000</td>
      <td>0.400000</td>
      <td>0.390000</td>
      <td>0.380000</td>
      <td>0.490000</td>
      <td>0.520000</td>
      <td>0.590000</td>
      <td>0.530000</td>
      <td>0.590000</td>
      <td>0.580000</td>
      <td>0.510000</td>
      <td>0.010000</td>
      <td>0.000000</td>
      <td>0.280000</td>
      <td>0.770000</td>
      <td>0.660000</td>
      <td>0.770000</td>
      <td>0.790000</td>
      <td>0.070000</td>
      <td>0.280000</td>
      <td>0.190000</td>
      <td>0.330000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>56.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[74]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[74]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>state                  0
communityname          0
population             0
householdsize          0
racepctblack           0
                      ..
PctSameState85         0
LandArea               0
PopDens                0
PctUsePubTrans         0
ViolentCrimesPerPop    0
Length: 100, dtype: int64</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[75]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;state&#39;</span><span class="p">,</span> <span class="s1">&#39;communityname&#39;</span><span class="p">,</span> <span class="s1">&#39;ViolentCrimesPerPop&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;ViolentCrimesPerPop&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[76]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[76]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(1993, 97)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[77]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[77]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>population</th>
      <th>householdsize</th>
      <th>racepctblack</th>
      <th>racePctWhite</th>
      <th>racePctAsian</th>
      <th>racePctHisp</th>
      <th>agePct12t21</th>
      <th>agePct12t29</th>
      <th>agePct16t24</th>
      <th>agePct65up</th>
      <th>numbUrban</th>
      <th>pctUrban</th>
      <th>medIncome</th>
      <th>pctWWage</th>
      <th>pctWFarmSelf</th>
      <th>pctWInvInc</th>
      <th>pctWSocSec</th>
      <th>pctWPubAsst</th>
      <th>pctWRetire</th>
      <th>medFamInc</th>
      <th>perCapInc</th>
      <th>whitePerCap</th>
      <th>blackPerCap</th>
      <th>indianPerCap</th>
      <th>AsianPerCap</th>
      <th>OtherPerCap</th>
      <th>HispPerCap</th>
      <th>NumUnderPov</th>
      <th>PctPopUnderPov</th>
      <th>PctLess9thGrade</th>
      <th>PctNotHSGrad</th>
      <th>PctBSorMore</th>
      <th>PctUnemployed</th>
      <th>PctEmploy</th>
      <th>PctEmplManu</th>
      <th>PctEmplProfServ</th>
      <th>MalePctDivorce</th>
      <th>MalePctNevMarr</th>
      <th>FemalePctDiv</th>
      <th>TotalPctDiv</th>
      <th>PersPerFam</th>
      <th>PctFam2Par</th>
      <th>PctKids2Par</th>
      <th>PctYoungKids2Par</th>
      <th>PctTeen2Par</th>
      <th>PctWorkMomYoungKids</th>
      <th>PctWorkMom</th>
      <th>NumIlleg</th>
      <th>PctIlleg</th>
      <th>NumImmig</th>
      <th>PctImmigRecent</th>
      <th>PctImmigRec5</th>
      <th>PctImmigRec8</th>
      <th>PctImmigRec10</th>
      <th>PctRecentImmig</th>
      <th>PctRecImmig5</th>
      <th>PctRecImmig8</th>
      <th>PctRecImmig10</th>
      <th>PctSpeakEnglOnly</th>
      <th>PctNotSpeakEnglWell</th>
      <th>PctLargHouseFam</th>
      <th>PctLargHouseOccup</th>
      <th>PersPerOccupHous</th>
      <th>PersPerOwnOccHous</th>
      <th>PersPerRentOccHous</th>
      <th>PctPersOwnOccup</th>
      <th>PctPersDenseHous</th>
      <th>PctHousLess3BR</th>
      <th>MedNumBR</th>
      <th>HousVacant</th>
      <th>PctHousOccup</th>
      <th>PctHousOwnOcc</th>
      <th>PctVacantBoarded</th>
      <th>PctVacMore6Mos</th>
      <th>MedYrHousBuilt</th>
      <th>PctHousNoPhone</th>
      <th>PctWOFullPlumb</th>
      <th>OwnOccLowQuart</th>
      <th>OwnOccMedVal</th>
      <th>OwnOccHiQuart</th>
      <th>RentLowQ</th>
      <th>RentMedian</th>
      <th>RentHighQ</th>
      <th>MedRent</th>
      <th>MedRentPctHousInc</th>
      <th>MedOwnCostPctInc</th>
      <th>MedOwnCostPctIncNoMtg</th>
      <th>NumInShelters</th>
      <th>NumStreet</th>
      <th>PctForeignBorn</th>
      <th>PctBornSameState</th>
      <th>PctSameHouse85</th>
      <th>PctSameCity85</th>
      <th>PctSameState85</th>
      <th>LandArea</th>
      <th>PopDens</th>
      <th>PctUsePubTrans</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.19</td>
      <td>0.33</td>
      <td>0.02</td>
      <td>0.90</td>
      <td>0.12</td>
      <td>0.17</td>
      <td>0.34</td>
      <td>0.47</td>
      <td>0.29</td>
      <td>0.32</td>
      <td>0.20</td>
      <td>1.0</td>
      <td>0.37</td>
      <td>0.72</td>
      <td>0.34</td>
      <td>0.60</td>
      <td>0.29</td>
      <td>0.15</td>
      <td>0.43</td>
      <td>0.39</td>
      <td>0.40</td>
      <td>0.39</td>
      <td>0.32</td>
      <td>0.27</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>0.41</td>
      <td>0.08</td>
      <td>0.19</td>
      <td>0.10</td>
      <td>0.18</td>
      <td>0.48</td>
      <td>0.27</td>
      <td>0.68</td>
      <td>0.23</td>
      <td>0.41</td>
      <td>0.68</td>
      <td>0.40</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.35</td>
      <td>0.55</td>
      <td>0.59</td>
      <td>0.61</td>
      <td>0.56</td>
      <td>0.74</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>0.14</td>
      <td>0.03</td>
      <td>0.24</td>
      <td>0.27</td>
      <td>0.37</td>
      <td>0.39</td>
      <td>0.07</td>
      <td>0.07</td>
      <td>0.08</td>
      <td>0.08</td>
      <td>0.89</td>
      <td>0.06</td>
      <td>0.14</td>
      <td>0.13</td>
      <td>0.33</td>
      <td>0.39</td>
      <td>0.28</td>
      <td>0.55</td>
      <td>0.09</td>
      <td>0.51</td>
      <td>0.5</td>
      <td>0.21</td>
      <td>0.71</td>
      <td>0.52</td>
      <td>0.05</td>
      <td>0.26</td>
      <td>0.65</td>
      <td>0.14</td>
      <td>0.06</td>
      <td>0.22</td>
      <td>0.19</td>
      <td>0.18</td>
      <td>0.36</td>
      <td>0.35</td>
      <td>0.38</td>
      <td>0.34</td>
      <td>0.38</td>
      <td>0.46</td>
      <td>0.25</td>
      <td>0.04</td>
      <td>0.0</td>
      <td>0.12</td>
      <td>0.42</td>
      <td>0.50</td>
      <td>0.51</td>
      <td>0.64</td>
      <td>0.12</td>
      <td>0.26</td>
      <td>0.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.00</td>
      <td>0.16</td>
      <td>0.12</td>
      <td>0.74</td>
      <td>0.45</td>
      <td>0.07</td>
      <td>0.26</td>
      <td>0.59</td>
      <td>0.35</td>
      <td>0.27</td>
      <td>0.02</td>
      <td>1.0</td>
      <td>0.31</td>
      <td>0.72</td>
      <td>0.11</td>
      <td>0.45</td>
      <td>0.25</td>
      <td>0.29</td>
      <td>0.39</td>
      <td>0.29</td>
      <td>0.37</td>
      <td>0.38</td>
      <td>0.33</td>
      <td>0.16</td>
      <td>0.30</td>
      <td>0.22</td>
      <td>0.35</td>
      <td>0.01</td>
      <td>0.24</td>
      <td>0.14</td>
      <td>0.24</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>0.73</td>
      <td>0.57</td>
      <td>0.15</td>
      <td>1.00</td>
      <td>0.63</td>
      <td>0.91</td>
      <td>1.00</td>
      <td>0.29</td>
      <td>0.43</td>
      <td>0.47</td>
      <td>0.60</td>
      <td>0.39</td>
      <td>0.46</td>
      <td>0.53</td>
      <td>0.00</td>
      <td>0.24</td>
      <td>0.01</td>
      <td>0.52</td>
      <td>0.62</td>
      <td>0.64</td>
      <td>0.63</td>
      <td>0.25</td>
      <td>0.27</td>
      <td>0.25</td>
      <td>0.23</td>
      <td>0.84</td>
      <td>0.10</td>
      <td>0.16</td>
      <td>0.10</td>
      <td>0.17</td>
      <td>0.29</td>
      <td>0.17</td>
      <td>0.26</td>
      <td>0.20</td>
      <td>0.82</td>
      <td>0.0</td>
      <td>0.02</td>
      <td>0.79</td>
      <td>0.24</td>
      <td>0.02</td>
      <td>0.25</td>
      <td>0.65</td>
      <td>0.16</td>
      <td>0.00</td>
      <td>0.21</td>
      <td>0.20</td>
      <td>0.21</td>
      <td>0.42</td>
      <td>0.38</td>
      <td>0.40</td>
      <td>0.37</td>
      <td>0.29</td>
      <td>0.32</td>
      <td>0.18</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.21</td>
      <td>0.50</td>
      <td>0.34</td>
      <td>0.60</td>
      <td>0.52</td>
      <td>0.02</td>
      <td>0.12</td>
      <td>0.45</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.49</td>
      <td>0.56</td>
      <td>0.17</td>
      <td>0.04</td>
      <td>0.39</td>
      <td>0.47</td>
      <td>0.28</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.30</td>
      <td>0.58</td>
      <td>0.19</td>
      <td>0.39</td>
      <td>0.38</td>
      <td>0.40</td>
      <td>0.84</td>
      <td>0.28</td>
      <td>0.27</td>
      <td>0.29</td>
      <td>0.27</td>
      <td>0.07</td>
      <td>0.29</td>
      <td>0.28</td>
      <td>0.39</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.27</td>
      <td>0.43</td>
      <td>0.19</td>
      <td>0.36</td>
      <td>0.58</td>
      <td>0.32</td>
      <td>0.29</td>
      <td>0.63</td>
      <td>0.41</td>
      <td>0.71</td>
      <td>0.70</td>
      <td>0.45</td>
      <td>0.42</td>
      <td>0.44</td>
      <td>0.43</td>
      <td>0.43</td>
      <td>0.71</td>
      <td>0.67</td>
      <td>0.01</td>
      <td>0.46</td>
      <td>0.00</td>
      <td>0.07</td>
      <td>0.06</td>
      <td>0.15</td>
      <td>0.19</td>
      <td>0.02</td>
      <td>0.02</td>
      <td>0.04</td>
      <td>0.05</td>
      <td>0.88</td>
      <td>0.04</td>
      <td>0.20</td>
      <td>0.20</td>
      <td>0.46</td>
      <td>0.52</td>
      <td>0.43</td>
      <td>0.42</td>
      <td>0.15</td>
      <td>0.51</td>
      <td>0.5</td>
      <td>0.01</td>
      <td>0.86</td>
      <td>0.41</td>
      <td>0.29</td>
      <td>0.30</td>
      <td>0.52</td>
      <td>0.47</td>
      <td>0.45</td>
      <td>0.18</td>
      <td>0.17</td>
      <td>0.16</td>
      <td>0.27</td>
      <td>0.29</td>
      <td>0.27</td>
      <td>0.31</td>
      <td>0.48</td>
      <td>0.39</td>
      <td>0.28</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.14</td>
      <td>0.49</td>
      <td>0.54</td>
      <td>0.67</td>
      <td>0.56</td>
      <td>0.01</td>
      <td>0.21</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.04</td>
      <td>0.77</td>
      <td>1.00</td>
      <td>0.08</td>
      <td>0.12</td>
      <td>0.10</td>
      <td>0.51</td>
      <td>0.50</td>
      <td>0.34</td>
      <td>0.21</td>
      <td>0.06</td>
      <td>1.0</td>
      <td>0.58</td>
      <td>0.89</td>
      <td>0.21</td>
      <td>0.43</td>
      <td>0.36</td>
      <td>0.20</td>
      <td>0.82</td>
      <td>0.51</td>
      <td>0.36</td>
      <td>0.40</td>
      <td>0.39</td>
      <td>0.16</td>
      <td>0.25</td>
      <td>0.36</td>
      <td>0.44</td>
      <td>0.01</td>
      <td>0.10</td>
      <td>0.09</td>
      <td>0.25</td>
      <td>0.31</td>
      <td>0.33</td>
      <td>0.71</td>
      <td>0.36</td>
      <td>0.45</td>
      <td>0.34</td>
      <td>0.45</td>
      <td>0.49</td>
      <td>0.44</td>
      <td>0.75</td>
      <td>0.65</td>
      <td>0.54</td>
      <td>0.83</td>
      <td>0.65</td>
      <td>0.85</td>
      <td>0.86</td>
      <td>0.03</td>
      <td>0.33</td>
      <td>0.02</td>
      <td>0.11</td>
      <td>0.20</td>
      <td>0.30</td>
      <td>0.31</td>
      <td>0.05</td>
      <td>0.08</td>
      <td>0.11</td>
      <td>0.11</td>
      <td>0.81</td>
      <td>0.08</td>
      <td>0.56</td>
      <td>0.62</td>
      <td>0.85</td>
      <td>0.77</td>
      <td>1.00</td>
      <td>0.94</td>
      <td>0.12</td>
      <td>0.01</td>
      <td>0.5</td>
      <td>0.01</td>
      <td>0.97</td>
      <td>0.96</td>
      <td>0.60</td>
      <td>0.47</td>
      <td>0.52</td>
      <td>0.11</td>
      <td>0.11</td>
      <td>0.24</td>
      <td>0.21</td>
      <td>0.19</td>
      <td>0.75</td>
      <td>0.70</td>
      <td>0.77</td>
      <td>0.89</td>
      <td>0.63</td>
      <td>0.51</td>
      <td>0.47</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.19</td>
      <td>0.30</td>
      <td>0.73</td>
      <td>0.64</td>
      <td>0.65</td>
      <td>0.02</td>
      <td>0.39</td>
      <td>0.28</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.01</td>
      <td>0.55</td>
      <td>0.02</td>
      <td>0.95</td>
      <td>0.09</td>
      <td>0.05</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.23</td>
      <td>0.36</td>
      <td>0.02</td>
      <td>0.9</td>
      <td>0.50</td>
      <td>0.72</td>
      <td>0.16</td>
      <td>0.68</td>
      <td>0.44</td>
      <td>0.11</td>
      <td>0.71</td>
      <td>0.46</td>
      <td>0.43</td>
      <td>0.41</td>
      <td>0.28</td>
      <td>0.00</td>
      <td>0.74</td>
      <td>0.51</td>
      <td>0.48</td>
      <td>0.00</td>
      <td>0.06</td>
      <td>0.25</td>
      <td>0.30</td>
      <td>0.33</td>
      <td>0.12</td>
      <td>0.65</td>
      <td>0.67</td>
      <td>0.38</td>
      <td>0.22</td>
      <td>0.27</td>
      <td>0.20</td>
      <td>0.21</td>
      <td>0.51</td>
      <td>0.91</td>
      <td>0.91</td>
      <td>0.89</td>
      <td>0.85</td>
      <td>0.40</td>
      <td>0.60</td>
      <td>0.00</td>
      <td>0.06</td>
      <td>0.00</td>
      <td>0.03</td>
      <td>0.07</td>
      <td>0.20</td>
      <td>0.27</td>
      <td>0.01</td>
      <td>0.02</td>
      <td>0.04</td>
      <td>0.05</td>
      <td>0.88</td>
      <td>0.05</td>
      <td>0.16</td>
      <td>0.19</td>
      <td>0.59</td>
      <td>0.60</td>
      <td>0.37</td>
      <td>0.89</td>
      <td>0.02</td>
      <td>0.19</td>
      <td>0.5</td>
      <td>0.01</td>
      <td>0.89</td>
      <td>0.87</td>
      <td>0.04</td>
      <td>0.55</td>
      <td>0.73</td>
      <td>0.05</td>
      <td>0.14</td>
      <td>0.31</td>
      <td>0.31</td>
      <td>0.30</td>
      <td>0.40</td>
      <td>0.36</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.22</td>
      <td>0.51</td>
      <td>0.21</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.11</td>
      <td>0.72</td>
      <td>0.64</td>
      <td>0.61</td>
      <td>0.53</td>
      <td>0.04</td>
      <td>0.09</td>
      <td>0.02</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[78]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[78]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0       0.20
1       0.67
2       0.43
3       0.12
4       0.03
        ... 
1989    0.09
1990    0.45
1991    0.23
1992    0.19
1993    0.48
Name: ViolentCrimesPerPop, Length: 1993, dtype: float64</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[79]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Create a 20%-80% randomized split of the data. Set aside the 20% test portion; </span>
<span class="c1">#the 80% training data partition will be used for cross-validation on various tasks specified below.</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">33</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[80]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[80]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(1594, 97)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>b) Perform standard linear regression on data using the closed form solution implementation (not the scikit-learn implementation). Compute the RMSE value on the full training data (the 80% partition). Also, plot the correlation between the predicted and actual values of the target attribute. Display the obtained regression coefficients (weights) and plot them using matplotlib. Finally, perform 10-fold cross-validation and compare the cross-validation RMSE to the training RMSE (for cross validation, you should use the KFold module from sklearn.model_selection).<a rel="noopener" class="anchor-link" href="#b)-Perform-standard-linear-regression-on-data-using-the-closed-form-solution-implementation-(not-the-scikit-learn-implementation).-Compute-the-RMSE-value-on-the-full-training-data-(the-80%-partition).-Also,-plot-the-correlation-between-the-predicted-and-actual-values-of-the-target-attribute.-Display-the-obtained-regression-coefficients-(weights)-and-plot-them-using-matplotlib.-Finally,-perform-10-fold-cross-validation-and-compare-the-cross-validation-RMSE-to-the-training-RMSE-(for-cross-validation,-you-should-use-the-KFold-module-from-sklearn.model_selection).">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[81]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x_train</span><span class="p">])</span>
<span class="n">x_train</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[81]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[0.05, 0.53, 0.31, ..., 0.11, 0.05, 1.  ],
       [0.  , 0.45, 0.41, ..., 0.15, 0.06, 1.  ],
       [0.  , 0.51, 0.02, ..., 0.05, 0.05, 1.  ],
       ...,
       [0.08, 0.3 , 0.19, ..., 0.17, 0.01, 1.  ],
       [0.03, 0.49, 0.52, ..., 0.19, 0.01, 1.  ],
       [0.  , 0.3 , 0.01, ..., 0.32, 0.05, 1.  ]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[82]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">y_train</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[82]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.31, 0.05, 0.22, ..., 0.6 , 0.22, 0.06])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[83]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Standard class notes</span>
<span class="c1">#from Class notes</span>
<span class="k">def</span> <span class="nf">standRegres</span><span class="p">(</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">):</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">xArr</span><span class="p">);</span> <span class="n">yMat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">yArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">xTx</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">xMat</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">xTx</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This matrix is singular, cannot do inverse&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">ws</span> <span class="o">=</span> <span class="n">xTx</span><span class="o">.</span><span class="n">I</span> <span class="o">*</span> <span class="p">(</span><span class="n">xMat</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">yMat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ws</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[84]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">standRegres</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[85]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[85]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>97</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[86]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#xMat=np.mat(X_train)</span>
<span class="c1">#yMat=np.mat(Y_train)</span>
<span class="c1">#yHat = xMat*w</span>

<span class="n">xMat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">yMat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">yHat</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">*</span><span class="n">w</span>
<span class="n">yHat</span> <span class="o">=</span> <span class="n">yHat</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="c1">#yHat[:10]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[87]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">sse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sse</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">err</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rmse</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0.12773957154846552
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[88]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Plot predicted against actual (in the training data)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;g-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Actual&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea">
<img src="javascript://"/>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[89]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># The following function can be used to plot the model coefficients</span>
<span class="o">%</span><span class="k">matplotlib</span> inline



<span class="k">def</span> <span class="nf">plot_coefficients</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">):</span>
    <span class="c1">#plt.rcParams[&quot;figure.figsize&quot;] = (7,20)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">weights</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">feature_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient Value&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_coefficients</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea">
<img src="javascript://"/>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[90]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">cross_validate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># model: regression model to be trained</span>
    <span class="c1"># X: the data matrix</span>
    <span class="c1"># y: the target variable array</span>
    <span class="c1"># n: the number of fold for x-validation</span>
    <span class="c1"># Returns mean RMSE across all folds</span>
    
    <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">KFold</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
    <span class="n">xval_err</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">f</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">train</span><span class="p">,</span><span class="n">test</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">standRegres</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train</span><span class="p">)],</span><span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train</span><span class="p">)])</span>
        <span class="n">yMat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train</span><span class="p">)])</span>
        <span class="n">yHat</span> <span class="o">=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train</span><span class="p">)]))</span><span class="o">*</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train</span><span class="p">)])</span>
        <span class="n">total_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span>
        <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train</span><span class="p">)]))</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fold </span><span class="si">%2d</span><span class="s2"> RMSE: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">rmse</span><span class="p">))</span>
        <span class="n">xval_err</span> <span class="o">+=</span> <span class="n">rmse</span>
        <span class="n">f</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">xval_err</span><span class="o">/</span><span class="n">n</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[91]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">cross_validate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Fold  1 RMSE: 0.1246
Fold  2 RMSE: 0.1255
Fold  3 RMSE: 0.1216
Fold  4 RMSE: 0.1245
Fold  5 RMSE: 0.1260
Fold  6 RMSE: 0.1254
Fold  7 RMSE: 0.1270
Fold  8 RMSE: 0.1279
Fold  9 RMSE: 0.1267
Fold 10 RMSE: 0.1265
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[91]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.12557351434398095</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>C) Feature Selection:  use the scikit-learn regression model from sklearn.linear_model with a subset of features to perform linear regression. For feature selection, write a script or function that takes as input the training data, target variable; the model; and any other parameters you find necessary, and returns the optimal percentage of the most informative features to use. Your approach should use k-fold cross-validation on the training data (you can use k=5). You can use feature_selection.SelectPercentile to find the most informative variables. Show the list of most informative variables and their weights [Note: since this is regression not classification, you should use feature_selection.f_regression as scoring function rather than chi2). Next, plot the model&#39;s mean absolute error values  on cross-validation using only the selected features (See scikit-learn&#39;s metrics.neg_mean_absolute_error). In order to use cross_val_score with regression you&#39;ll need to pass to it scoring=&#39;mean_absolute_error&#39; as a parameter. [Hint: for an example of a similar feature selection process please review the class example notebook (though note that the task in this example was classification not regression). Also, review scikit-learn documentation for feature selection.] As a final step, train your model on the full 80% training data and evaluate it using the set-aside 20% test partition<a rel="noopener" class="anchor-link" href="#C)-Feature-Selection:--use-the-scikit-learn-regression-model-from-sklearn.linear_model-with-a-subset-of-features-to-perform-linear-regression.-For-feature-selection,-write-a-script-or-function-that-takes-as-input-the-training-data,-target-variable;-the-model;-and-any-other-parameters-you-find-necessary,-and-returns-the-optimal-percentage-of-the-most-informative-features-to-use.-Your-approach-should-use-k-fold-cross-validation-on-the-training-data-(you-can-use-k=5).-You-can-use-feature_selection.SelectPercentile-to-find-the-most-informative-variables.-Show-the-list-of-most-informative-variables-and-their-weights-[Note:-since-this-is-regression-not-classification,-you-should-use-feature_selection.f_regression-as-scoring-function-rather-than-chi2).-Next,-plot-the-model&#39;s-mean-absolute-error-values--on-cross-validation-using-only-the-selected-features-(See-scikit-learn&#39;s-metrics.neg_mean_absolute_error).-In-order-to-use-cross_val_score-with-regression-you&#39;ll-need-to-pass-to-it-scoring=&#39;mean_absolute_error&#39;-as-a-parameter.-[Hint:-for-an-example-of-a-similar-feature-selection-process-please-review-the-class-example-notebook-(though-note-that-the-task-in-this-example-was-classification-not-regression).-Also,-review-scikit-learn-documentation-for-feature-selection.]-As-a-final-step,-train-your-model-on-the-full-80%-training-data-and-evaluate-it-using-the-set-aside-20%-test-partition">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[92]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">ElasticNet</span><span class="p">,</span> <span class="n">SGDRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">feature_selection</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">cross_validate</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[93]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">percentiles</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="n">feature_selection</span><span class="o">.</span><span class="n">SelectPercentile</span><span class="p">(</span><span class="n">feature_selection</span><span class="o">.</span><span class="n">f_regression</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">X_train_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">linreg</span><span class="p">,</span> <span class="n">X_train_fs</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%2d</span><span class="s2">  </span><span class="si">%0.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre> 5  0.5933
10  0.5993
15  0.6034
20  0.6151
25  0.6280
30  0.6329
35  0.6359
40  0.6386
45  0.6348
50  0.6327
55  0.6335
60  0.6334
65  0.6309
70  0.6299
75  0.6306
80  0.6323
85  0.6344
90  0.6322
95  0.6387
100  0.6400
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[94]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">optimal_percentile_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">results</span> <span class="o">==</span> <span class="n">results</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimal_percentile_ind</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>19
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[95]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">optimal_percentile_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">results</span> <span class="o">==</span> <span class="n">results</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal percentile of features:</span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentiles</span><span class="p">[</span><span class="n">optimal_percentile_ind</span><span class="p">]),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">optimal_num_features</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">percentiles</span><span class="p">[</span><span class="n">optimal_percentile_ind</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal number of features:</span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_num_features</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot percentile of features VS. cross-validation scores</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Percentage of features selected&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cross validation accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">percentiles</span><span class="p">,</span><span class="n">results</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Optimal percentile of features:96 

Optimal number of features:93 

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[95]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x7f9cc8c29710&gt;]</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea">
<img src="javascript://"/>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[96]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">fs</span><span class="o">.</span><span class="n">get_support</span><span class="p">()[</span><span class="n">i</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10s</span><span class="se">\t\t\t</span><span class="si">%3.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fs</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>population			229.60
householdsize			2.93
racepctblack			992.16
racePctWhite			1304.88
racePctAsian			3.11
racePctHisp			132.07
agePct12t21			5.33
agePct12t29			36.42
agePct16t24			14.74
agePct65up			7.75
 numbUrban			223.62
  pctUrban			11.60
 medIncome			341.31
  pctWWage			170.55
pctWFarmSelf			48.60
pctWInvInc			761.06
pctWSocSec			22.07
pctWPubAsst			783.22
pctWRetire			11.19
 medFamInc			374.43
 perCapInc			214.97
whitePerCap			65.97
blackPerCap			122.12
indianPerCap			15.66
AsianPerCap			38.84
OtherPerCap			26.77
HispPerCap			105.56
NumUnderPov			361.96
PctPopUnderPov			569.49
PctLess9thGrade			296.03
PctNotHSGrad			461.06
PctBSorMore			169.04
PctUnemployed			532.36
 PctEmploy			196.38
PctEmplManu			1.08
PctEmplProfServ			9.10
MalePctDivorce			580.34
MalePctNevMarr			157.85
FemalePctDiv			693.86
TotalPctDiv			677.23
PersPerFam			27.99
PctFam2Par			1596.84
PctKids2Par			1901.30
PctYoungKids2Par			1269.98
PctTeen2Par			1231.32
PctWorkMomYoungKids			1.53
PctWorkMom			42.46
  NumIlleg			399.83
  PctIlleg			1856.85
  NumImmig			133.42
PctImmigRecent			53.04
PctImmigRec5			77.89
PctImmigRec8			104.91
PctImmigRec10			147.39
PctRecentImmig			77.22
PctRecImmig5			88.57
PctRecImmig8			92.19
PctRecImmig10			101.00
PctSpeakEnglOnly			87.17
PctNotSpeakEnglWell			139.91
PctLargHouseFam			240.01
PctLargHouseOccup			133.19
PersPerOccupHous			3.20
PersPerOwnOccHous			31.12
PersPerRentOccHous			101.71
PctPersOwnOccup			600.10
PctPersDenseHous			348.81
PctHousLess3BR			446.48
  MedNumBR			237.21
HousVacant			314.72
PctHousOccup			167.39
PctHousOwnOcc			448.76
PctVacantBoarded			489.30
PctVacMore6Mos			0.25
MedYrHousBuilt			22.23
PctHousNoPhone			485.84
PctWOFullPlumb			229.04
OwnOccLowQuart			70.55
OwnOccMedVal			58.38
OwnOccHiQuart			48.24
  RentLowQ			105.64
RentMedian			97.70
 RentHighQ			92.14
   MedRent			97.00
MedRentPctHousInc			173.17
MedOwnCostPctInc			1.71
MedOwnCostPctIncNoMtg			4.19
NumInShelters			246.13
 NumStreet			176.24
PctForeignBorn			54.50
PctBornSameState			8.14
PctSameHouse85			39.17
PctSameCity85			8.97
PctSameState85			0.92
  LandArea			55.11
   PopDens			133.43
PctUsePubTrans			31.78
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a final step, train your model on the full 80% training data and evaluate it using the set-aside 20% test partition.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[97]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train the model using the training set</span>
<span class="n">linreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">)</span>
<span class="c1"># Let&#39;s see predictions for the first 10 instances and compare to actual MEDV values</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">yArr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%2d</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> </span><span class="si">%2.2f</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> </span><span class="si">%2.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pred</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">yArr</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre> 0 	 0.33 	 0.31
 1 	 0.26 	 0.05
 2 	 0.01 	 0.22
 3 	 -0.05 	 0.10
 4 	 0.27 	 0.31
 5 	 0.10 	 0.19
 6 	 0.34 	 0.52
 7 	 0.06 	 0.08
 8 	 0.22 	 0.22
 9 	 0.16 	 0.71
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[98]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># First, let&#39;s compute errors on all training instances</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">linreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># p is the array of predicted values</span>

<span class="c1"># Now we can constuct an array of errors</span>
<span class="n">err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">Y_test</span><span class="p">)</span>

<span class="c1"># Let&#39;s see the error on the first 10 predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">err</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>1159    0.199120
1080    0.107321
1634    0.026302
1700    0.237067
1956    0.073282
1808    0.222419
137     0.141683
46      0.014871
1989    0.049865
4       0.047577
Name: ViolentCrimesPerPop, dtype: float64
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[99]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Dot product of error vector with itself gives us the sum of squared errors</span>
<span class="n">total_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span>

<span class="c1"># Finally compute RMSE</span>
<span class="n">rmse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total_error</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE on Training Data: &quot;</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">)</span>

<span class="c1"># We can view the regression coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Regression Coefficients: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>RMSE on Training Data:  0.13710537091581151
Regression Coefficients: 
 [-1.23274125e-01  6.75179555e-03  2.30380105e-01  1.19090859e-02
  2.23184672e-03  1.03141096e-01  1.74785968e-01 -2.95684337e-01
 -1.50353957e-01  1.06324388e-01  6.27284543e-02  4.18333159e-02
 -1.27178914e-01 -3.02481695e-01  3.17482567e-02 -1.82704491e-01
 -6.22150671e-02  2.04305861e-02 -7.86345616e-02  1.47145551e-01
  9.37761711e-03 -1.40064473e-01 -2.19732896e-02 -3.10570337e-02
  1.70178754e-02  2.84278945e-02  3.76389589e-02  1.52989956e-01
 -1.95656878e-01 -1.10977370e-01  6.88207406e-02  9.40507991e-02
  3.55343798e-02  2.87584105e-01 -1.93113730e-02 -1.39061270e-02
  3.62876056e-01  2.59733194e-01 -3.53559980e-02 -2.76849870e-01
 -6.68238876e-02  8.06394517e-02 -3.33432611e-01 -2.78871594e-02
 -9.79507415e-03  6.57579495e-02 -1.91489610e-01 -2.34402322e-01
  1.45490323e-01 -9.28827172e-02  4.29715572e-02  4.78269678e-02
 -1.04016599e-01  5.21765256e-02 -5.41566428e-02 -2.69720598e-01
  6.06306320e-01 -3.05519055e-01  3.04490608e-03 -1.48641483e-01
 -1.10929542e-01 -4.16292268e-02  4.65243515e-01  4.93554192e-02
 -2.88217249e-01 -9.52290130e-01  1.97785936e-01  1.28053458e-01
  2.21768047e-02  1.09867470e-01 -5.08158833e-02  8.47013558e-01
  5.05607552e-02 -6.10038547e-02 -9.89461424e-03 -2.64947879e-04
 -9.60089697e-03 -3.71138686e-01  4.52027154e-01 -1.46281922e-01
 -2.36384451e-01 -9.96566067e-03 -9.08828754e-02  3.70974015e-01
  6.56500708e-02 -7.24676689e-02 -8.65905749e-02  1.59665652e-01
  1.38731735e-01  1.17427233e-01  1.78514670e-02  9.70454437e-04
  2.60463611e-02 -2.02858762e-02  1.61847850e-02  7.58514430e-03
 -4.62914805e-02]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[100]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">xArr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="c1"># Let&#39;s put some names to the faces</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%7s</span><span class="s2">   </span><span class="si">%2.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">xArr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>population   -0.12
householdsize   0.01
racepctblack   0.23
racePctWhite   0.01
racePctAsian   0.00
racePctHisp   0.10
agePct12t21   0.17
agePct12t29   -0.30
agePct16t24   -0.15
agePct65up   0.11
numbUrban   0.06
pctUrban   0.04
medIncome   -0.13
pctWWage   -0.30
pctWFarmSelf   0.03
pctWInvInc   -0.18
pctWSocSec   -0.06
pctWPubAsst   0.02
pctWRetire   -0.08
medFamInc   0.15
perCapInc   0.01
whitePerCap   -0.14
blackPerCap   -0.02
indianPerCap   -0.03
AsianPerCap   0.02
OtherPerCap   0.03
HispPerCap   0.04
NumUnderPov   0.15
PctPopUnderPov   -0.20
PctLess9thGrade   -0.11
PctNotHSGrad   0.07
PctBSorMore   0.09
PctUnemployed   0.04
PctEmploy   0.29
PctEmplManu   -0.02
PctEmplProfServ   -0.01
MalePctDivorce   0.36
MalePctNevMarr   0.26
FemalePctDiv   -0.04
TotalPctDiv   -0.28
PersPerFam   -0.07
PctFam2Par   0.08
PctKids2Par   -0.33
PctYoungKids2Par   -0.03
PctTeen2Par   -0.01
PctWorkMomYoungKids   0.07
PctWorkMom   -0.19
NumIlleg   -0.23
PctIlleg   0.15
NumImmig   -0.09
PctImmigRecent   0.04
PctImmigRec5   0.05
PctImmigRec8   -0.10
PctImmigRec10   0.05
PctRecentImmig   -0.05
PctRecImmig5   -0.27
PctRecImmig8   0.61
PctRecImmig10   -0.31
PctSpeakEnglOnly   0.00
PctNotSpeakEnglWell   -0.15
PctLargHouseFam   -0.11
PctLargHouseOccup   -0.04
PersPerOccupHous   0.47
PersPerOwnOccHous   0.05
PersPerRentOccHous   -0.29
PctPersOwnOccup   -0.95
PctPersDenseHous   0.20
PctHousLess3BR   0.13
MedNumBR   0.02
HousVacant   0.11
PctHousOccup   -0.05
PctHousOwnOcc   0.85
PctVacantBoarded   0.05
PctVacMore6Mos   -0.06
MedYrHousBuilt   -0.01
PctHousNoPhone   -0.00
PctWOFullPlumb   -0.01
OwnOccLowQuart   -0.37
OwnOccMedVal   0.45
OwnOccHiQuart   -0.15
RentLowQ   -0.24
RentMedian   -0.01
RentHighQ   -0.09
MedRent   0.37
MedRentPctHousInc   0.07
MedOwnCostPctInc   -0.07
MedOwnCostPctIncNoMtg   -0.09
NumInShelters   0.16
NumStreet   0.14
PctForeignBorn   0.12
PctBornSameState   0.02
PctSameHouse85   0.00
PctSameCity85   0.03
PctSameState85   -0.02
LandArea   0.02
PopDens   0.01
PctUsePubTrans   -0.05
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>d) Next, perform Ridge Regression and Lasso Regression using the modules from sklearn.linear_model. In each case, perform systematic model selection to identify the optimal alpha parameter. You should create a function that takes as input the data and target variable; the parameter to vary and a list of its values; the model to be trained; and any other relevant input needed to determine the optimal value for the specified parameter. The model selection process should perform k-fold cross validation (k should be a parameter, but you can select k=5 for this problem). For each model, you should also plot the error values on the training and cross-validation splits across the specified values of the alpha parameter. Finally, using the best alpha values, train the model on the full training data and evaluate it on the set-aside test data. Discuss your observation and conclusions. [Hint: for an example of a similar model selection process please review the class example notebook.]<a rel="noopener" class="anchor-link" href="#d)-Next,-perform-Ridge-Regression-and-Lasso-Regression-using-the-modules-from-sklearn.linear_model.-In-each-case,-perform-systematic-model-selection-to-identify-the-optimal-alpha-parameter.-You-should-create-a-function-that-takes-as-input-the-data-and-target-variable;-the-parameter-to-vary-and-a-list-of-its-values;-the-model-to-be-trained;-and-any-other-relevant-input-needed-to-determine-the-optimal-value-for-the-specified-parameter.-The-model-selection-process-should-perform-k-fold-cross-validation-(k-should-be-a-parameter,-but-you-can-select-k=5-for-this-problem).-For-each-model,-you-should-also-plot-the-error-values-on-the-training-and-cross-validation-splits-across-the-specified-values-of-the-alpha-parameter.-Finally,-using-the-best-alpha-values,-train-the-model-on-the-full-training-data-and-evaluate-it-on-the-set-aside-test-data.-Discuss-your-observation-and-conclusions.-[Hint:-for-an-example-of-a-similar-model-selection-process-please-review-the-class-example-notebook.]">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[101]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">KFold</span>

<span class="k">def</span> <span class="nf">calc_params</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">param_values</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    
    <span class="c1"># Convert input to Numpy arrays</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># initialize training and testing score arrays with zeros</span>
    <span class="n">train_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_values</span><span class="p">))</span>
    <span class="n">test_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_values</span><span class="p">))</span>
    
    <span class="c1"># iterate over the different parameter values</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_values</span><span class="p">):</span>
        <span class="c1">#print(param_name, &#39; = &#39;, param_value)</span>
        
        <span class="c1"># set classifier parameters</span>
        <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">param_name</span><span class="p">:</span><span class="n">param_value</span><span class="p">})</span>
        
        <span class="c1"># initialize the K scores obtained for each fold</span>
        <span class="n">k_train_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="n">k_test_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        
        <span class="c1"># create KFold cross validation</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># iterate over the K folds</span>
        <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># fit the classifier in the corresponding fold</span>
            <span class="c1"># and obtain the corresponding accuracy scores on train and test sets</span>
            <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
            <span class="n">k_train_scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
            <span class="n">k_test_scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
            
        <span class="c1"># store the mean of the K fold scores</span>
        <span class="n">train_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_train_scores</span><span class="p">)</span>
        <span class="n">test_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_test_scores</span><span class="p">)</span>
       
    <span class="c1"># plot the training and testing scores in a log scale</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">param_values</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">param_values</span><span class="p">,</span> <span class="n">test_scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;X-Val&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot; values&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean cross validation accuracy&quot;</span><span class="p">)</span>

    <span class="c1"># return the training and testing scores on each parameter value</span>
    <span class="k">return</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[102]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s create an evenly spaced range of numbers in a specified interval</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">alpha</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98 100]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[103]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span> <span class="o">=</span> <span class="n">calc_params</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> <span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea">
<img src="javascript://"/>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[104]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#lasso</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">alpha</span><span class="p">])</span>

<span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span> <span class="o">=</span> <span class="n">calc_params</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> <span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  positive)
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.95002833660047, tolerance: 0.006718587717647059
  positive)
/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  positive)
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.513300533724436, tolerance: 0.006897467450980392
  positive)
/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  positive)
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.219043499331741, tolerance: 0.006654039482352943
  positive)
/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  positive)
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.344037186584169, tolerance: 0.006523476658823531
  positive)
/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
  positive)
/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.016724628659645, tolerance: 0.006844143416927898
  positive)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea">
<img src="javascript://"/>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[105]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">met</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;linear regression&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;elastic-net&#39;</span><span class="p">,</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">))</span>
        <span class="p">]:</span>
    
    <span class="c1"># computing the RMSE on training data</span>
    <span class="n">met</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">met</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">p</span><span class="o">-</span><span class="n">Y_train</span>
    <span class="n">total_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total_error</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

    <span class="c1"># computing the RMSE for x-validation</span>
    <span class="c1">#rmse_10cv = cross_validate(met, X_train, Y_train, 10)</span>
    
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Method: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="k">name</span>)
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RMSE on training: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="k">rmse_train</span>)
    <span class="c1">#print(&#39;RMSE on 10-fold CV: %.4f&#39; %rmse_10cv)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Method: linear regression
RMSE on training: 0.1275


Method: lasso
RMSE on training: 0.1534


Method: ridge
RMSE on training: 0.1275


Method: elastic-net
RMSE on training: 0.1441


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>e) Next, perform regression using Stochastic Gradient Descent for regression. For this part, you should use the SGDRegessor module from sklearn.linear_model. Note that SGDRegessor requires that features be standardized (with 0 mean and scaled by standard deviation). Prior to fiting the model, perform the scaling using StandardScaler from sklearn.preprocessing. For this problem, perform a grid search (using GridSearchCV from sklearn.grid_search) Your grid search should compare combinations of two penalty parameters (&#39;l2&#39;, &#39;l1&#39;) and different values of alpha (alpha could vary from 0.0001 which is the default to relatively large values, say 10). Using the best parameters, apply the model to the set-aside test data. Finally, perform model selection (similar to part d, above) to find the best &quot;l1_ratio&quot; parameter using SGDRegressor with  the &quot;elasticnet&quot; penalty parameter. [Note: &quot;l1_ratio&quot; is The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1;  l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1 penalty; defaults to 0.15.] Using the best mixing ratio, apply the Elastic Net model to the set-aside test data. Provide a brief summary of your findings from the above experiments.<a rel="noopener" class="anchor-link" href="#e)-Next,-perform-regression-using-Stochastic-Gradient-Descent-for-regression.-For-this-part,-you-should-use-the-SGDRegessor-module-from-sklearn.linear_model.-Note-that-SGDRegessor-requires-that-features-be-standardized-(with-0-mean-and-scaled-by-standard-deviation).-Prior-to-fiting-the-model,-perform-the-scaling-using-StandardScaler-from-sklearn.preprocessing.-For-this-problem,-perform-a-grid-search-(using-GridSearchCV-from-sklearn.grid_search)-Your-grid-search-should-compare-combinations-of-two-penalty-parameters-(&#39;l2&#39;,-&#39;l1&#39;)-and-different-values-of-alpha-(alpha-could-vary-from-0.0001-which-is-the-default-to-relatively-large-values,-say-10).-Using-the-best-parameters,-apply-the-model-to-the-set-aside-test-data.-Finally,-perform-model-selection-(similar-to-part-d,-above)-to-find-the-best-&quot;l1_ratio&quot;-parameter-using-SGDRegressor-with--the-&quot;elasticnet&quot;-penalty-parameter.-[Note:-&quot;l1_ratio&quot;-is-The-Elastic-Net-mixing-parameter,-with-0-&lt;=-l1_ratio-&lt;=-1;--l1_ratio=0-corresponds-to-L2-penalty,-l1_ratio=1-to-L1-penalty;-defaults-to-0.15.]-Using-the-best-mixing-ratio,-apply-the-Elastic-Net-model-to-the-set-aside-test-data.-Provide-a-brief-summary-of-your-findings-from-the-above-experiments.">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[106]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_s</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">feature_selection</span>
<span class="c1">#from sklearn.cross_validation import train_test_split</span>
<span class="n">x_train_s</span><span class="p">,</span> <span class="n">x_test_s</span><span class="p">,</span> <span class="n">y_train_s</span><span class="p">,</span> <span class="n">y_test_s</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">35</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[107]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">GridSearchCV</span>
<span class="n">sgdreg</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">()</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span><span class="s1">&#39;l1&#39;</span><span class="p">],</span>
    <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">.</span><span class="mi">0001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">sgdreg</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_s</span><span class="p">,</span> <span class="n">y_train_s</span><span class="p">))</span>


<span class="n">best_params</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">best_params</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 5 folds for each of 200 candidates, totalling 1000 fits
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;,
             estimator=SGDRegressor(alpha=0.0001, average=False,
                                    early_stopping=False, epsilon=0.1,
                                    eta0=0.01, fit_intercept=True,
                                    l1_ratio=0.15, learning_rate=&#39;invscaling&#39;,
                                    loss=&#39;squared_loss&#39;, max_iter=1000,
                                    n_iter_no_change=5, penalty=&#39;l2&#39;,
                                    power_t=0.25, random_state=None,
                                    shuffle=True, tol=0.001,
                                    validation_fraction=0.1, ver...
       7.273e-01, 7.374e-01, 7.475e-01, 7.576e-01, 7.677e-01, 7.778e-01,
       7.879e-01, 7.980e-01, 8.081e-01, 8.182e-01, 8.283e-01, 8.384e-01,
       8.485e-01, 8.586e-01, 8.687e-01, 8.788e-01, 8.889e-01, 8.990e-01,
       9.091e-01, 9.192e-01, 9.293e-01, 9.394e-01, 9.495e-01, 9.596e-01,
       9.697e-01, 9.798e-01, 9.899e-01, 1.000e+00]),
                         &#39;penalty&#39;: [&#39;l2&#39;, &#39;l1&#39;]},
             pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False,
             scoring=None, verbose=1)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.7s finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[107]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>({&#39;alpha&#39;: 0.15159999999999998, &#39;penalty&#39;: &#39;l2&#39;}, 0.6375118158089081)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[&#160;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>2 . Automatic Document Clustering [Dataset: newsgroups5.zip]<a rel="noopener" class="anchor-link" href="#2-.-Automatic-Document-Clustering-[Dataset:-newsgroups5.zip]">&#182;</a></h2><p>For this problem you will use a different subset of the 20 Newsgroup data set that you used in Assignment 2  (see the description of the full dataset). The subset for this assignment includes 2,500 documents (newsgroup posts), each belonging to one of 5 categories windows (0), crypt (1), christian (2), hockey (3), forsale (4). The documents are represented by 9328 terms (stems). The dictionary (vocabulary) for the data set is given in the file &quot;terms.txt&quot; and the full term-by-document matrix is given in &quot;matrix.txt&quot; (comma separated values). The actual category labels for the documents are provided in the file &quot;classes.txt&quot;. Your goal in this assignment is to perform clustering on the documents and compare the clusters to the actual categories.</p>
<p>Your tasks in this problem are the following [Note: for the clustering part of this assignment you should use the kMeans module form Ch. 10 of MLA (use the version provided here as it includes some corrections to the book version). You may also use Pandas and other modules from scikit-learn that you may need for preprocessing or evaluation.]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>a) Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[108]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">cosD</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">normX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">normD</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">normX</span> <span class="o">*</span> <span class="n">normD</span><span class="p">)</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">similarity</span>
    <span class="k">return</span> <span class="n">distance</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>b) Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you&#39;ll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the &quot;.T&quot; operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[109]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MatData</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span> <span class="p">(</span><span class="s2">&quot;./newsgroups5/matrix.txt&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">MatData</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[109]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>40</th>
      <th>41</th>
      <th>42</th>
      <th>43</th>
      <th>44</th>
      <th>45</th>
      <th>46</th>
      <th>47</th>
      <th>48</th>
      <th>49</th>
      <th>...</th>
      <th>9278</th>
      <th>9279</th>
      <th>9280</th>
      <th>9281</th>
      <th>9282</th>
      <th>9283</th>
      <th>9284</th>
      <th>9285</th>
      <th>9286</th>
      <th>9287</th>
      <th>9288</th>
      <th>9289</th>
      <th>9290</th>
      <th>9291</th>
      <th>9292</th>
      <th>9293</th>
      <th>9294</th>
      <th>9295</th>
      <th>9296</th>
      <th>9297</th>
      <th>9298</th>
      <th>9299</th>
      <th>9300</th>
      <th>9301</th>
      <th>9302</th>
      <th>9303</th>
      <th>9304</th>
      <th>9305</th>
      <th>9306</th>
      <th>9307</th>
      <th>9308</th>
      <th>9309</th>
      <th>9310</th>
      <th>9311</th>
      <th>9312</th>
      <th>9313</th>
      <th>9314</th>
      <th>9315</th>
      <th>9316</th>
      <th>9317</th>
      <th>9318</th>
      <th>9319</th>
      <th>9320</th>
      <th>9321</th>
      <th>9322</th>
      <th>9323</th>
      <th>9324</th>
      <th>9325</th>
      <th>9326</th>
      <th>9327</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows &#215; 9328 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[110]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">classesData</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;./newsgroups5/classes.txt&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span><span class="n">skip_header</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">classesData</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[110]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2495</th>
      <td>2495</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2496</th>
      <td>2496</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2497</th>
      <td>2497</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2498</th>
      <td>2498</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2499</th>
      <td>2499</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>2500 rows &#215; 2 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[111]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">TermData</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span> <span class="p">(</span><span class="s2">&quot;./newsgroups5/terms.txt&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="nb">str</span><span class="p">))</span>
<span class="n">TermData</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[111]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([&#39;aa&#39;, &#39;aargh&#39;, &#39;aaron&#39;, ..., &#39;zw&#39;, &#39;zx&#39;, &#39;zz&#39;], dtype=&#39;&lt;U30&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[112]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">MatData</span><span class="p">,</span> <span class="n">classesData</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">33</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>c) Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term and the size of the cluster. The cluster DF value for a term t in a cluster C is the percentage of docs in cluster C in which term t appears (so, if a cluster has 500 documents, and term &quot;game&quot; appears in 100 of those 500 documents, then DF value of &quot;game&quot; in that cluster is 0.2 or 20%). Sort the terms for each cluster in decreasing order of the DF percentage. Here is an example of how this output might look like (here the top 10 terms for 3 of the 5 clusters are displayed in decreasing order of cluster DF values, but the mean frequnecy from the cluster centroid is also shown). [Extra Credit: use your favorite third party tool or library, ideally with a Python based API, to create a word cloud for each cluster.]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[113]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">k Means Clustering for Ch10 of Machine Learning in Action</span>
<span class="sd">@author: Peter Harrington</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">distEuclid</span><span class="p">(</span><span class="n">vecA</span><span class="p">,</span> <span class="n">vecB</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">power</span><span class="p">(</span><span class="n">vecA</span> <span class="o">-</span> <span class="n">vecB</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="c1">#la.norm(vecA-vecB)</span>

<span class="k">def</span> <span class="nf">randCent</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> <span class="c1">#create random cluster centers</span>
        <span class="n">minJ</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dataSet</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
        <span class="n">rangeJ</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">dataSet</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span> <span class="o">-</span> <span class="n">minJ</span><span class="p">)</span>
        <span class="n">centroids</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">minJ</span> <span class="o">+</span> <span class="n">rangeJ</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">centroids</span> 

<span class="k">def</span> <span class="nf">kMeans</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">distMeas</span><span class="o">=</span><span class="n">distEuclid</span><span class="p">,</span> <span class="n">createCent</span><span class="o">=</span><span class="n">randCent</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">clusterAssment</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#create mat to assign data points </span>
                                  <span class="c1">#to a centroid, also holds SE of each point</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">createCent</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">clusterChanged</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">while</span> <span class="n">clusterChanged</span><span class="p">:</span>
        <span class="n">clusterChanged</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span> <span class="c1">#for each data point assign it to the closest centroid</span>
            <span class="n">minDist</span> <span class="o">=</span> <span class="n">inf</span><span class="p">;</span> <span class="n">minIndex</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">distJI</span> <span class="o">=</span> <span class="n">distMeas</span><span class="p">(</span><span class="n">centroids</span><span class="p">[</span><span class="n">j</span><span class="p">,:],</span><span class="n">dataSet</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
                <span class="k">if</span> <span class="n">distJI</span> <span class="o">&lt;</span> <span class="n">minDist</span><span class="p">:</span>
                    <span class="n">minDist</span> <span class="o">=</span> <span class="n">distJI</span><span class="p">;</span> <span class="n">minIndex</span> <span class="o">=</span> <span class="n">j</span>
            <span class="k">if</span> <span class="n">clusterAssment</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">minIndex</span><span class="p">:</span> <span class="n">clusterChanged</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">clusterAssment</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">minIndex</span><span class="p">,</span><span class="n">minDist</span><span class="o">**</span><span class="mi">2</span>
        <span class="c1"># print centroids</span>
        <span class="k">for</span> <span class="n">cent</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span><span class="c1">#recalculate centroids</span>
            <span class="n">ptsInClust</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">cent</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span> <span class="c1">#get all the point in this cluster - Note: this was incorrect in the original distribution.</span>
            <span class="k">if</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ptsInClust</span><span class="p">)</span><span class="o">!=</span><span class="mi">0</span><span class="p">):</span>
                <span class="n">centroids</span><span class="p">[</span><span class="n">cent</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">ptsInClust</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#assign centroid to mean - Note condition was added 10/28/2013</span>
    <span class="k">return</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">clusterAssment</span>

<span class="k">def</span> <span class="nf">biKmeans</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">distMeas</span><span class="o">=</span><span class="n">distEuclid</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">clusterAssment</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">centroid0</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">centList</span> <span class="o">=</span><span class="p">[</span><span class="n">centroid0</span><span class="p">]</span> <span class="c1">#create a list with one centroid</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span> <span class="c1">#calc initial Error</span>
        <span class="n">clusterAssment</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">distMeas</span><span class="p">(</span><span class="n">mat</span><span class="p">(</span><span class="n">centroid0</span><span class="p">),</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">j</span><span class="p">,:])</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">while</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">centList</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">lowestSSE</span> <span class="o">=</span> <span class="n">inf</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">centList</span><span class="p">)):</span>
            <span class="n">ptsInCurrCluster</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">==</span><span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">],:]</span> <span class="c1">#get the data points currently in cluster i</span>
            <span class="n">centroidMat</span><span class="p">,</span> <span class="n">splitClustAss</span> <span class="o">=</span> <span class="n">kMeans</span><span class="p">(</span><span class="n">ptsInCurrCluster</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">distMeas</span><span class="p">)</span>
            <span class="n">sseSplit</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">splitClustAss</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="c1">#compare the SSE to the currrent minimum</span>
            <span class="n">sseNotSplit</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">!=</span><span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sseSplit, and notSplit: &quot;</span><span class="p">,</span><span class="n">sseSplit</span><span class="p">,</span><span class="n">sseNotSplit</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">sseSplit</span> <span class="o">+</span> <span class="n">sseNotSplit</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">lowestSSE</span><span class="p">:</span>
                <span class="n">bestCentToSplit</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">bestNewCents</span> <span class="o">=</span> <span class="n">centroidMat</span>
                <span class="n">bestClustAss</span> <span class="o">=</span> <span class="n">splitClustAss</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">lowestSSE</span> <span class="o">=</span> <span class="n">sseSplit</span> <span class="o">+</span> <span class="n">sseNotSplit</span>
        <span class="n">bestClustAss</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">bestClustAss</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">centList</span><span class="p">)</span> <span class="c1">#change 1 to 3,4, or whatever</span>
        <span class="n">bestClustAss</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">bestClustAss</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">bestCentToSplit</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the bestCentToSplit is: &#39;</span><span class="p">,</span><span class="n">bestCentToSplit</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the len of bestClustAss is: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bestClustAss</span><span class="p">))</span>
        <span class="n">centList</span><span class="p">[</span><span class="n">bestCentToSplit</span><span class="p">]</span> <span class="o">=</span> <span class="n">bestNewCents</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#replace a centroid with two best centroids </span>
        <span class="n">centList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bestNewCents</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">clusterAssment</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span> <span class="o">==</span> <span class="n">bestCentToSplit</span><span class="p">)[</span><span class="mi">0</span><span class="p">],:]</span><span class="o">=</span> <span class="n">bestClustAss</span> <span class="c1">#reassign new clusters, and SSE</span>
    <span class="k">return</span> <span class="n">mat</span><span class="p">(</span><span class="n">centList</span><span class="p">),</span> <span class="n">clusterAssment</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[114]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">numterms</span> <span class="o">=</span><span class="mi">10</span>
<span class="n">XtrainArr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">centroids_tfidf</span><span class="p">,</span> <span class="n">clusters_tfidf</span> <span class="o">=</span> <span class="n">kMeans</span><span class="p">(</span><span class="n">DT_tfidf_train</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">)</span>
    <span class="n">cluster_term</span> <span class="o">=</span> <span class="n">XtrainArr</span><span class="p">[</span><span class="n">clusters_tfidf</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of documents in Cluster&quot;</span><span class="p">,</span> <span class="n">cluster_term</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">cluster_doc_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">cluster_term</span><span class="o">.</span><span class="n">T</span><span class="o">!=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">term_doc_cluster</span> <span class="o">=</span> <span class="n">cluster_doc_freq</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">cluster_term</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">term_doc_cluster</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">term_doc_cluster</span><span class="p">)</span>
    <span class="n">term_doc_cluster</span> <span class="o">=</span> <span class="p">[</span><span class="n">elem</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">term_doc_cluster</span><span class="p">]</span>
    <span class="n">cluster_doc_freq</span> <span class="o">=</span> <span class="p">[</span><span class="n">elem</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">cluster_doc_freq</span><span class="p">]</span>
    <span class="n">a</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">TermData</span><span class="p">,</span> <span class="n">cluster_doc_freq</span><span class="p">,</span> <span class="n">term_doc_cluster</span><span class="p">),</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1">#Sort in decreasing order</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word: &#39;</span> <span class="p">,</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Frequency: &#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">% &#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">a</span><span class="p">[:</span><span class="n">numterms</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">elem</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="p">,</span><span class="n">elem</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">elem</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cluster 1 :
Number of documents in Cluster 1994
Word:  	Frequency:  	% 
subject 	 1994 		 100.0
write 	 960 		 48.144433299899696
on 	 774 		 38.81644934804413
articl 	 720 		 36.10832497492478
know 	 585 		 29.33801404212638
get 	 582 		 29.187562688064194
just 	 549 		 27.53259779338014
think 	 508 		 25.47642928786359
time 	 469 		 23.520561685055167
go 	 444 		 22.26680040120361


Cluster 2 :
Number of documents in Cluster 2
Word:  	Frequency:  	% 
asq 	 2 		 100.0
asqq 	 2 		 100.0
ax 	 2 		 100.0
bhj 	 2 		 100.0
bhjbhjn 	 2 		 100.0
bhjbiz 	 2 		 100.0
bhjkn 	 2 		 100.0
bhjn 	 2 		 100.0
bj 	 2 		 100.0
bwm 	 2 		 100.0


Cluster 3 :
Number of documents in Cluster 2
Word:  	Frequency:  	% 
asq 	 2 		 100.0
asqq 	 2 		 100.0
ax 	 2 		 100.0
bhj 	 2 		 100.0
bhjbhjkn 	 2 		 100.0
bhjbiz 	 2 		 100.0
bhjbj 	 2 		 100.0
bhjkn 	 2 		 100.0
bhjn 	 2 		 100.0
biz 	 2 		 100.0


Cluster 4 :
Number of documents in Cluster 1
Word:  	Frequency:  	% 
asf 	 1 		 100.0
asq 	 1 		 100.0
asqq 	 1 		 100.0
ax 	 1 		 100.0
bhj 	 1 		 100.0
bhjbiz 	 1 		 100.0
bhjbj 	 1 		 100.0
bhjgiz 	 1 		 100.0
bhjkn 	 1 		 100.0
bhjn 	 1 		 100.0


Cluster 5 :
Number of documents in Cluster 1
Word:  	Frequency:  	% 
ad 	 1 		 100.0
af 	 1 		 100.0
ahl 	 1 		 100.0
aj 	 1 		 100.0
ak 	 1 		 100.0
ao 	 1 		 100.0
ap 	 1 		 100.0
aq 	 1 		 100.0
ar 	 1 		 100.0
asq 	 1 		 100.0


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>d) Using the cluster assignments from Kmeans clustering, compare your 5 clusters to the 5 pre-assigned classes by computing the Completeness and Homogeneity values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[115]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">completeness_score</span><span class="p">,</span> <span class="n">homogeneity_score</span>
<span class="n">centroids_tfidf</span><span class="p">,</span> <span class="n">clusters_tfidf</span> <span class="o">=</span> <span class="n">kMeans</span><span class="p">(</span><span class="n">XtrainArr</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">clusters_tfidf</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">y_train2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_train2</span><span class="o">.</span><span class="n">shape</span>
<span class="n">completeness</span> <span class="o">=</span> <span class="n">completeness_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train2</span><span class="o">.</span><span class="n">T</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="n">clusters</span><span class="p">)</span>
<span class="n">homogeneity</span> <span class="o">=</span> <span class="n">homogeneity_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train2</span><span class="o">.</span><span class="n">T</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="n">clusters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Completeness: &quot;</span><span class="p">,</span> <span class="n">completeness</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Homogeneity: &quot;</span><span class="p">,</span> <span class="n">homogeneity</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Completeness:  1.00000000000002
Homogeneity:  0.0029381336082044763
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>e)Finally, using your cluster assignments as class labels, categorize each of the documents in the 20% set-aside data into each of the appropriate cluster. Your categorization should be based on Cosine similarity between each test document and cluster centroids. For each test document show the predicted class label as well as Cosine similarity to the corresponding cluster.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[116]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x_test_np</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">x_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[116]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>40</th>
      <th>41</th>
      <th>42</th>
      <th>43</th>
      <th>44</th>
      <th>45</th>
      <th>46</th>
      <th>47</th>
      <th>48</th>
      <th>49</th>
      <th>...</th>
      <th>9278</th>
      <th>9279</th>
      <th>9280</th>
      <th>9281</th>
      <th>9282</th>
      <th>9283</th>
      <th>9284</th>
      <th>9285</th>
      <th>9286</th>
      <th>9287</th>
      <th>9288</th>
      <th>9289</th>
      <th>9290</th>
      <th>9291</th>
      <th>9292</th>
      <th>9293</th>
      <th>9294</th>
      <th>9295</th>
      <th>9296</th>
      <th>9297</th>
      <th>9298</th>
      <th>9299</th>
      <th>9300</th>
      <th>9301</th>
      <th>9302</th>
      <th>9303</th>
      <th>9304</th>
      <th>9305</th>
      <th>9306</th>
      <th>9307</th>
      <th>9308</th>
      <th>9309</th>
      <th>9310</th>
      <th>9311</th>
      <th>9312</th>
      <th>9313</th>
      <th>9314</th>
      <th>9315</th>
      <th>9316</th>
      <th>9317</th>
      <th>9318</th>
      <th>9319</th>
      <th>9320</th>
      <th>9321</th>
      <th>9322</th>
      <th>9323</th>
      <th>9324</th>
      <th>9325</th>
      <th>9326</th>
      <th>9327</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1590</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>277</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2297</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1739</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>511</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows &#215; 9328 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[121]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">npX</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">centroids</span><span class="p">,</span> <span class="n">clusters</span> <span class="o">=</span> <span class="n">kMeans</span><span class="p">(</span><span class="n">npX</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cosD</span><span class="p">,</span> <span class="n">randCent</span><span class="p">)</span>
<span class="n">cosDmat</span><span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cosD</span><span class="p">(</span><span class="n">npX</span><span class="p">,</span><span class="n">centroids</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">cosDmat</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[121]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[1.48197165e-08, 6.62229616e-09, 1.98668885e-08, 1.86919650e-08,
        1.24977168e-08],
       [2.23566008e-08, 1.10371602e-08, 2.45970999e-08, 1.77717451e-08,
        2.32029324e-08],
       [3.53556092e-08, 6.62229616e-09, 4.91941999e-08, 3.20762334e-08,
        2.48460584e-08],
       ...,
       [7.66365645e-07, 2.00876316e-07, 1.67449488e-07, 1.53865682e-07,
        2.52460518e-07],
       [6.05999376e-08, 6.62229616e-09, 7.00071306e-08, 6.28214346e-08,
        3.48707875e-08],
       [6.64516084e-08, 6.62229616e-09, 1.41906346e-08, 2.54703698e-08,
        2.91447420e-08]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[122]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cosDmat</span><span class="p">)):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cosDmat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">preds</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">i</span><span class="p">:(</span><span class="n">m</span><span class="p">,</span><span class="n">ravel</span><span class="p">(</span><span class="n">cosDmat</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="n">m</span><span class="p">])})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cluster predicted: </span><span class="si">{}</span><span class="s2">, Cosine Similarity: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cluster predicted: 2, Cosine Similarity: 1.986688846766782e-08
Cluster predicted: 2, Cosine Similarity: 2.4597099912426756e-08
Cluster predicted: 2, Cosine Similarity: 4.9194199935875815e-08
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 0, Cosine Similarity: 6.479180025831255e-08
Cluster predicted: 0, Cosine Similarity: 2.16181554990591e-07
Cluster predicted: 3, Cosine Similarity: 1.1005664624086364e-07
Cluster predicted: 0, Cosine Similarity: 2.0249660537263736e-07
Cluster predicted: 0, Cosine Similarity: 2.2424348045113618e-08
Cluster predicted: 2, Cosine Similarity: 8.419776520440792e-08
Cluster predicted: 0, Cosine Similarity: 5.4121604420309666e-08
Cluster predicted: 4, Cosine Similarity: 2.508007945767332e-07
Cluster predicted: 3, Cosine Similarity: 8.22857268989452e-08
Cluster predicted: 0, Cosine Similarity: 1.803771200181714e-07
Cluster predicted: 0, Cosine Similarity: 6.677340802507814e-08
Cluster predicted: 0, Cosine Similarity: 1.4701158734808928e-08
Cluster predicted: 2, Cosine Similarity: 1.7974803823150864e-07
Cluster predicted: 2, Cosine Similarity: 2.270501531231872e-08
Cluster predicted: 2, Cosine Similarity: 2.365105766788389e-08
Cluster predicted: 3, Cosine Similarity: 4.0382860500010054e-08
Cluster predicted: 4, Cosine Similarity: 9.257936817164136e-08
Cluster predicted: 0, Cosine Similarity: 8.62846233351533e-08
Cluster predicted: 3, Cosine Similarity: 1.0835588282187558e-07
Cluster predicted: 0, Cosine Similarity: 3.9606750767973153e-08
Cluster predicted: 3, Cosine Similarity: 5.16473369627235e-08
Cluster predicted: 0, Cosine Similarity: 4.313807744349418e-08
Cluster predicted: 0, Cosine Similarity: 1.0009659878740251e-08
Cluster predicted: 0, Cosine Similarity: 1.2052240250248758e-07
Cluster predicted: 0, Cosine Similarity: 7.254039480208974e-08
Cluster predicted: 0, Cosine Similarity: 6.449540590480751e-08
Cluster predicted: 4, Cosine Similarity: 9.342582707461133e-08
Cluster predicted: 0, Cosine Similarity: 2.129466225575527e-07
Cluster predicted: 3, Cosine Similarity: 1.0156926166793312e-07
Cluster predicted: 4, Cosine Similarity: 1.5705962008727425e-07
Cluster predicted: 0, Cosine Similarity: 4.355302951619677e-08
Cluster predicted: 0, Cosine Similarity: 1.1628819773790156e-07
Cluster predicted: 4, Cosine Similarity: 1.3521766084600984e-07
Cluster predicted: 2, Cosine Similarity: 3.1976229952768165e-07
Cluster predicted: 3, Cosine Similarity: 2.256181785842415e-08
Cluster predicted: 0, Cosine Similarity: 4.2655378118539033e-08
Cluster predicted: 2, Cosine Similarity: 1.0406465356105343e-08
Cluster predicted: 2, Cosine Similarity: 1.021725690719677e-07
Cluster predicted: 4, Cosine Similarity: 1.7113407402291614e-07
Cluster predicted: 0, Cosine Similarity: 3.6041550410814693e-08
Cluster predicted: 0, Cosine Similarity: 3.119423283015621e-07
Cluster predicted: 0, Cosine Similarity: 8.269401774452234e-08
Cluster predicted: 0, Cosine Similarity: 2.4143435140011604e-08
Cluster predicted: 4, Cosine Similarity: 4.501169714732356e-08
Cluster predicted: 2, Cosine Similarity: 7.852151140408381e-08
Cluster predicted: 3, Cosine Similarity: 8.279513430409224e-08
Cluster predicted: 4, Cosine Similarity: 1.2519293191015635e-07
Cluster predicted: 2, Cosine Similarity: 5.014024218041868e-08
Cluster predicted: 4, Cosine Similarity: 1.4037940043731822e-07
Cluster predicted: 2, Cosine Similarity: 1.229854997841784e-07
Cluster predicted: 4, Cosine Similarity: 6.034754085337823e-08
Cluster predicted: 4, Cosine Similarity: 1.9266068584933294e-07
Cluster predicted: 2, Cosine Similarity: 4.758592800913064e-07
Cluster predicted: 4, Cosine Similarity: 1.0094437374874587e-07
Cluster predicted: 0, Cosine Similarity: 5.607780695360276e-08
Cluster predicted: 0, Cosine Similarity: 1.1545829359249637e-07
Cluster predicted: 3, Cosine Similarity: 2.2104172847647874e-07
Cluster predicted: 0, Cosine Similarity: 1.623394080274565e-07
Cluster predicted: 4, Cosine Similarity: 2.0032860814644948e-08
Cluster predicted: 2, Cosine Similarity: 5.581649609176509e-08
Cluster predicted: 3, Cosine Similarity: 4.634785677470887e-08
Cluster predicted: 0, Cosine Similarity: 3.654118652107741e-08
Cluster predicted: 0, Cosine Similarity: 8.162699816072205e-08
Cluster predicted: 2, Cosine Similarity: 1.7123365747551134e-07
Cluster predicted: 4, Cosine Similarity: 1.573417730549309e-07
Cluster predicted: 3, Cosine Similarity: 4.777748396911363e-08
Cluster predicted: 0, Cosine Similarity: 1.2443480756907377e-07
Cluster predicted: 2, Cosine Similarity: 2.1758973067775855e-08
Cluster predicted: 0, Cosine Similarity: 7.199841667304696e-08
Cluster predicted: 4, Cosine Similarity: 1.5402232600436605e-08
Cluster predicted: 0, Cosine Similarity: 1.7590579992621258e-07
Cluster predicted: 2, Cosine Similarity: 1.1352507678363821e-07
Cluster predicted: 4, Cosine Similarity: 4.562579469791217e-08
Cluster predicted: 0, Cosine Similarity: 8.81646101680289e-08
Cluster predicted: 0, Cosine Similarity: 1.6367741673128933e-07
Cluster predicted: 0, Cosine Similarity: 1.994818515926866e-07
Cluster predicted: 4, Cosine Similarity: 1.325621035164204e-07
Cluster predicted: 0, Cosine Similarity: 1.7538075880452197e-08
Cluster predicted: 0, Cosine Similarity: 9.051035954055209e-08
Cluster predicted: 0, Cosine Similarity: 2.966483814148546e-08
Cluster predicted: 2, Cosine Similarity: 6.338483449219723e-08
Cluster predicted: 0, Cosine Similarity: 3.72694697325926e-08
Cluster predicted: 0, Cosine Similarity: 1.0978445930120984e-07
Cluster predicted: 2, Cosine Similarity: 2.3461849196770856e-07
Cluster predicted: 0, Cosine Similarity: 7.728693830477695e-07
Cluster predicted: 4, Cosine Similarity: 6.421469622708287e-08
Cluster predicted: 0, Cosine Similarity: 1.1190240853942157e-06
Cluster predicted: 3, Cosine Similarity: 6.091526505436207e-08
Cluster predicted: 2, Cosine Similarity: 1.229854997841784e-07
Cluster predicted: 4, Cosine Similarity: 9.030554715128147e-08
Cluster predicted: 3, Cosine Similarity: 1.1206141081743226e-07
Cluster predicted: 4, Cosine Similarity: 2.119466713246254e-08
Cluster predicted: 4, Cosine Similarity: 3.867155395909094e-08
Cluster predicted: 3, Cosine Similarity: 2.0483107099344977e-08
Cluster predicted: 0, Cosine Similarity: 5.57322958893991e-07
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 0, Cosine Similarity: 1.0046920906336965e-07
Cluster predicted: 0, Cosine Similarity: 3.738802745179015e-08
Cluster predicted: 2, Cosine Similarity: 1.8164012283161668e-07
Cluster predicted: 4, Cosine Similarity: 1.962954799816785e-07
Cluster predicted: 3, Cosine Similarity: 6.27146233833642e-08
Cluster predicted: 2, Cosine Similarity: 7.757546915954094e-08
Cluster predicted: 2, Cosine Similarity: 5.581649609176509e-08
Cluster predicted: 0, Cosine Similarity: 8.957036612411429e-08
Cluster predicted: 3, Cosine Similarity: 3.224055844519569e-08
Cluster predicted: 0, Cosine Similarity: 2.205935962784622e-07
Cluster predicted: 2, Cosine Similarity: 2.4313287272370587e-07
Cluster predicted: 0, Cosine Similarity: 1.973647493214159e-07
Cluster predicted: 4, Cosine Similarity: 2.2666841714702457e-07
Cluster predicted: 0, Cosine Similarity: 1.7466941215626974e-07
Cluster predicted: 2, Cosine Similarity: 2.459709996793791e-07
Cluster predicted: 0, Cosine Similarity: 3.1864084015786887e-07
Cluster predicted: 3, Cosine Similarity: 3.7367497318641085e-08
Cluster predicted: 3, Cosine Similarity: 2.3104090307590752e-08
Cluster predicted: 4, Cosine Similarity: 2.185855640934875e-08
Cluster predicted: 0, Cosine Similarity: 4.7406155778695336e-08
Cluster predicted: 0, Cosine Similarity: 8.420986297164035e-08
Cluster predicted: 0, Cosine Similarity: 7.541965407753537e-08
Cluster predicted: 0, Cosine Similarity: 3.446642626858676e-08
Cluster predicted: 3, Cosine Similarity: 1.2789412129343702e-07
Cluster predicted: 2, Cosine Similarity: 2.838126922366513e-08
Cluster predicted: 0, Cosine Similarity: 1.679539634036331e-07
Cluster predicted: 0, Cosine Similarity: 6.414820108435748e-08
Cluster predicted: 0, Cosine Similarity: 6.87719526881736e-08
Cluster predicted: 0, Cosine Similarity: 1.8260431167149704e-07
Cluster predicted: 2, Cosine Similarity: 8.135963835975701e-08
Cluster predicted: 0, Cosine Similarity: 1.2982918440318514e-07
Cluster predicted: 0, Cosine Similarity: 1.9717844423894348e-07
Cluster predicted: 2, Cosine Similarity: 1.3623009209595693e-07
Cluster predicted: 0, Cosine Similarity: 7.071968677330176e-08
Cluster predicted: 2, Cosine Similarity: 4.5410030735659745e-08
Cluster predicted: 3, Cosine Similarity: 9.438004444639603e-08
Cluster predicted: 0, Cosine Similarity: 6.217506176575682e-08
Cluster predicted: 0, Cosine Similarity: 5.223314913926913e-08
Cluster predicted: 2, Cosine Similarity: 1.371761344515221e-07
Cluster predicted: 3, Cosine Similarity: 2.322733394510834e-08
Cluster predicted: 2, Cosine Similarity: 1.721796997200542e-07
Cluster predicted: 4, Cosine Similarity: 1.195996641722985e-07
Cluster predicted: 0, Cosine Similarity: 1.0845491904554194e-07
Cluster predicted: 0, Cosine Similarity: 5.44179987738147e-08
Cluster predicted: 2, Cosine Similarity: 1.1352507678363821e-07
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 0, Cosine Similarity: 2.2353213424697316e-07
Cluster predicted: 0, Cosine Similarity: 2.0380074039483276e-07
Cluster predicted: 4, Cosine Similarity: 1.759306744730793e-07
Cluster predicted: 0, Cosine Similarity: 2.3621781153337906e-07
Cluster predicted: 2, Cosine Similarity: 5.6762538336307955e-08
Cluster predicted: 2, Cosine Similarity: 1.693415728754033e-07
Cluster predicted: 0, Cosine Similarity: 5.21992754576317e-08
Cluster predicted: 2, Cosine Similarity: 3.689564997966244e-08
Cluster predicted: 4, Cosine Similarity: 1.335247430400699e-07
Cluster predicted: 0, Cosine Similarity: 9.071190771869908e-07
Cluster predicted: 0, Cosine Similarity: 7.830738169012363e-08
Cluster predicted: 4, Cosine Similarity: 3.792467839769387e-08
Cluster predicted: 4, Cosine Similarity: 7.541782875986058e-08
Cluster predicted: 0, Cosine Similarity: 2.1060934107808293e-08
Cluster predicted: 4, Cosine Similarity: 3.374715475956691e-07
Cluster predicted: 0, Cosine Similarity: 3.4313994867751774e-08
Cluster predicted: 3, Cosine Similarity: 3.516554281102202e-08
Cluster predicted: 4, Cosine Similarity: 1.1390681309286776e-07
Cluster predicted: 0, Cosine Similarity: 9.548131585379593e-08
Cluster predicted: 0, Cosine Similarity: 9.681085610946383e-08
Cluster predicted: 0, Cosine Similarity: 5.150486592775394e-08
Cluster predicted: 0, Cosine Similarity: 6.134515762035164e-08
Cluster predicted: 4, Cosine Similarity: 8.623922498340875e-08
Cluster predicted: 2, Cosine Similarity: 8.608984991553825e-08
Cluster predicted: 4, Cosine Similarity: 2.4539009646495913e-07
Cluster predicted: 4, Cosine Similarity: 5.883719256249975e-08
Cluster predicted: 0, Cosine Similarity: 9.312709803310781e-08
Cluster predicted: 4, Cosine Similarity: 1.428523882518462e-07
Cluster predicted: 2, Cosine Similarity: 6.14927498920892e-08
Cluster predicted: 4, Cosine Similarity: 1.1692750956360243e-07
Cluster predicted: 4, Cosine Similarity: 9.364159103686376e-08
Cluster predicted: 0, Cosine Similarity: 1.0419530915850572e-07
Cluster predicted: 0, Cosine Similarity: 1.8727887363123585e-07
Cluster predicted: 4, Cosine Similarity: 1.7503442384381174e-07
Cluster predicted: 1, Cosine Similarity: 4.061674968580675e-07
Cluster predicted: 2, Cosine Similarity: 1.2109341518407035e-07
Cluster predicted: 4, Cosine Similarity: 5.955087356568356e-08
Cluster predicted: 0, Cosine Similarity: 1.6015455839291093e-07
Cluster predicted: 2, Cosine Similarity: 5.2978369136091885e-08
Cluster predicted: 3, Cosine Similarity: 3.6841657502506564e-08
Cluster predicted: 4, Cosine Similarity: 4.167565315071897e-08
Cluster predicted: 0, Cosine Similarity: 2.4719287061714112e-08
Cluster predicted: 2, Cosine Similarity: 9.744235751618646e-08
Cluster predicted: 0, Cosine Similarity: 1.9536620465832044e-07
Cluster predicted: 0, Cosine Similarity: 2.057400061428183e-07
Cluster predicted: 2, Cosine Similarity: 7.662942680397578e-08
Cluster predicted: 0, Cosine Similarity: 1.2518002756589652e-07
Cluster predicted: 0, Cosine Similarity: 2.488357415675324e-07
Cluster predicted: 2, Cosine Similarity: 1.5609698056362475e-07
Cluster predicted: 3, Cosine Similarity: 8.47670338366413e-08
Cluster predicted: 0, Cosine Similarity: 3.8192526363722834e-08
Cluster predicted: 0, Cosine Similarity: 2.1158320862024738e-07
Cluster predicted: 3, Cosine Similarity: 2.412290500686254e-08
Cluster predicted: 0, Cosine Similarity: 1.8393385148307573e-08
Cluster predicted: 0, Cosine Similarity: 6.300750639010033e-07
Cluster predicted: 0, Cosine Similarity: 4.500112749106222e-08
Cluster predicted: 0, Cosine Similarity: 1.802162202801938e-07
Cluster predicted: 2, Cosine Similarity: 2.478630842794871e-07
Cluster predicted: 4, Cosine Similarity: 2.015568029145598e-07
Cluster predicted: 0, Cosine Similarity: 5.943976555400354e-08
Cluster predicted: 4, Cosine Similarity: 1.2024695628110038e-07
Cluster predicted: 0, Cosine Similarity: 6.55878307220803e-08
Cluster predicted: 0, Cosine Similarity: 2.3313531105628726e-08
Cluster predicted: 0, Cosine Similarity: 5.1521802713061504e-08
Cluster predicted: 2, Cosine Similarity: 1.0974090747239984e-07
Cluster predicted: 0, Cosine Similarity: 4.5365269096819816e-08
Cluster predicted: 4, Cosine Similarity: 6.628935034225236e-08
Cluster predicted: 2, Cosine Similarity: 1.8164012283161668e-07
Cluster predicted: 2, Cosine Similarity: 2.365105766788389e-08
Cluster predicted: 0, Cosine Similarity: 6.99998720099515e-08
Cluster predicted: 0, Cosine Similarity: 1.3970334977742027e-07
Cluster predicted: 0, Cosine Similarity: 4.081773330444349e-08
Cluster predicted: 2, Cosine Similarity: 2.5543142267991925e-08
Cluster predicted: 3, Cosine Similarity: 3.604468135076644e-08
Cluster predicted: 3, Cosine Similarity: 5.161447191870394e-08
Cluster predicted: 4, Cosine Similarity: 3.0640152659522357e-07
Cluster predicted: 0, Cosine Similarity: 4.1215748480816217e-08
Cluster predicted: 0, Cosine Similarity: 3.610082921490232e-08
Cluster predicted: 0, Cosine Similarity: 4.2426731017286556e-08
Cluster predicted: 0, Cosine Similarity: 1.7405121821578717e-07
Cluster predicted: 4, Cosine Similarity: 9.277853496580946e-08
Cluster predicted: 2, Cosine Similarity: 8.135963835975701e-08
Cluster predicted: 0, Cosine Similarity: 1.8467060358684506e-07
Cluster predicted: 0, Cosine Similarity: 3.4851569481464395e-06
Cluster predicted: 4, Cosine Similarity: 1.6530844504369924e-07
Cluster predicted: 4, Cosine Similarity: 7.531824541828769e-08
Cluster predicted: 4, Cosine Similarity: 2.587010776711196e-07
Cluster predicted: 0, Cosine Similarity: 7.947602209679161e-08
Cluster predicted: 4, Cosine Similarity: 2.6479226233888653e-07
Cluster predicted: 0, Cosine Similarity: 6.752709647006583e-08
Cluster predicted: 4, Cosine Similarity: 8.682012808680639e-08
Cluster predicted: 2, Cosine Similarity: 5.2978369136091885e-08
Cluster predicted: 4, Cosine Similarity: 1.8837859938614088e-08
Cluster predicted: 4, Cosine Similarity: 7.10693536243312e-08
Cluster predicted: 0, Cosine Similarity: 1.841032204463744e-08
Cluster predicted: 0, Cosine Similarity: 3.634641310146236e-08
Cluster predicted: 4, Cosine Similarity: 6.096163840396684e-08
Cluster predicted: 0, Cosine Similarity: 4.1444395582068694e-08
Cluster predicted: 3, Cosine Similarity: 1.501930191327716e-08
Cluster predicted: 4, Cosine Similarity: 1.2497716750381471e-08
Cluster predicted: 0, Cosine Similarity: 1.229528359125709e-07
Cluster predicted: 0, Cosine Similarity: 1.0684592133269888e-07
Cluster predicted: 1, Cosine Similarity: 3.211813629500071e-07
Cluster predicted: 4, Cosine Similarity: 9.675689180976832e-07
Cluster predicted: 4, Cosine Similarity: 1.1110188069540783e-07
Cluster predicted: 0, Cosine Similarity: 8.000106355154912e-08
Cluster predicted: 3, Cosine Similarity: 7.038038307705108e-08
Cluster predicted: 0, Cosine Similarity: 6.270010310949203e-08
Cluster predicted: 0, Cosine Similarity: 4.623328107022928e-07
Cluster predicted: 0, Cosine Similarity: 5.722104223782054e-08
Cluster predicted: 0, Cosine Similarity: 8.241456017632487e-08
Cluster predicted: 0, Cosine Similarity: 7.977241645029665e-08
Cluster predicted: 0, Cosine Similarity: 1.5387946705747169e-07
Cluster predicted: 0, Cosine Similarity: 2.4101093121231543e-08
Cluster predicted: 2, Cosine Similarity: 8.199348668469497e-06
Cluster predicted: 3, Cosine Similarity: 2.0088727126221784e-08
Cluster predicted: 0, Cosine Similarity: 1.8706716398142476e-08
Cluster predicted: 0, Cosine Similarity: 6.862798973550355e-08
Cluster predicted: 0, Cosine Similarity: 6.308964994872213e-08
Cluster predicted: 0, Cosine Similarity: 2.0645135256902591e-07
Cluster predicted: 2, Cosine Similarity: 5.6762538336307955e-08
Cluster predicted: 4, Cosine Similarity: 1.4881079501982697e-07
Cluster predicted: 4, Cosine Similarity: 5.312774420396238e-08
Cluster predicted: 0, Cosine Similarity: 6.439378497091752e-08
Cluster predicted: 0, Cosine Similarity: 6.148065212485676e-08
Cluster predicted: 2, Cosine Similarity: 1.0311861142753287e-07
Cluster predicted: 3, Cosine Similarity: 3.604468135076644e-08
Cluster predicted: 0, Cosine Similarity: 7.677459956667576e-08
Cluster predicted: 4, Cosine Similarity: 9.638013465096407e-08
Cluster predicted: 4, Cosine Similarity: 1.9719173049992378e-07
Cluster predicted: 2, Cosine Similarity: 1.4758259980762745e-07
Cluster predicted: 2, Cosine Similarity: 7.568338444841061e-08
Cluster predicted: 3, Cosine Similarity: 4.8886677483928054e-08
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 0, Cosine Similarity: 3.699001227541743e-08
Cluster predicted: 3, Cosine Similarity: 2.1239001890549503e-08
Cluster predicted: 3, Cosine Similarity: 7.065151930163438e-08
Cluster predicted: 0, Cosine Similarity: 7.57668587869631e-08
Cluster predicted: 2, Cosine Similarity: 2.7624435339212994e-07
Cluster predicted: 0, Cosine Similarity: 1.0698988428536893e-07
Cluster predicted: 0, Cosine Similarity: 9.622653596164099e-08
Cluster predicted: 4, Cosine Similarity: 2.1885112022612674e-07
Cluster predicted: 0, Cosine Similarity: 6.962641517782586e-07
Cluster predicted: 2, Cosine Similarity: 1.1730924598385428e-07
Cluster predicted: 0, Cosine Similarity: 5.867760866085092e-08
Cluster predicted: 0, Cosine Similarity: 1.6188211393597385e-07
Cluster predicted: 4, Cosine Similarity: 1.1571591151948724e-07
Cluster predicted: 0, Cosine Similarity: 2.7722184980927267e-07
Cluster predicted: 0, Cosine Similarity: 1.680555843375231e-07
Cluster predicted: 0, Cosine Similarity: 1.4928112079815037e-07
Cluster predicted: 0, Cosine Similarity: 5.049712514804128e-08
Cluster predicted: 0, Cosine Similarity: 1.276782083392547e-07
Cluster predicted: 2, Cosine Similarity: 5.581649609176509e-08
Cluster predicted: 4, Cosine Similarity: 3.8917192979326387e-07
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 0, Cosine Similarity: 1.0216289092479514e-07
Cluster predicted: 0, Cosine Similarity: 7.083824460352162e-08
Cluster predicted: 2, Cosine Similarity: 6.054670764754633e-08
Cluster predicted: 0, Cosine Similarity: 3.375507984237913e-08
Cluster predicted: 0, Cosine Similarity: 1.1970943514238996e-07
Cluster predicted: 2, Cosine Similarity: 5.108628453598385e-08
Cluster predicted: 4, Cosine Similarity: 5.173357664478573e-08
Cluster predicted: 0, Cosine Similarity: 1.615772515783931e-08
Cluster predicted: 2, Cosine Similarity: 1.1636320362828911e-07
Cluster predicted: 4, Cosine Similarity: 3.4605231680195914e-08
Cluster predicted: 0, Cosine Similarity: 5.770374156277569e-08
Cluster predicted: 0, Cosine Similarity: 3.736939696574737e-07
Cluster predicted: 3, Cosine Similarity: 5.98389364725449e-08
Cluster predicted: 0, Cosine Similarity: 3.8498235932493685e-07
Cluster predicted: 2, Cosine Similarity: 3.0273353823773164e-08
Cluster predicted: 2, Cosine Similarity: 2.743522687920219e-07
Cluster predicted: 4, Cosine Similarity: 1.0162486041576102e-07
Cluster predicted: 2, Cosine Similarity: 9.460423067153556e-08
Cluster predicted: 0, Cosine Similarity: 1.1828674240099701e-07
Cluster predicted: 2, Cosine Similarity: 1.0595673827218377e-07
Cluster predicted: 0, Cosine Similarity: 2.208561169503298e-07
Cluster predicted: 0, Cosine Similarity: 7.223553211144207e-08
Cluster predicted: 3, Cosine Similarity: 2.6735672009614575e-08
Cluster predicted: 1, Cosine Similarity: 7.220510228833632e-06
Cluster predicted: 0, Cosine Similarity: 5.783923617830311e-08
Cluster predicted: 0, Cosine Similarity: 8.900298265057671e-08
Cluster predicted: 0, Cosine Similarity: 3.2518692072436295e-08
Cluster predicted: 2, Cosine Similarity: 5.108628453598385e-08
Cluster predicted: 2, Cosine Similarity: 1.3906821905163014e-07
Cluster predicted: 4, Cosine Similarity: 1.527775338461268e-07
Cluster predicted: 2, Cosine Similarity: 8.930639371351745e-07
Cluster predicted: 0, Cosine Similarity: 3.322157005047899e-08
Cluster predicted: 0, Cosine Similarity: 3.6477673448498393e-07
Cluster predicted: 0, Cosine Similarity: 3.1480465068067787e-07
Cluster predicted: 0, Cosine Similarity: 2.9172823512446655e-07
Cluster predicted: 2, Cosine Similarity: 7.568338444841061e-08
Cluster predicted: 4, Cosine Similarity: 2.516140584774007e-08
Cluster predicted: 0, Cosine Similarity: 2.53086883117426e-07
Cluster predicted: 3, Cosine Similarity: 3.52230565825451e-08
Cluster predicted: 3, Cosine Similarity: 5.977320649552809e-08
Cluster predicted: 0, Cosine Similarity: 4.882376750670048e-07
Cluster predicted: 4, Cosine Similarity: 1.0960812968363598e-07
Cluster predicted: 0, Cosine Similarity: 6.163308352569175e-08
Cluster predicted: 0, Cosine Similarity: 5.773761524441312e-08
Cluster predicted: 0, Cosine Similarity: 6.640079808217791e-08
Cluster predicted: 0, Cosine Similarity: 1.0153622864716993e-07
Cluster predicted: 0, Cosine Similarity: 1.0018975149517217e-07
Cluster predicted: 4, Cosine Similarity: 4.9675519697700565e-08
Cluster predicted: 0, Cosine Similarity: 8.392193706630025e-08
Cluster predicted: 3, Cosine Similarity: 1.3934757125966257e-08
Cluster predicted: 0, Cosine Similarity: 3.728810022973761e-07
Cluster predicted: 0, Cosine Similarity: 1.0240000636319024e-07
Cluster predicted: 2, Cosine Similarity: 6.906108840354364e-08
Cluster predicted: 4, Cosine Similarity: 1.9460256217573857e-07
Cluster predicted: 0, Cosine Similarity: 1.848907822399326e-07
Cluster predicted: 0, Cosine Similarity: 2.0579928505792822e-07
Cluster predicted: 0, Cosine Similarity: 8.976513954372933e-08
Cluster predicted: 2, Cosine Similarity: 1.8920846112102652e-08
Cluster predicted: 1, Cosine Similarity: 3.6312257188919617e-06
Cluster predicted: 0, Cosine Similarity: 2.167065960012593e-08
Cluster predicted: 2, Cosine Similarity: 3.4057523023989233e-08
Cluster predicted: 2, Cosine Similarity: 1.1825528822839715e-07
Cluster predicted: 4, Cosine Similarity: 1.049609043013433e-07
Cluster predicted: 4, Cosine Similarity: 5.223149368571711e-08
Cluster predicted: 4, Cosine Similarity: 1.3143349164579377e-07
Cluster predicted: 0, Cosine Similarity: 1.126213764468531e-07
Cluster predicted: 0, Cosine Similarity: 1.9324910227602743e-08
Cluster predicted: 4, Cosine Similarity: 1.559642026638386e-07
Cluster predicted: 0, Cosine Similarity: 3.13669883844625e-07
Cluster predicted: 4, Cosine Similarity: 9.631374564555983e-08
Cluster predicted: 1, Cosine Similarity: 4.3044924935564666e-07
Cluster predicted: 0, Cosine Similarity: 2.6722912671584e-07
Cluster predicted: 0, Cosine Similarity: 4.9582536965075974e-08
Cluster predicted: 0, Cosine Similarity: 1.7916613748170107e-07
Cluster predicted: 0, Cosine Similarity: 9.732742911605641e-08
Cluster predicted: 2, Cosine Similarity: 7.568338444841061e-08
Cluster predicted: 2, Cosine Similarity: 3.50035653795544e-08
Cluster predicted: 0, Cosine Similarity: 1.0418684071034079e-07
Cluster predicted: 0, Cosine Similarity: 1.0434774055934071e-07
Cluster predicted: 0, Cosine Similarity: 3.5509734264138615e-07
Cluster predicted: 0, Cosine Similarity: 3.15278881890535e-08
Cluster predicted: 2, Cosine Similarity: 6.877727566356739e-07
Cluster predicted: 0, Cosine Similarity: 2.526973363892182e-08
Cluster predicted: 0, Cosine Similarity: 1.9296117637068733e-07
Cluster predicted: 0, Cosine Similarity: 2.509189700461434e-08
Cluster predicted: 2, Cosine Similarity: 5.6762538336307955e-08
Cluster predicted: 2, Cosine Similarity: 8.987401911575432e-08
Cluster predicted: 2, Cosine Similarity: 2.838126922366513e-08
Cluster predicted: 3, Cosine Similarity: 2.8000974316988447e-08
Cluster predicted: 0, Cosine Similarity: 1.6365201149781683e-07
Cluster predicted: 4, Cosine Similarity: 1.255248764930883e-07
Cluster predicted: 0, Cosine Similarity: 7.776540345005856e-08
Cluster predicted: 2, Cosine Similarity: 2.3461849196770856e-07
Cluster predicted: 3, Cosine Similarity: 4.096621408766765e-08
Cluster predicted: 2, Cosine Similarity: 6.338483449219723e-08
Cluster predicted: 0, Cosine Similarity: 9.92836316493495e-08
Cluster predicted: 3, Cosine Similarity: 2.5437504769243446e-08
Cluster predicted: 2, Cosine Similarity: 7.852151140408381e-08
Cluster predicted: 4, Cosine Similarity: 7.312741057141636e-08
Cluster predicted: 0, Cosine Similarity: 9.491393249128066e-08
Cluster predicted: 0, Cosine Similarity: 8.303275411680744e-08
Cluster predicted: 3, Cosine Similarity: 4.069507786308435e-08
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 1.072693417425441e-07
Cluster predicted: 4, Cosine Similarity: 2.0932430822018233e-07
Cluster predicted: 4, Cosine Similarity: 7.223115994214879e-08
Cluster predicted: 4, Cosine Similarity: 9.573284254216219e-08
Cluster predicted: 0, Cosine Similarity: 4.423050226076697e-08
Cluster predicted: 0, Cosine Similarity: 1.0450017184915339e-08
Cluster predicted: 2, Cosine Similarity: 2.743522686809996e-08
Cluster predicted: 0, Cosine Similarity: 3.161257222661362e-08
Cluster predicted: 0, Cosine Similarity: 1.5844393974617788e-07
Cluster predicted: 0, Cosine Similarity: 1.541165824958668e-07
Cluster predicted: 0, Cosine Similarity: 1.2840649155076989e-07
Cluster predicted: 0, Cosine Similarity: 1.0533007599455857e-07
Cluster predicted: 0, Cosine Similarity: 7.104995480844423e-08
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 0, Cosine Similarity: 8.006034235563675e-08
Cluster predicted: 2, Cosine Similarity: 4.730211533576778e-08
Cluster predicted: 3, Cosine Similarity: 2.1041811959499057e-08
Cluster predicted: 0, Cosine Similarity: 3.341041556748081e-07
Cluster predicted: 2, Cosine Similarity: 4.635607298020261e-08
Cluster predicted: 0, Cosine Similarity: 2.8538539642575245e-08
Cluster predicted: 0, Cosine Similarity: 1.5658935803575247e-07
Cluster predicted: 4, Cosine Similarity: 1.490763508193993e-07
Cluster predicted: 0, Cosine Similarity: 7.438650806435021e-08
Cluster predicted: 0, Cosine Similarity: 8.14576299745795e-08
Cluster predicted: 0, Cosine Similarity: 2.3548105998472124e-07
Cluster predicted: 0, Cosine Similarity: 4.191015812171628e-08
Cluster predicted: 3, Cosine Similarity: 7.80543589762317e-08
Cluster predicted: 2, Cosine Similarity: 3.3111480668424065e-08
Cluster predicted: 2, Cosine Similarity: 6.906108840354364e-08
Cluster predicted: 0, Cosine Similarity: 2.3686141048528953e-08
Cluster predicted: 1, Cosine Similarity: 0.2222019134195704
Cluster predicted: 0, Cosine Similarity: 5.469745623098987e-08
Cluster predicted: 0, Cosine Similarity: 1.1986186654322495e-07
Cluster predicted: 0, Cosine Similarity: 4.552616883479743e-08
Cluster predicted: 2, Cosine Similarity: 1.0974090747239984e-07
Cluster predicted: 0, Cosine Similarity: 1.6522713564004476e-07
Cluster predicted: 0, Cosine Similarity: 6.420747999946741e-08
Cluster predicted: 0, Cosine Similarity: 1.3894119288426765e-07
Cluster predicted: 0, Cosine Similarity: 4.018006203310165e-07
Cluster predicted: 0, Cosine Similarity: 3.292432887436192e-07
Cluster predicted: 0, Cosine Similarity: 7.606325314046813e-08
Cluster predicted: 0, Cosine Similarity: 1.3347906879790372e-07
Cluster predicted: 4, Cosine Similarity: 1.252261263573473e-07
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 0, Cosine Similarity: 6.022732756960636e-08
Cluster predicted: 2, Cosine Similarity: 1.343380074958489e-07
Cluster predicted: 2, Cosine Similarity: 1.3244592311778547e-08
Cluster predicted: 0, Cosine Similarity: 1.3684949573544714e-07
Cluster predicted: 0, Cosine Similarity: 3.678677040763745e-08
Cluster predicted: 3, Cosine Similarity: 1.6014289594412645e-07
Cluster predicted: 4, Cosine Similarity: 9.397851492565223e-07
Cluster predicted: 0, Cosine Similarity: 9.768310227364907e-08
Cluster predicted: 0, Cosine Similarity: 3.216809991712921e-07
Cluster predicted: 2, Cosine Similarity: 1.2109341518407035e-07
Cluster predicted: 4, Cosine Similarity: 1.394001637455844e-07
Cluster predicted: 0, Cosine Similarity: 3.0274563522780795e-08
Cluster predicted: 2, Cosine Similarity: 4.351794613555171e-08
Cluster predicted: 2, Cosine Similarity: 9.93344421162945e-08
Cluster predicted: 4, Cosine Similarity: 2.106188923267638e-08
Cluster predicted: 0, Cosine Similarity: 1.4447953267104907e-07
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 4, Cosine Similarity: 4.4777676111440456e-07
Cluster predicted: 4, Cosine Similarity: 3.689564997966244e-08
Cluster predicted: 3, Cosine Similarity: 9.07073813838366e-08
Cluster predicted: 0, Cosine Similarity: 6.385180684187475e-08
Cluster predicted: 4, Cosine Similarity: 1.001477065720735e-07
Cluster predicted: 0, Cosine Similarity: 1.028573004546729e-07
Cluster predicted: 3, Cosine Similarity: 1.903704738293044e-08
Cluster predicted: 0, Cosine Similarity: 8.192339240320479e-08
Cluster predicted: 4, Cosine Similarity: 8.102769366580276e-08
Cluster predicted: 2, Cosine Similarity: 1.1636320362828911e-07
Cluster predicted: 3, Cosine Similarity: 5.674962710866538e-08
Cluster predicted: 4, Cosine Similarity: 9.991534533071444e-08
Cluster predicted: 0, Cosine Similarity: 2.836747777790194e-07
Cluster predicted: 4, Cosine Similarity: 1.146370913751582e-07
Cluster predicted: 0, Cosine Similarity: 4.1444395582068694e-08
Cluster predicted: 0, Cosine Similarity: 1.2847423891404475e-07
Cluster predicted: 0, Cosine Similarity: 1.3405492049756162e-08
Cluster predicted: 1, Cosine Similarity: 0.5625675153647409
Cluster predicted: 2, Cosine Similarity: 1.3055383829563283e-07
Cluster predicted: 0, Cosine Similarity: 7.907800692041889e-08
Cluster predicted: 0, Cosine Similarity: 1.1019941137391243e-07
Cluster predicted: 4, Cosine Similarity: 6.421469622708287e-08
Cluster predicted: 0, Cosine Similarity: 1.2881297528632984e-07
Cluster predicted: 0, Cosine Similarity: 2.1828172047655414e-07
Cluster predicted: 0, Cosine Similarity: 1.1720278592086686e-07
Cluster predicted: 4, Cosine Similarity: 7.49199117189292e-08
Cluster predicted: 0, Cosine Similarity: 1.0848879272717937e-07
Cluster predicted: 0, Cosine Similarity: 2.1001655292618437e-07
Cluster predicted: 0, Cosine Similarity: 5.223314913926913e-08
Cluster predicted: 0, Cosine Similarity: 1.8968390191886897e-07
Cluster predicted: 0, Cosine Similarity: 1.2787298175886974e-07
Cluster predicted: 0, Cosine Similarity: 6.762871740395582e-08
Cluster predicted: 0, Cosine Similarity: 4.535680064865488e-08
Cluster predicted: 0, Cosine Similarity: 2.4315343882808094e-07
Cluster predicted: 2, Cosine Similarity: 1.0028048447185967e-07
Cluster predicted: 0, Cosine Similarity: 8.480265167865042e-08
Cluster predicted: 0, Cosine Similarity: 8.87658672121816e-08
Cluster predicted: 2, Cosine Similarity: 2.743522686809996e-08
Cluster predicted: 0, Cosine Similarity: 1.4000821246806794e-07
Cluster predicted: 2, Cosine Similarity: 7.662942680397578e-08
Cluster predicted: 4, Cosine Similarity: 1.75067618402025e-07
Cluster predicted: 0, Cosine Similarity: 1.4088892708041811e-07
Cluster predicted: 0, Cosine Similarity: 6.478333181014762e-08
Cluster predicted: 0, Cosine Similarity: 2.2864705351288706e-08
Cluster predicted: 0, Cosine Similarity: 3.7980816158800224e-08
Cluster predicted: 4, Cosine Similarity: 3.326085584731686e-08
Cluster predicted: 2, Cosine Similarity: 4.257190377998654e-08
Cluster predicted: 0, Cosine Similarity: 1.2743262456371696e-07
Cluster predicted: 2, Cosine Similarity: 8.135963835975701e-08
Cluster predicted: 2, Cosine Similarity: 1.2393154213974356e-07
Cluster predicted: 2, Cosine Similarity: 2.270501531231872e-08
Cluster predicted: 3, Cosine Similarity: 4.124556651774469e-08
Cluster predicted: 4, Cosine Similarity: 4.697846925916238e-07
Cluster predicted: 2, Cosine Similarity: 3.7841692224205303e-08
Cluster predicted: 0, Cosine Similarity: 6.43345061668299e-08
Cluster predicted: 2, Cosine Similarity: 1.1636320362828911e-07
Cluster predicted: 3, Cosine Similarity: 1.2077063404891675e-07
Cluster predicted: 2, Cosine Similarity: 1.229854997841784e-07
Cluster predicted: 4, Cosine Similarity: 1.691424059702129e-07
Cluster predicted: 4, Cosine Similarity: 5.8140108727400275e-08
Cluster predicted: 4, Cosine Similarity: 7.691157977163243e-08
Cluster predicted: 0, Cosine Similarity: 1.1135111510629514e-07
Cluster predicted: 0, Cosine Similarity: 6.696818144469319e-08
Cluster predicted: 2, Cosine Similarity: 1.6744948816427296e-07
Cluster predicted: 0, Cosine Similarity: 5.181819706656654e-08
Cluster predicted: 0, Cosine Similarity: 6.710367606022061e-08
Cluster predicted: 4, Cosine Similarity: 2.736883797371803e-08
Cluster predicted: 1, Cosine Similarity: 0.0004138272860308989
Cluster predicted: 4, Cosine Similarity: 1.4889378119331553e-07
Cluster predicted: 3, Cosine Similarity: 3.05973086867084e-08
Cluster predicted: 2, Cosine Similarity: 2.0812930712210687e-08
Cluster predicted: 2, Cosine Similarity: 1.3339196514028373e-07
Cluster predicted: 0, Cosine Similarity: 1.3159908196502812e-07
Cluster predicted: 0, Cosine Similarity: 1.1803269006627204e-07
Cluster predicted: 3, Cosine Similarity: 2.708075441670843e-08
Cluster predicted: 0, Cosine Similarity: 1.0248469040075037e-07
Cluster predicted: 0, Cosine Similarity: 3.4413075244987823e-07
Cluster predicted: 2, Cosine Similarity: 1.0974090747239984e-07
Cluster predicted: 4, Cosine Similarity: 6.424789067427383e-08
Cluster predicted: 0, Cosine Similarity: 9.228025710239507e-08
Cluster predicted: 2, Cosine Similarity: 5.581649609176509e-08
Cluster predicted: 4, Cosine Similarity: 1.5068628245185067e-07
Cluster predicted: 4, Cosine Similarity: 1.516157275283092e-07
Cluster predicted: 2, Cosine Similarity: 2.450249573238139e-07
Cluster predicted: 0, Cosine Similarity: 5.6924647884315505e-08
Cluster predicted: 0, Cosine Similarity: 2.1663038063390871e-07
Cluster predicted: 0, Cosine Similarity: 7.027932957814897e-08
Cluster predicted: 4, Cosine Similarity: 1.8739936280542935e-07
Cluster predicted: 4, Cosine Similarity: 1.6431261129490338e-08
Cluster predicted: 3, Cosine Similarity: 3.0276875007118065e-08
Cluster predicted: 4, Cosine Similarity: 8.5757905221584e-08
Cluster predicted: 0, Cosine Similarity: 1.248074176229963e-07
Cluster predicted: 0, Cosine Similarity: 1.4854436913847024e-07
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 2, Cosine Similarity: 4.730211533576778e-08
Cluster predicted: 0, Cosine Similarity: 3.826027361597539e-08
Cluster predicted: 3, Cosine Similarity: 5.0998253287026785e-08
Cluster predicted: 0, Cosine Similarity: 2.2125413212670253e-07
Cluster predicted: 2, Cosine Similarity: 6.243879224765436e-08
Cluster predicted: 0, Cosine Similarity: 2.8186253819839635e-07
Cluster predicted: 2, Cosine Similarity: 4.446398838009458e-08
Cluster predicted: 4, Cosine Similarity: 6.91440745770322e-08
Cluster predicted: 0, Cosine Similarity: 1.3659544351174446e-07
Cluster predicted: 1, Cosine Similarity: 4.0727121286554535e-07
Cluster predicted: 4, Cosine Similarity: 2.3815370264745894e-07
Cluster predicted: 0, Cosine Similarity: 9.333880834905273e-08
Cluster predicted: 2, Cosine Similarity: 2.573235072800273e-07
Cluster predicted: 4, Cosine Similarity: 2.2753147332910117e-07
Cluster predicted: 2, Cosine Similarity: 2.3584834695222767e-06
Cluster predicted: 0, Cosine Similarity: 7.133788071378433e-08
Cluster predicted: 0, Cosine Similarity: 7.671532065156583e-08
Cluster predicted: 0, Cosine Similarity: 2.0818737644923146e-07
Cluster predicted: 2, Cosine Similarity: 2.4407891496824874e-07
Cluster predicted: 2, Cosine Similarity: 1.1352507678363821e-07
Cluster predicted: 0, Cosine Similarity: 1.0409368822461573e-07
Cluster predicted: 0, Cosine Similarity: 7.071121843615913e-08
Cluster predicted: 4, Cosine Similarity: 7.412324454225683e-08
Cluster predicted: 3, Cosine Similarity: 4.128664776725799e-08
Cluster predicted: 0, Cosine Similarity: 7.789242961742104e-08
Cluster predicted: 0, Cosine Similarity: 2.560847001120692e-08
Cluster predicted: 4, Cosine Similarity: 1.6112594225514698e-07
Cluster predicted: 0, Cosine Similarity: 5.308845851814681e-08
Cluster predicted: 0, Cosine Similarity: 8.763956882429369e-08
Cluster predicted: 2, Cosine Similarity: 1.9393867278783006e-07
Cluster predicted: 0, Cosine Similarity: 2.6294411203942047e-08
Cluster predicted: 0, Cosine Similarity: 1.0242541159666274e-07
Cluster predicted: 0, Cosine Similarity: 1.1799034804749198e-07
Cluster predicted: 0, Cosine Similarity: 5.24872013629718e-08
Cluster predicted: 0, Cosine Similarity: 5.6399606540580294e-08
Cluster predicted: 0, Cosine Similarity: 5.4045388719892173e-08
Cluster predicted: 4, Cosine Similarity: 2.0668534805867722e-07
Cluster predicted: 0, Cosine Similarity: 4.61697680087525e-08
Cluster predicted: 2, Cosine Similarity: 1.7028761511994617e-08
Cluster predicted: 2, Cosine Similarity: 9.64963152716436e-08
Cluster predicted: 0, Cosine Similarity: 1.0111280812630241e-07
Cluster predicted: 2, Cosine Similarity: 1.456905152075194e-07
Cluster predicted: 3, Cosine Similarity: 9.919476595676713e-08
Cluster predicted: 3, Cosine Similarity: 3.8246635969407805e-08
Cluster predicted: 2, Cosine Similarity: 2.0812930712210687e-08
Cluster predicted: 0, Cosine Similarity: 1.6792008983301798e-07
Cluster predicted: 0, Cosine Similarity: 4.6737151482290074e-08
Cluster predicted: 2, Cosine Similarity: 1.8637033438739792e-07
Cluster predicted: 2, Cosine Similarity: 5.960066529198116e-08
Cluster predicted: 2, Cosine Similarity: 9.176610371586236e-08
Cluster predicted: 4, Cosine Similarity: 2.2383029019135137e-07
Cluster predicted: 0, Cosine Similarity: 8.181330313217217e-08
Cluster predicted: 0, Cosine Similarity: 6.646007688626554e-08
Cluster predicted: 2, Cosine Similarity: 1.1257903442807304e-07
Cluster predicted: 0, Cosine Similarity: 1.718409633477691e-07
Cluster predicted: 2, Cosine Similarity: 9.082006136029719e-08
Cluster predicted: 0, Cosine Similarity: 1.588673601560231e-07
Cluster predicted: 0, Cosine Similarity: 9.024783886868448e-08
Cluster predicted: 4, Cosine Similarity: 8.429734865700311e-08
Cluster predicted: 0, Cosine Similarity: 8.388806338466281e-08
Cluster predicted: 4, Cosine Similarity: 2.375230078177637e-07
Cluster predicted: 2, Cosine Similarity: 1.6707107128866028e-06
Cluster predicted: 0, Cosine Similarity: 1.1543288835902388e-07
Cluster predicted: 2, Cosine Similarity: 1.2109341518407035e-07
Cluster predicted: 2, Cosine Similarity: 4.730211533576778e-08
Cluster predicted: 0, Cosine Similarity: 1.3690877465055706e-07
Cluster predicted: 0, Cosine Similarity: 6.828925336321845e-08
Cluster predicted: 2, Cosine Similarity: 8.514380755997308e-07
Cluster predicted: 0, Cosine Similarity: 7.298075210826482e-08
Cluster predicted: 0, Cosine Similarity: 2.8267550522542706e-08
Cluster predicted: 0, Cosine Similarity: 4.481566934222414e-07
Cluster predicted: 0, Cosine Similarity: 4.461158065183213e-08
Cluster predicted: 0, Cosine Similarity: 3.5294806028929315e-06
Cluster predicted: 4, Cosine Similarity: 8.202352763664322e-08
Cluster predicted: 0, Cosine Similarity: 4.915064810706582e-08
Cluster predicted: 2, Cosine Similarity: 1.3812217669606497e-07
Cluster predicted: 2, Cosine Similarity: 5.865462293641599e-08
Cluster predicted: 0, Cosine Similarity: 8.942640317144424e-08
Cluster predicted: 2, Cosine Similarity: 1.5136676911886582e-08
Cluster predicted: 0, Cosine Similarity: 1.3743381599429227e-07
Cluster predicted: 3, Cosine Similarity: 2.3761390188781206e-08
Cluster predicted: 0, Cosine Similarity: 3.207579425401619e-07
Cluster predicted: 0, Cosine Similarity: 9.755607621730888e-08
Cluster predicted: 4, Cosine Similarity: 4.8530310547967304e-08
Cluster predicted: 2, Cosine Similarity: 2.7340622654747904e-07
Cluster predicted: 2, Cosine Similarity: 2.100213919442595e-07
Cluster predicted: 0, Cosine Similarity: 8.516679328440802e-08
Cluster predicted: 3, Cosine Similarity: 2.551966726827004e-08
Cluster predicted: 0, Cosine Similarity: 8.478571478232055e-08
Cluster predicted: 2, Cosine Similarity: 5.7708580691873124e-08
Cluster predicted: 0, Cosine Similarity: 2.1458102561489056e-07
Cluster predicted: 0, Cosine Similarity: 7.343804619974748e-08
Cluster predicted: 0, Cosine Similarity: 8.732623768548109e-08
Cluster predicted: 2, Cosine Similarity: 9.555027291607843e-08
Cluster predicted: 4, Cosine Similarity: 5.5783301644574124e-08
Cluster predicted: 2, Cosine Similarity: 2.9327311468207995e-08
Cluster predicted: 0, Cosine Similarity: 4.456077018488713e-08
Cluster predicted: 2, Cosine Similarity: 7.189921524819454e-08
Cluster predicted: 2, Cosine Similarity: 8.041359600419185e-08
Cluster predicted: 0, Cosine Similarity: 1.9832167952316127e-07
Cluster predicted: 4, Cosine Similarity: 9.43718693191542e-08
Cluster predicted: 0, Cosine Similarity: 4.919299012584588e-08
Cluster predicted: 0, Cosine Similarity: 4.8651011996803106e-08
Cluster predicted: 0, Cosine Similarity: 1.0582124376767865e-07
Cluster predicted: 2, Cosine Similarity: 5.960066529198116e-08
Cluster predicted: 3, Cosine Similarity: 6.949302822079062e-08
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 2, Cosine Similarity: 1.1541716138374625e-07
Cluster predicted: 0, Cosine Similarity: 7.215084796285964e-08
Cluster predicted: 2, Cosine Similarity: 1.7123365747551134e-07
Cluster predicted: 4, Cosine Similarity: 1.3294383982564995e-07
Cluster predicted: 4, Cosine Similarity: 8.686991981310399e-08
Cluster predicted: 2, Cosine Similarity: 2.1758973067775855e-08
Cluster predicted: 2, Cosine Similarity: 3.4341335719556554e-07
Cluster predicted: 3, Cosine Similarity: 1.1313773917720482e-08
Cluster predicted: 2, Cosine Similarity: 3.3111480668424065e-08
Cluster predicted: 2, Cosine Similarity: 1.986688846766782e-08
Cluster predicted: 2, Cosine Similarity: 4.446398838009458e-08
Cluster predicted: 3, Cosine Similarity: 4.98972759777061e-08
Cluster predicted: 1, Cosine Similarity: 3.6157736945652275e-06
Cluster predicted: 0, Cosine Similarity: 4.123268537714608e-08
Cluster predicted: 0, Cosine Similarity: 2.7878003683312613e-08
Cluster predicted: 0, Cosine Similarity: 6.014264353204624e-08
Cluster predicted: 4, Cosine Similarity: 1.2383195868714836e-07
Cluster predicted: 0, Cosine Similarity: 7.983169536540657e-08
Cluster predicted: 0, Cosine Similarity: 6.430063248519247e-08
Cluster predicted: 3, Cosine Similarity: 8.009198859681987e-08
Cluster predicted: 4, Cosine Similarity: 9.515193932774224e-08
Cluster predicted: 2, Cosine Similarity: 8.571143295110772e-07
Cluster predicted: 0, Cosine Similarity: 1.0096884517363236e-07
Cluster predicted: 0, Cosine Similarity: 2.87925919328913e-07
Cluster predicted: 0, Cosine Similarity: 8.87912724456541e-08
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 0, Cosine Similarity: 2.700745123096482e-07
Cluster predicted: 2, Cosine Similarity: 3.0273353823773164e-08
Cluster predicted: 0, Cosine Similarity: 1.006131720160397e-07
Cluster predicted: 2, Cosine Similarity: 5.6762538336307955e-08
Cluster predicted: 3, Cosine Similarity: 7.650148803328705e-08
Cluster predicted: 3, Cosine Similarity: 3.802479719983154e-08
Cluster predicted: 4, Cosine Similarity: 8.686991981310399e-08
Cluster predicted: 2, Cosine Similarity: 3.036795802602299e-07
Cluster predicted: 0, Cosine Similarity: 1.463171774851446e-07
Cluster predicted: 0, Cosine Similarity: 1.3180232376619472e-06
Cluster predicted: 2, Cosine Similarity: 5.014024218041868e-08
Cluster predicted: 0, Cosine Similarity: 4.759246075014545e-08
Cluster predicted: 2, Cosine Similarity: 5.960066529198116e-08
Cluster predicted: 0, Cosine Similarity: 2.1729938493031398e-07
Cluster predicted: 0, Cosine Similarity: 4.539067433029231e-08
Cluster predicted: 0, Cosine Similarity: 5.72887894900731e-08
Cluster predicted: 3, Cosine Similarity: 5.720973705880539e-08
Cluster predicted: 3, Cosine Similarity: 4.226438132803878e-08
Cluster predicted: 0, Cosine Similarity: 1.0054542465276484e-07
Cluster predicted: 3, Cosine Similarity: 1.674471405976874e-08
Cluster predicted: 0, Cosine Similarity: 8.79020894961613e-08
Cluster predicted: 0, Cosine Similarity: 2.595567483165695e-08
Cluster predicted: 3, Cosine Similarity: 2.3498470169691643e-08
Cluster predicted: 0, Cosine Similarity: 1.5236362171933138e-07
Cluster predicted: 4, Cosine Similarity: 8.984082466856336e-08
Cluster predicted: 2, Cosine Similarity: 1.7974803823150864e-07
Cluster predicted: 3, Cosine Similarity: 6.526166040909942e-08
Cluster predicted: 2, Cosine Similarity: 4.730211533576778e-08
Cluster predicted: 0, Cosine Similarity: 8.772425286185381e-08
Cluster predicted: 2, Cosine Similarity: 1.929926304322649e-07
Cluster predicted: 4, Cosine Similarity: 6.361719573355629e-08
Cluster predicted: 0, Cosine Similarity: 1.8734662099451072e-07
Cluster predicted: 0, Cosine Similarity: 3.705860633917979e-07
Cluster predicted: 4, Cosine Similarity: 7.108595079241553e-08
Cluster predicted: 4, Cosine Similarity: 1.496406567547126e-07
Cluster predicted: 2, Cosine Similarity: 2.4597099912426756e-08
Cluster predicted: 2, Cosine Similarity: 2.0245305354382737e-07
Cluster predicted: 2, Cosine Similarity: 3.973377682431334e-08
Cluster predicted: 4, Cosine Similarity: 1.672669186492115e-07
Cluster predicted: 4, Cosine Similarity: 1.5988114976384082e-07
Cluster predicted: 2, Cosine Similarity: 9.93344421162945e-08
Cluster predicted: 4, Cosine Similarity: 1.137242435778063e-07
Cluster predicted: 4, Cosine Similarity: 2.1217903234393987e-07
Cluster predicted: 0, Cosine Similarity: 1.1672008659591171e-07
Cluster predicted: 4, Cosine Similarity: 1.1035500513401075e-07
Cluster predicted: 2, Cosine Similarity: 2.5826954963559245e-07
Cluster predicted: 0, Cosine Similarity: 1.5710593104234505e-07
Cluster predicted: 0, Cosine Similarity: 1.2493444379035878e-07
Cluster predicted: 2, Cosine Similarity: 8.892797676018915e-08
Cluster predicted: 0, Cosine Similarity: 1.7853100675591094e-07
Cluster predicted: 2, Cosine Similarity: 9.744235751618646e-08
Cluster predicted: 0, Cosine Similarity: 1.098013961975397e-07
Cluster predicted: 0, Cosine Similarity: 9.495627451006072e-08
Cluster predicted: 0, Cosine Similarity: 3.759973765671276e-08
Cluster predicted: 2, Cosine Similarity: 3.689564997966244e-08
Cluster predicted: 2, Cosine Similarity: 7.048015181476686e-07
Cluster predicted: 4, Cosine Similarity: 1.8391394374095427e-07
Cluster predicted: 3, Cosine Similarity: 8.264724182804883e-08
Cluster predicted: 3, Cosine Similarity: 9.538242673468034e-08
Cluster predicted: 4, Cosine Similarity: 4.2953640155829476e-07
Cluster predicted: 4, Cosine Similarity: 8.799853168373062e-08
Cluster predicted: 2, Cosine Similarity: 1.0974090747239984e-07
Cluster predicted: 0, Cosine Similarity: 7.830738169012363e-08
Cluster predicted: 3, Cosine Similarity: 4.045680668252061e-08
Cluster predicted: 2, Cosine Similarity: 6.243879224765436e-08
Cluster predicted: 2, Cosine Similarity: 1.0728119752556609e-06
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 0, Cosine Similarity: 1.0096884517363236e-07
Cluster predicted: 0, Cosine Similarity: 2.505040181954854e-07
Cluster predicted: 4, Cosine Similarity: 2.103699339173204e-07
Cluster predicted: 0, Cosine Similarity: 3.5514815299730884e-07
Cluster predicted: 4, Cosine Similarity: 7.868748375106094e-08
Cluster predicted: 2, Cosine Similarity: 5.960066529198116e-08
Cluster predicted: 2, Cosine Similarity: 7.379129984830257e-08
Cluster predicted: 0, Cosine Similarity: 3.381435870197791e-07
Cluster predicted: 0, Cosine Similarity: 1.6353345377861928e-07
Cluster predicted: 2, Cosine Similarity: 1.0974090747239984e-07
Cluster predicted: 0, Cosine Similarity: 2.9464983630766994e-07
Cluster predicted: 2, Cosine Similarity: 1.0595673827218377e-07
Cluster predicted: 0, Cosine Similarity: 3.9704137433371756e-07
Cluster predicted: 1, Cosine Similarity: 3.7316638771267563e-06
Cluster predicted: 0, Cosine Similarity: 4.5811554283403666e-07
Cluster predicted: 0, Cosine Similarity: 1.9756799085612897e-08
Cluster predicted: 2, Cosine Similarity: 1.0122652671640253e-07
Cluster predicted: 4, Cosine Similarity: 1.421387072486624e-07
Cluster predicted: 0, Cosine Similarity: 1.2871135435243986e-07
Cluster predicted: 0, Cosine Similarity: 1.3572319734755922e-07
Cluster predicted: 4, Cosine Similarity: 2.742360881713424e-07
Cluster predicted: 0, Cosine Similarity: 2.370646524640918e-07
Cluster predicted: 2, Cosine Similarity: 1.3244592311778547e-08
Cluster predicted: 2, Cosine Similarity: 1.0406465356105343e-08
Cluster predicted: 4, Cosine Similarity: 3.921926261529762e-08
Cluster predicted: 0, Cosine Similarity: 6.786583286455539e-07
Cluster predicted: 0, Cosine Similarity: 7.436110283087771e-08
Cluster predicted: 0, Cosine Similarity: 7.706252547201586e-08
Cluster predicted: 3, Cosine Similarity: 1.536520597422708e-07
Cluster predicted: 0, Cosine Similarity: 1.7101952765052886e-07
Cluster predicted: 0, Cosine Similarity: 6.766259108559325e-08
Cluster predicted: 0, Cosine Similarity: 9.304241399554769e-08
Cluster predicted: 2, Cosine Similarity: 3.878773457977047e-08
Cluster predicted: 0, Cosine Similarity: 4.386636065500937e-08
Cluster predicted: 0, Cosine Similarity: 2.444491055131337e-07
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 0, Cosine Similarity: 1.82358727784937e-07
Cluster predicted: 2, Cosine Similarity: 2.4597099912426756e-08
Cluster predicted: 0, Cosine Similarity: 1.3614661775740444e-07
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 2.5306994622109613e-07
Cluster predicted: 2, Cosine Similarity: 2.6489184623557094e-08
Cluster predicted: 0, Cosine Similarity: 7.48268653705253e-08
Cluster predicted: 0, Cosine Similarity: 2.2263448307136002e-08
Cluster predicted: 0, Cosine Similarity: 1.9743249657366846e-07
Cluster predicted: 4, Cosine Similarity: 1.0213937462477674e-07
Cluster predicted: 2, Cosine Similarity: 7.568338444841061e-08
Cluster predicted: 2, Cosine Similarity: 1.021725690719677e-07
Cluster predicted: 0, Cosine Similarity: 1.8548357094694268e-07
Cluster predicted: 2, Cosine Similarity: 7.379129984830257e-08
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 2.5237553669121837e-07
Cluster predicted: 2, Cosine Similarity: 7.189921524819454e-08
Cluster predicted: 0, Cosine Similarity: 1.1651684483915403e-07
Cluster predicted: 1, Cosine Similarity: 3.613566262550272e-06
Cluster predicted: 0, Cosine Similarity: 4.974343681407589e-08
Cluster predicted: 2, Cosine Similarity: 2.3556453421225143e-07
Cluster predicted: 1, Cosine Similarity: 4.083749288730232e-07
Cluster predicted: 4, Cosine Similarity: 1.3968231671324105e-07
Cluster predicted: 0, Cosine Similarity: 8.251618111021486e-08
Cluster predicted: 0, Cosine Similarity: 3.97761189541157e-08
Cluster predicted: 2, Cosine Similarity: 4.8248157580310647e-08
Cluster predicted: 3, Cosine Similarity: 3.335796816550385e-08
Cluster predicted: 0, Cosine Similarity: 1.6013762205169257e-08
Cluster predicted: 0, Cosine Similarity: 1.75126706469797e-08
Cluster predicted: 0, Cosine Similarity: 7.429335557862515e-08
Cluster predicted: 4, Cosine Similarity: 7.475393948297437e-08
Cluster predicted: 4, Cosine Similarity: 2.2311660918816756e-07
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 4, Cosine Similarity: 1.0613930789826753e-07
Cluster predicted: 4, Cosine Similarity: 7.823935843642715e-08
Cluster predicted: 3, Cosine Similarity: 1.0755069057566402e-08
Cluster predicted: 2, Cosine Similarity: 1.4379843060741138e-07
Cluster predicted: 0, Cosine Similarity: 1.966110608764282e-07
Cluster predicted: 2, Cosine Similarity: 1.7596386892027027e-07
Cluster predicted: 4, Cosine Similarity: 1.4597266817517607e-07
Cluster predicted: 0, Cosine Similarity: 1.1627972940075892e-07
Cluster predicted: 0, Cosine Similarity: 1.1999736104773007e-07
Cluster predicted: 3, Cosine Similarity: 7.472677843178843e-08
Cluster predicted: 4, Cosine Similarity: 5.0090450565143385e-08
Cluster predicted: 2, Cosine Similarity: 5.487045373619992e-08
Cluster predicted: 0, Cosine Similarity: 4.3457336473817776e-07
Cluster predicted: 0, Cosine Similarity: 1.0164631791820256e-07
Cluster predicted: 0, Cosine Similarity: 6.634151916706799e-08
Cluster predicted: 1, Cosine Similarity: 0.3874225478721359
Cluster predicted: 4, Cosine Similarity: 9.890291408076735e-08
Cluster predicted: 2, Cosine Similarity: 4.351794613555171e-08
Cluster predicted: 0, Cosine Similarity: 5.9219586900916e-08
Cluster predicted: 0, Cosine Similarity: 1.2722091424777204e-07
Cluster predicted: 2, Cosine Similarity: 3.1976229952768165e-07
Cluster predicted: 2, Cosine Similarity: 2.4597099912426756e-08
Cluster predicted: 0, Cosine Similarity: 1.4107523205186823e-07
Cluster predicted: 4, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 2, Cosine Similarity: 2.838126922366513e-08
Cluster predicted: 4, Cosine Similarity: 1.482630863636203e-07
Cluster predicted: 1, Cosine Similarity: 0.00026807496276826104
Cluster predicted: 4, Cosine Similarity: 3.110819464247072e-07
Cluster predicted: 2, Cosine Similarity: 1.286617536955248e-07
Cluster predicted: 4, Cosine Similarity: 3.7841692224205303e-08
Cluster predicted: 2, Cosine Similarity: 4.257190377998654e-08
Cluster predicted: 4, Cosine Similarity: 5.4239758906504676e-08
Cluster predicted: 2, Cosine Similarity: 2.9705728421536293e-07
Cluster predicted: 2, Cosine Similarity: 1.0028048447185967e-07
Cluster predicted: 0, Cosine Similarity: 3.9245149641153887e-07
Cluster predicted: 0, Cosine Similarity: 9.491393249128066e-08
Cluster predicted: 3, Cosine Similarity: 7.784895283968751e-08
Cluster predicted: 2, Cosine Similarity: 1.5136676900784352e-07
Cluster predicted: 0, Cosine Similarity: 6.784889605704336e-08
Cluster predicted: 3, Cosine Similarity: 1.814969252666998e-08
Cluster predicted: 2, Cosine Similarity: 2.365105766788389e-08
Cluster predicted: 2, Cosine Similarity: 1.1068694982796501e-07
Cluster predicted: 3, Cosine Similarity: 3.1879043516092054e-08
Cluster predicted: 0, Cosine Similarity: 9.085756436100212e-08
Cluster predicted: 0, Cosine Similarity: 1.4950976778838054e-07
Cluster predicted: 4, Cosine Similarity: 5.6696149441926025e-08
Cluster predicted: 2, Cosine Similarity: 1.4758259980762745e-07
Cluster predicted: 3, Cosine Similarity: 1.483361471432687e-07
Cluster predicted: 2, Cosine Similarity: 2.365105766788389e-08
Cluster predicted: 0, Cosine Similarity: 4.3315914077801665e-08
Cluster predicted: 4, Cosine Similarity: 4.4331210480308414e-08
Cluster predicted: 0, Cosine Similarity: 6.209884606533933e-08
Cluster predicted: 0, Cosine Similarity: 7.133788071378433e-08
Cluster predicted: 4, Cosine Similarity: 4.544820433327601e-07
Cluster predicted: 0, Cosine Similarity: 3.980999252473083e-08
Cluster predicted: 4, Cosine Similarity: 6.132677754511207e-08
Cluster predicted: 4, Cosine Similarity: 1.2099383184249746e-07
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 0, Cosine Similarity: 2.2708039781882405e-07
Cluster predicted: 4, Cosine Similarity: 2.615392046267928e-07
Cluster predicted: 0, Cosine Similarity: 4.946397924587842e-08
Cluster predicted: 0, Cosine Similarity: 2.5142707471559333e-08
Cluster predicted: 3, Cosine Similarity: 6.460436052790897e-08
Cluster predicted: 2, Cosine Similarity: 5.487045373619992e-08
Cluster predicted: 0, Cosine Similarity: 1.171096334351418e-07
Cluster predicted: 2, Cosine Similarity: 9.744235751618646e-08
Cluster predicted: 0, Cosine Similarity: 1.6070500497011864e-07
Cluster predicted: 4, Cosine Similarity: 9.800666345149978e-08
Cluster predicted: 0, Cosine Similarity: 6.646854533443047e-08
Cluster predicted: 1, Cosine Similarity: 3.6223959907211167e-06
Cluster predicted: 3, Cosine Similarity: 1.6218874132611916e-08
Cluster predicted: 4, Cosine Similarity: 5.857163687394973e-08
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 0, Cosine Similarity: 1.2724631948124454e-07
Cluster predicted: 2, Cosine Similarity: 1.2109341518407035e-07
Cluster predicted: 0, Cosine Similarity: 1.3718823188568763e-08
Cluster predicted: 0, Cosine Similarity: 3.184121930566164e-07
Cluster predicted: 3, Cosine Similarity: 2.1222569368539723e-08
Cluster predicted: 0, Cosine Similarity: 1.5494648664127197e-07
Cluster predicted: 4, Cosine Similarity: 5.1899548991762856e-08
Cluster predicted: 3, Cosine Similarity: 1.2915120728429486e-07
Cluster predicted: 0, Cosine Similarity: 1.0915779702358464e-07
Cluster predicted: 2, Cosine Similarity: 9.838839987175163e-08
Cluster predicted: 0, Cosine Similarity: 2.1619849177589856e-07
Cluster predicted: 0, Cosine Similarity: 4.838002287677057e-08
Cluster predicted: 2, Cosine Similarity: 3.8125504941977084e-07
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 3, Cosine Similarity: 8.871083312378403e-08
Cluster predicted: 2, Cosine Similarity: 4.446398838009458e-08
Cluster predicted: 3, Cosine Similarity: 9.359950081666568e-08
Cluster predicted: 2, Cosine Similarity: 3.21654384238812e-08
Cluster predicted: 0, Cosine Similarity: 1.5794430352489286e-07
Cluster predicted: 0, Cosine Similarity: 9.311016124780025e-08
Cluster predicted: 0, Cosine Similarity: 2.75248710490672e-07
Cluster predicted: 0, Cosine Similarity: 4.693192479088282e-08
Cluster predicted: 0, Cosine Similarity: 2.419678615250831e-07
Cluster predicted: 0, Cosine Similarity: 6.32928918165021e-08
Cluster predicted: 2, Cosine Similarity: 6.906108840354364e-08
Cluster predicted: 4, Cosine Similarity: 1.2857876752203623e-07
Cluster predicted: 0, Cosine Similarity: 3.734568543301009e-08
Cluster predicted: 2, Cosine Similarity: 6.527691909230526e-08
Cluster predicted: 0, Cosine Similarity: 3.123149383554846e-08
Cluster predicted: 4, Cosine Similarity: 1.2388175041344596e-07
Cluster predicted: 0, Cosine Similarity: 1.8033477799939135e-07
Cluster predicted: 0, Cosine Similarity: 9.741211326463883e-08
Cluster predicted: 2, Cosine Similarity: 3.7841692224205303e-08
Cluster predicted: 0, Cosine Similarity: 4.3358256096581727e-08
Cluster predicted: 2, Cosine Similarity: 1.0595673827218377e-07
Cluster predicted: 0, Cosine Similarity: 1.5720755197623504e-07
Cluster predicted: 2, Cosine Similarity: 3.973377682431334e-08
Cluster predicted: 0, Cosine Similarity: 1.6001059544024088e-07
Cluster predicted: 0, Cosine Similarity: 9.239034648444999e-08
Cluster predicted: 0, Cosine Similarity: 3.3076760241890213e-07
Cluster predicted: 0, Cosine Similarity: 3.804856341105278e-08
Cluster predicted: 4, Cosine Similarity: 2.43298845070683e-07
Cluster predicted: 4, Cosine Similarity: 2.822525518864083e-07
Cluster predicted: 0, Cosine Similarity: 1.5824916632656283e-07
Cluster predicted: 2, Cosine Similarity: 1.428523882518462e-07
Cluster predicted: 0, Cosine Similarity: 1.0040993014825972e-07
Cluster predicted: 4, Cosine Similarity: 2.5782142432095867e-07
Cluster predicted: 2, Cosine Similarity: 9.838839987175163e-08
Cluster predicted: 3, Cosine Similarity: 3.466106515137568e-07
Cluster predicted: 3, Cosine Similarity: 8.568725373692132e-08
Cluster predicted: 0, Cosine Similarity: 2.129212173240802e-07
Cluster predicted: 4, Cosine Similarity: 8.997360245732722e-08
Cluster predicted: 2, Cosine Similarity: 7.852151140408381e-08
Cluster predicted: 0, Cosine Similarity: 3.6016145177342196e-08
Cluster predicted: 4, Cosine Similarity: 5.984962381244685e-08
Cluster predicted: 4, Cosine Similarity: 1.5057010183117114e-07
Cluster predicted: 0, Cosine Similarity: 1.1190156168350285e-07
Cluster predicted: 3, Cosine Similarity: 3.025222627961455e-08
Cluster predicted: 0, Cosine Similarity: 8.960423980575172e-08
Cluster predicted: 0, Cosine Similarity: 1.952476469391229e-07
Cluster predicted: 0, Cosine Similarity: 1.2197896892551796e-07
Cluster predicted: 0, Cosine Similarity: 9.318637694821774e-08
Cluster predicted: 0, Cosine Similarity: 4.748237147911283e-08
Cluster predicted: 4, Cosine Similarity: 1.253755014252178e-07
Cluster predicted: 4, Cosine Similarity: 5.362566124489376e-08
Cluster predicted: 2, Cosine Similarity: 7.473734220386774e-08
Cluster predicted: 4, Cosine Similarity: 6.685033686615327e-07
Cluster predicted: 1, Cosine Similarity: 0.46192158153736074
Cluster predicted: 0, Cosine Similarity: 1.4386980717873143e-07
Cluster predicted: 0, Cosine Similarity: 1.5455693980204188e-07
Cluster predicted: 2, Cosine Similarity: 4.834276184917385e-07
Cluster predicted: 0, Cosine Similarity: 2.3476971355851362e-07
Cluster predicted: 0, Cosine Similarity: 2.0756071417160626e-08
Cluster predicted: 4, Cosine Similarity: 9.657930144513216e-08
Cluster predicted: 2, Cosine Similarity: 6.811504604797847e-08
Cluster predicted: 0, Cosine Similarity: 4.3739334487646886e-08
Cluster predicted: 4, Cosine Similarity: 1.011601378220206e-07
Cluster predicted: 0, Cosine Similarity: 1.296598162170426e-07
Cluster predicted: 4, Cosine Similarity: 1.683789332407315e-07
Cluster predicted: 0, Cosine Similarity: 9.946993673182192e-08
Cluster predicted: 0, Cosine Similarity: 1.6695469118310768e-07
Cluster predicted: 0, Cosine Similarity: 8.966351860983934e-08
Cluster predicted: 4, Cosine Similarity: 2.417221078854226e-07
Cluster predicted: 0, Cosine Similarity: 3.0621768343230826e-08
Cluster predicted: 4, Cosine Similarity: 1.1719306536317475e-07
Cluster predicted: 0, Cosine Similarity: 6.795898532807598e-08
Cluster predicted: 0, Cosine Similarity: 2.369291579595867e-07
Cluster predicted: 0, Cosine Similarity: 9.04087386066621e-08
Cluster predicted: 4, Cosine Similarity: 1.7483525704964364e-07
Cluster predicted: 4, Cosine Similarity: 7.168345128594211e-08
Cluster predicted: 3, Cosine Similarity: 6.016758646865128e-08
Cluster predicted: 0, Cosine Similarity: 7.833278692359613e-08
Cluster predicted: 0, Cosine Similarity: 1.2151320638587038e-07
Cluster predicted: 4, Cosine Similarity: 5.043899242718197e-08
Cluster predicted: 3, Cosine Similarity: 2.1107541936515872e-08
Cluster predicted: 0, Cosine Similarity: 1.0623619584038124e-07
Cluster predicted: 2, Cosine Similarity: 3.6044211870756016e-07
Cluster predicted: 0, Cosine Similarity: 2.4026571121549267e-07
Cluster predicted: 4, Cosine Similarity: 1.4124245661939483e-07
Cluster predicted: 0, Cosine Similarity: 1.8698247938875312e-07
Cluster predicted: 0, Cosine Similarity: 4.541353904041756e-07
Cluster predicted: 0, Cosine Similarity: 8.439616794309046e-08
Cluster predicted: 0, Cosine Similarity: 9.120476918145215e-08
Cluster predicted: 3, Cosine Similarity: 4.145097276531118e-08
Cluster predicted: 0, Cosine Similarity: 6.398730134637987e-08
Cluster predicted: 2, Cosine Similarity: 4.635607298020261e-08
Cluster predicted: 0, Cosine Similarity: 1.0637169034488636e-07
Cluster predicted: 2, Cosine Similarity: 4.446398838009458e-08
Cluster predicted: 3, Cosine Similarity: 2.7302593186284696e-08
Cluster predicted: 4, Cosine Similarity: 1.061725023454585e-07
Cluster predicted: 0, Cosine Similarity: 1.8317169514503462e-07
Cluster predicted: 0, Cosine Similarity: 7.696090453812587e-08
Cluster predicted: 0, Cosine Similarity: 1.5310884227126564e-08
Cluster predicted: 4, Cosine Similarity: 1.2281952743720126e-07
Cluster predicted: 2, Cosine Similarity: 8.703589216008112e-08
Cluster predicted: 0, Cosine Similarity: 1.270261407171347e-08
Cluster predicted: 0, Cosine Similarity: 3.361111688970908e-08
Cluster predicted: 0, Cosine Similarity: 2.8132056018037588e-08
Cluster predicted: 4, Cosine Similarity: 1.2235480495448314e-07
Cluster predicted: 3, Cosine Similarity: 1.0755069057566402e-08
Cluster predicted: 4, Cosine Similarity: 3.6108941081636203e-07
Cluster predicted: 0, Cosine Similarity: 1.1949772493746735e-07
Cluster predicted: 2, Cosine Similarity: 2.1758973067775855e-08
Cluster predicted: 0, Cosine Similarity: 1.4577519924507953e-07
Cluster predicted: 0, Cosine Similarity: 3.1701490554869594e-07
Cluster predicted: 4, Cosine Similarity: 2.1377236647524e-08
Cluster predicted: 0, Cosine Similarity: 2.9254120237354186e-07
Cluster predicted: 2, Cosine Similarity: 1.485286420521703e-07
Cluster predicted: 0, Cosine Similarity: 8.041601562425171e-08
Cluster predicted: 0, Cosine Similarity: 1.5488720750411744e-08
Cluster predicted: 0, Cosine Similarity: 1.5238055861566124e-07
Cluster predicted: 0, Cosine Similarity: 3.693412071736901e-07
Cluster predicted: 1, Cosine Similarity: 4.061674968580675e-07
Cluster predicted: 0, Cosine Similarity: 1.1815971623363453e-07
Cluster predicted: 0, Cosine Similarity: 1.618990507212814e-07
Cluster predicted: 0, Cosine Similarity: 1.655743404604948e-07
Cluster predicted: 0, Cosine Similarity: 2.1441165742874801e-07
Cluster predicted: 0, Cosine Similarity: 1.9260550321309466e-07
Cluster predicted: 0, Cosine Similarity: 1.9529845740606788e-07
Cluster predicted: 2, Cosine Similarity: 9.176610371586236e-08
Cluster predicted: 4, Cosine Similarity: 5.017343673863195e-08
Cluster predicted: 0, Cosine Similarity: 2.0666306277394852e-07
Cluster predicted: 2, Cosine Similarity: 0.00010370894178235446
Cluster predicted: 0, Cosine Similarity: 5.143711867550138e-08
Cluster predicted: 2, Cosine Similarity: 4.635607298020261e-08
Cluster predicted: 0, Cosine Similarity: 4.867133613917218e-07
Cluster predicted: 0, Cosine Similarity: 2.1701145902497387e-07
Cluster predicted: 0, Cosine Similarity: 5.1420181779171514e-08
Cluster predicted: 2, Cosine Similarity: 1.9772284198804613e-07
Cluster predicted: 2, Cosine Similarity: 3.471975263957816e-07
Cluster predicted: 4, Cosine Similarity: 8.258783357195654e-08
Cluster predicted: 2, Cosine Similarity: 4.730211533576778e-08
Cluster predicted: 0, Cosine Similarity: 2.1958585549874954e-07
Cluster predicted: 0, Cosine Similarity: 2.9707180160265523e-08
Cluster predicted: 4, Cosine Similarity: 5.9285317877133537e-08
Cluster predicted: 4, Cosine Similarity: 1.421884988639377e-07
Cluster predicted: 0, Cosine Similarity: 9.149269508679225e-08
Cluster predicted: 2, Cosine Similarity: 9.365818831597039e-08
Cluster predicted: 2, Cosine Similarity: 7.379129984830257e-08
Cluster predicted: 2, Cosine Similarity: 2.6489184623557094e-08
Cluster predicted: 0, Cosine Similarity: 5.645041700752529e-08
Cluster predicted: 4, Cosine Similarity: 5.334018884362024e-07
Cluster predicted: 0, Cosine Similarity: 1.2921099046270257e-07
Cluster predicted: 4, Cosine Similarity: 7.644685728891432e-08
Cluster predicted: 4, Cosine Similarity: 1.4535857051356516e-07
Cluster predicted: 0, Cosine Similarity: 3.356877487092902e-08
Cluster predicted: 2, Cosine Similarity: 1.5515093820805959e-07
Cluster predicted: 4, Cosine Similarity: 4.4148640965246955e-08
Cluster predicted: 0, Cosine Similarity: 4.568706868379735e-08
Cluster predicted: 2, Cosine Similarity: 1.8920846112102652e-08
Cluster predicted: 3, Cosine Similarity: 3.571603146568236e-08
Cluster predicted: 3, Cosine Similarity: 4.3767954815976395e-08
Cluster predicted: 2, Cosine Similarity: 1.8447824967626758e-07
Cluster predicted: 4, Cosine Similarity: 7.458796713599725e-08
Cluster predicted: 0, Cosine Similarity: 1.2706001450979443e-07
Cluster predicted: 1, Cosine Similarity: 0.00010042932845399921
Cluster predicted: 0, Cosine Similarity: 6.320820766791968e-08
Cluster predicted: 0, Cosine Similarity: 2.7914417877195064e-07
Cluster predicted: 2, Cosine Similarity: 3.50035653795544e-08
Cluster predicted: 0, Cosine Similarity: 6.561577649000228e-07
Cluster predicted: 2, Cosine Similarity: 1.0595673827218377e-07
Cluster predicted: 2, Cosine Similarity: 1.4758259980762745e-07
Cluster predicted: 0, Cosine Similarity: 1.7367860816186464e-07
Cluster predicted: 2, Cosine Similarity: 2.4597099912426756e-08
Cluster predicted: 3, Cosine Similarity: 1.6681448955502276e-07
Cluster predicted: 3, Cosine Similarity: 9.838135739403953e-08
Cluster predicted: 0, Cosine Similarity: 8.701290654666849e-08
Cluster predicted: 0, Cosine Similarity: 8.798677353372142e-08
Cluster predicted: 0, Cosine Similarity: 1.3567238688061423e-07
Cluster predicted: 0, Cosine Similarity: 2.7272512448384134e-07
Cluster predicted: 4, Cosine Similarity: 6.819803222146703e-08
Cluster predicted: 3, Cosine Similarity: 2.3440956398168566e-08
Cluster predicted: 2, Cosine Similarity: 8.703589216008112e-08
Cluster predicted: 2, Cosine Similarity: 1.3244592289574086e-07
Cluster predicted: 2, Cosine Similarity: 1.6177323436394886e-07
Cluster predicted: 0, Cosine Similarity: 2.0457983407329294e-07
Cluster predicted: 3, Cosine Similarity: 8.443016774606349e-08
Cluster predicted: 0, Cosine Similarity: 7.380218780550507e-08
Cluster predicted: 0, Cosine Similarity: 4.5433016349072375e-08
Cluster predicted: 0, Cosine Similarity: 7.162580661912443e-08
Cluster predicted: 0, Cosine Similarity: 4.365465045008676e-08
Cluster predicted: 0, Cosine Similarity: 9.563374725463092e-08
Cluster predicted: 4, Cosine Similarity: 6.36487305305522e-07
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 0, Cosine Similarity: 2.1711307995886386e-07
Cluster predicted: 3, Cosine Similarity: 5.063673835792315e-08
Cluster predicted: 0, Cosine Similarity: 3.59653347103972e-08
Cluster predicted: 4, Cosine Similarity: 6.41317100535943e-08
Cluster predicted: 1, Cosine Similarity: 1.0828557911235492e-05
Cluster predicted: 2, Cosine Similarity: 4.730211533576778e-08
Cluster predicted: 0, Cosine Similarity: 1.5261767405405635e-07
Cluster predicted: 0, Cosine Similarity: 6.369090699287483e-08
Cluster predicted: 0, Cosine Similarity: 1.1349362261103835e-07
Cluster predicted: 0, Cosine Similarity: 3.900125945532906e-07
Cluster predicted: 0, Cosine Similarity: 6.36824386557322e-08
Cluster predicted: 2, Cosine Similarity: 5.487045373619992e-08
Cluster predicted: 2, Cosine Similarity: 1.1447111902818108e-07
Cluster predicted: 3, Cosine Similarity: 3.4697216655388274e-08
Cluster predicted: 0, Cosine Similarity: 3.825180516781046e-08
Cluster predicted: 0, Cosine Similarity: 1.2282072876512728e-06
Cluster predicted: 0, Cosine Similarity: 6.103182648153904e-08
Cluster predicted: 0, Cosine Similarity: 1.919873092726121e-07
Cluster predicted: 4, Cosine Similarity: 2.8192060730347634e-07
Cluster predicted: 3, Cosine Similarity: 1.4448272667166862e-07
Cluster predicted: 2, Cosine Similarity: 7.00071306480865e-08
Cluster predicted: 0, Cosine Similarity: 2.0391082977688768e-07
Cluster predicted: 0, Cosine Similarity: 2.2927371590153456e-07
Cluster predicted: 0, Cosine Similarity: 2.046475813255455e-07
Cluster predicted: 0, Cosine Similarity: 5.60100597013502e-08
Cluster predicted: 4, Cosine Similarity: 7.852151140408381e-08
Cluster predicted: 0, Cosine Similarity: 5.469745623098987e-08
Cluster predicted: 0, Cosine Similarity: 6.322006347314613e-07
Cluster predicted: 0, Cosine Similarity: 5.8542114156345804e-08
Cluster predicted: 4, Cosine Similarity: 8.540936324852311e-08
Cluster predicted: 0, Cosine Similarity: 1.1246047670887549e-07
Cluster predicted: 4, Cosine Similarity: 2.14270283738216e-07
Cluster predicted: 4, Cosine Similarity: 3.14534170930969e-07
Cluster predicted: 0, Cosine Similarity: 1.1699107571594425e-07
Cluster predicted: 0, Cosine Similarity: 6.791833699892891e-07
Cluster predicted: 0, Cosine Similarity: 1.3081998828656793e-07
Cluster predicted: 2, Cosine Similarity: 2.9327311468207995e-08
Cluster predicted: 0, Cosine Similarity: 7.38360614871425e-08
Cluster predicted: 0, Cosine Similarity: 1.0682051609922638e-07
Cluster predicted: 3, Cosine Similarity: 4.700515665589933e-08
Cluster predicted: 4, Cosine Similarity: 3.541849613597492e-08
Cluster predicted: 2, Cosine Similarity: 7.56833840043214e-09
Cluster predicted: 0, Cosine Similarity: 1.3424969402819897e-07
Cluster predicted: 0, Cosine Similarity: 6.327595492017224e-08
Cluster predicted: 3, Cosine Similarity: 2.7458701867821844e-08
Cluster predicted: 0, Cosine Similarity: 5.555615296692906e-07
Cluster predicted: 4, Cosine Similarity: 6.113258994577819e-07
Cluster predicted: 0, Cosine Similarity: 1.1623738727095656e-07
Cluster predicted: 3, Cosine Similarity: 1.2414751227041165e-08
Cluster predicted: 2, Cosine Similarity: 3.689564997966244e-08
Cluster predicted: 0, Cosine Similarity: 2.675424578546526e-07
Cluster predicted: 0, Cosine Similarity: 1.7754867132069307e-07
Cluster predicted: 2, Cosine Similarity: 4.635607298020261e-08
Cluster predicted: 4, Cosine Similarity: 1.2145855432521557e-07
Cluster predicted: 0, Cosine Similarity: 9.885174279133935e-08
Cluster predicted: 0, Cosine Similarity: 5.086126675379887e-08
Cluster predicted: 0, Cosine Similarity: 1.6015455839291093e-07
Cluster predicted: 4, Cosine Similarity: 1.584039960311756e-07
Cluster predicted: 2, Cosine Similarity: 4.8248157580310647e-08
Cluster predicted: 2, Cosine Similarity: 3.973377682431334e-08
Cluster predicted: 0, Cosine Similarity: 1.211829381286833e-08
Cluster predicted: 2, Cosine Similarity: 5.108628453598385e-08
Cluster predicted: 0, Cosine Similarity: 3.6222774324468077e-07
Cluster predicted: 2, Cosine Similarity: 3.121939606831603e-08
Cluster predicted: 0, Cosine Similarity: 5.783923617830311e-08
Cluster predicted: 4, Cosine Similarity: 6.90942828507346e-08
Cluster predicted: 2, Cosine Similarity: 1.5515093820805959e-07
Cluster predicted: 2, Cosine Similarity: 4.635607298020261e-08
Cluster predicted: 2, Cosine Similarity: 2.0812930712210687e-08
Cluster predicted: 0, Cosine Similarity: 2.0405479272955773e-07
Cluster predicted: 0, Cosine Similarity: 8.332914802622327e-09
Cluster predicted: 0, Cosine Similarity: 1.1920979903212725e-07
Cluster predicted: 0, Cosine Similarity: 4.414581811218454e-08
Cluster predicted: 0, Cosine Similarity: 1.8888787145510122e-07
Cluster predicted: 4, Cosine Similarity: 1.4798093339596363e-07
Cluster predicted: 2, Cosine Similarity: 8.892797676018915e-08
Cluster predicted: 0, Cosine Similarity: 4.318888791043918e-08
Cluster predicted: 0, Cosine Similarity: 4.346072383087929e-07
Cluster predicted: 0, Cosine Similarity: 1.245448969511287e-07
Cluster predicted: 4, Cosine Similarity: 1.0529284899529756e-07
Cluster predicted: 4, Cosine Similarity: 1.2502695956317922e-07
Cluster predicted: 4, Cosine Similarity: 1.7490164605504788e-07
Cluster predicted: 0, Cosine Similarity: 4.680489873454263e-08
Cluster predicted: 2, Cosine Similarity: 5.865462293641599e-08
Cluster predicted: 2, Cosine Similarity: 3.973377682431334e-08
Cluster predicted: 0, Cosine Similarity: 1.5048363444236657e-08
Cluster predicted: 0, Cosine Similarity: 1.2661965720361934e-07
Cluster predicted: 0, Cosine Similarity: 4.860612937696018e-07
Cluster predicted: 0, Cosine Similarity: 1.0512683412677859e-07
Cluster predicted: 0, Cosine Similarity: 5.4646645764044877e-08
Cluster predicted: 0, Cosine Similarity: 1.0566034391867873e-07
Cluster predicted: 2, Cosine Similarity: 6.527691909230526e-08
Cluster predicted: 0, Cosine Similarity: 1.2693298834243194e-07
Cluster predicted: 0, Cosine Similarity: 1.265637657121843e-06
Cluster predicted: 0, Cosine Similarity: 3.1062125649405914e-08
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 0, Cosine Similarity: 5.131856095630383e-08
Cluster predicted: 2, Cosine Similarity: 2.743522686809996e-08
Cluster predicted: 0, Cosine Similarity: 6.40465802614898e-08
Cluster predicted: 0, Cosine Similarity: 3.105365720124098e-08
Cluster predicted: 3, Cosine Similarity: 4.1204485268231394e-08
Cluster predicted: 0, Cosine Similarity: 9.413483881282048e-08
Cluster predicted: 4, Cosine Similarity: 6.715240652432897e-08
Cluster predicted: 2, Cosine Similarity: 2.5543142267991925e-08
Cluster predicted: 4, Cosine Similarity: 1.0286965290706718e-07
Cluster predicted: 0, Cosine Similarity: 1.2210599498185815e-07
Cluster predicted: 2, Cosine Similarity: 3.878773457977047e-08
Cluster predicted: 3, Cosine Similarity: 2.2175654201816997e-08
Cluster predicted: 2, Cosine Similarity: 5.2978369136091885e-08
Cluster predicted: 0, Cosine Similarity: 1.1069904759519744e-07
Cluster predicted: 4, Cosine Similarity: 9.066570713089916e-07
Cluster predicted: 4, Cosine Similarity: 2.9093290498938273e-07
Cluster predicted: 2, Cosine Similarity: 2.336724496121434e-07
Cluster predicted: 0, Cosine Similarity: 8.935865591919168e-08
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 6.74678176659782e-08
Cluster predicted: 0, Cosine Similarity: 1.0653259019388628e-07
Cluster predicted: 2, Cosine Similarity: 5.487045373619992e-08
Cluster predicted: 0, Cosine Similarity: 8.69620960797235e-08
Cluster predicted: 0, Cosine Similarity: 9.281376689429521e-08
Cluster predicted: 4, Cosine Similarity: 6.351761239198339e-08
Cluster predicted: 2, Cosine Similarity: 6.253339643880196e-07
Cluster predicted: 0, Cosine Similarity: 1.8108846644437904e-07
Cluster predicted: 2, Cosine Similarity: 1.8826241898750595e-07
Cluster predicted: 4, Cosine Similarity: 4.3936196369998015e-07
Cluster predicted: 3, Cosine Similarity: 6.790729234484871e-08
Cluster predicted: 0, Cosine Similarity: 4.312114065818662e-08
Cluster predicted: 3, Cosine Similarity: 5.70454120607522e-08
Cluster predicted: 2, Cosine Similarity: 9.93344421162945e-08
Cluster predicted: 0, Cosine Similarity: 2.1373418457315552e-07
Cluster predicted: 2, Cosine Similarity: 9.460423067153556e-08
Cluster predicted: 0, Cosine Similarity: 1.3555382916141667e-07
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 1.2374886648736094e-07
Cluster predicted: 4, Cosine Similarity: 1.4859503105757454e-07
Cluster predicted: 0, Cosine Similarity: 4.928614261157094e-08
Cluster predicted: 0, Cosine Similarity: 5.954138637687123e-08
Cluster predicted: 4, Cosine Similarity: 1.4291877714622814e-07
Cluster predicted: 0, Cosine Similarity: 1.0333153144248541e-07
Cluster predicted: 0, Cosine Similarity: 3.6193981700627376e-08
Cluster predicted: 0, Cosine Similarity: 5.650969586712407e-07
Cluster predicted: 2, Cosine Similarity: 5.6762538336307955e-08
Cluster predicted: 0, Cosine Similarity: 2.1000808447801944e-07
Cluster predicted: 2, Cosine Similarity: 1.2960779594006766e-07
Cluster predicted: 3, Cosine Similarity: 7.240979638112321e-08
Cluster predicted: 0, Cosine Similarity: 4.364618200192183e-08
Cluster predicted: 2, Cosine Similarity: 1.6461136131962206e-07
Cluster predicted: 0, Cosine Similarity: 1.3641760687743698e-07
Cluster predicted: 0, Cosine Similarity: 2.0544361212238016e-08
Cluster predicted: 3, Cosine Similarity: 1.1793602838761075e-07
Cluster predicted: 0, Cosine Similarity: 1.746863489415773e-07
Cluster predicted: 2, Cosine Similarity: 1.665034459197301e-07
Cluster predicted: 0, Cosine Similarity: 5.371512079577201e-08
Cluster predicted: 0, Cosine Similarity: 4.932001629320837e-08
Cluster predicted: 0, Cosine Similarity: 1.0551638096600868e-07
Cluster predicted: 2, Cosine Similarity: 5.960066529198116e-08
Cluster predicted: 1, Cosine Similarity: 5.022790881925587e-05
Cluster predicted: 4, Cosine Similarity: 4.779339342064759e-07
Cluster predicted: 0, Cosine Similarity: 1.4541952608748687e-07
Cluster predicted: 2, Cosine Similarity: 3.0273353801568703e-07
Cluster predicted: 0, Cosine Similarity: 9.604869932733351e-08
Cluster predicted: 0, Cosine Similarity: 1.0701528951884143e-07
Cluster predicted: 2, Cosine Similarity: 4.0679819168776277e-07
Cluster predicted: 0, Cosine Similarity: 2.0154814373007923e-08
Cluster predicted: 4, Cosine Similarity: 5.6397399195162734e-08
Cluster predicted: 0, Cosine Similarity: 1.1243507147540299e-07
Cluster predicted: 4, Cosine Similarity: 1.1715987091598379e-07
Cluster predicted: 4, Cosine Similarity: 1.4293205496951344e-06
Cluster predicted: 2, Cosine Similarity: 2.9989541106001383e-07
Cluster predicted: 4, Cosine Similarity: 4.887885252102819e-08
Cluster predicted: 4, Cosine Similarity: 5.887038700969072e-08
Cluster predicted: 3, Cosine Similarity: 8.714974586432334e-08
Cluster predicted: 4, Cosine Similarity: 1.5871934344602323e-07
Cluster predicted: 0, Cosine Similarity: 1.4180351526338342e-07
Cluster predicted: 4, Cosine Similarity: 3.0836000008971354e-07
Cluster predicted: 0, Cosine Similarity: 4.187628444007885e-08
Cluster predicted: 4, Cosine Similarity: 5.1833160097380926e-08
Cluster predicted: 0, Cosine Similarity: 4.1588358534738745e-08
Cluster predicted: 3, Cosine Similarity: 4.613423432164865e-08
Cluster predicted: 3, Cosine Similarity: 2.876508531368671e-08
Cluster predicted: 2, Cosine Similarity: 1.9393867278783006e-07
Cluster predicted: 2, Cosine Similarity: 1.806940804760515e-07
Cluster predicted: 0, Cosine Similarity: 9.68023877723212e-08
Cluster predicted: 4, Cosine Similarity: 4.942822091624777e-07
Cluster predicted: 2, Cosine Similarity: 8.608984991553825e-08
Cluster predicted: 2, Cosine Similarity: 7.095317300365167e-08
Cluster predicted: 4, Cosine Similarity: 1.24661820422034e-07
Cluster predicted: 0, Cosine Similarity: 1.5245677431607874e-07
Cluster predicted: 0, Cosine Similarity: 9.319484539638268e-08
Cluster predicted: 0, Cosine Similarity: 8.537003504116569e-08
Cluster predicted: 0, Cosine Similarity: 3.1670157440988334e-07
Cluster predicted: 0, Cosine Similarity: 1.2700920404284943e-07
Cluster predicted: 0, Cosine Similarity: 9.56049546640969e-07
Cluster predicted: 0, Cosine Similarity: 7.61310003927207e-08
Cluster predicted: 3, Cosine Similarity: 1.188644643823622e-07
Cluster predicted: 2, Cosine Similarity: 3.0273353823773164e-08
Cluster predicted: 2, Cosine Similarity: 1.0311861142753287e-07
Cluster predicted: 4, Cosine Similarity: 5.319413320936661e-08
Cluster predicted: 0, Cosine Similarity: 1.5629296368224743e-07
Cluster predicted: 0, Cosine Similarity: 8.797830519657879e-08
Cluster predicted: 4, Cosine Similarity: 5.0090450565143385e-08
Cluster predicted: 3, Cosine Similarity: 7.887598385547534e-08
Cluster predicted: 0, Cosine Similarity: 1.355877027320318e-07
Cluster predicted: 3, Cosine Similarity: 5.4383347602993126e-08
Cluster predicted: 1, Cosine Similarity: 3.6157736945652275e-06
Cluster predicted: 0, Cosine Similarity: 9.225485186892257e-08
Cluster predicted: 2, Cosine Similarity: 1.0311861142753287e-07
Cluster predicted: 4, Cosine Similarity: 4.182502832961177e-08
Cluster predicted: 4, Cosine Similarity: 3.204427860836745e-07
Cluster predicted: 0, Cosine Similarity: 1.9006498042095643e-07
Cluster predicted: 4, Cosine Similarity: 9.921826149561497e-08
Cluster predicted: 0, Cosine Similarity: 2.1874748301620173e-07
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 3, Cosine Similarity: 8.301697296264621e-08
Cluster predicted: 3, Cosine Similarity: 8.614736357603903e-08
Cluster predicted: 0, Cosine Similarity: 6.205650404655927e-08
Cluster predicted: 2, Cosine Similarity: 1.050106960276409e-07
Cluster predicted: 0, Cosine Similarity: 1.1387470111312581e-07
Cluster predicted: 0, Cosine Similarity: 7.850215510973868e-08
Cluster predicted: 2, Cosine Similarity: 9.460423067153556e-08
Cluster predicted: 2, Cosine Similarity: 7.946755375964898e-08
Cluster predicted: 0, Cosine Similarity: 4.735534531175034e-08
Cluster predicted: 2, Cosine Similarity: 3.093558341715763e-07
Cluster predicted: 0, Cosine Similarity: 7.507244925708534e-08
Cluster predicted: 0, Cosine Similarity: 1.5070381353954332e-07
Cluster predicted: 2, Cosine Similarity: 4.257190377998654e-08
Cluster predicted: 0, Cosine Similarity: 1.503312034856208e-07
Cluster predicted: 0, Cosine Similarity: 1.7632075188789287e-07
Cluster predicted: 0, Cosine Similarity: 2.5594073704837683e-07
Cluster predicted: 2, Cosine Similarity: 2.3178036501203536e-07
Cluster predicted: 0, Cosine Similarity: 9.882633755786685e-08
Cluster predicted: 3, Cosine Similarity: 5.394788649137894e-08
Cluster predicted: 0, Cosine Similarity: 2.5400993963753393e-07
Cluster predicted: 0, Cosine Similarity: 2.093221432852843e-07
Cluster predicted: 2, Cosine Similarity: 3.0273353823773164e-08
Cluster predicted: 3, Cosine Similarity: 2.9036221538270013e-08
Cluster predicted: 4, Cosine Similarity: 3.812716466988775e-07
Cluster predicted: 0, Cosine Similarity: 2.9107616761336885e-07
Cluster predicted: 3, Cosine Similarity: 1.1510963848770928e-08
Cluster predicted: 2, Cosine Similarity: 4.5410030735659745e-08
Cluster predicted: 2, Cosine Similarity: 4.162586142442137e-08
Cluster predicted: 0, Cosine Similarity: 1.299477421223827e-07
Cluster predicted: 4, Cosine Similarity: 1.6122552570774218e-07
Cluster predicted: 0, Cosine Similarity: 2.6768642080732263e-08
Cluster predicted: 0, Cosine Similarity: 7.180364325343191e-08
Cluster predicted: 4, Cosine Similarity: 1.138238269193792e-07
Cluster predicted: 2, Cosine Similarity: 3.973377682431334e-08
Cluster predicted: 0, Cosine Similarity: 3.5423356470332124e-08
Cluster predicted: 4, Cosine Similarity: 5.523559287734514e-08
Cluster predicted: 0, Cosine Similarity: 1.6530335100739535e-08
Cluster predicted: 2, Cosine Similarity: 4.257190377998654e-08
Cluster predicted: 2, Cosine Similarity: 2.0812930734415147e-07
Cluster predicted: 0, Cosine Similarity: 3.712974097069832e-07
Cluster predicted: 0, Cosine Similarity: 3.044477859814876e-07
Cluster predicted: 4, Cosine Similarity: 2.693730988267973e-07
Cluster predicted: 0, Cosine Similarity: 1.0360252045149565e-07
Cluster predicted: 0, Cosine Similarity: 1.967126816992959e-07
Cluster predicted: 0, Cosine Similarity: 9.460060135246806e-08
Cluster predicted: 3, Cosine Similarity: 3.586392394172577e-08
Cluster predicted: 4, Cosine Similarity: 2.7563025584154133e-07
Cluster predicted: 0, Cosine Similarity: 1.530495629120665e-07
Cluster predicted: 0, Cosine Similarity: 5.609474384993263e-08
Cluster predicted: 0, Cosine Similarity: 2.0434271863489784e-07
Cluster predicted: 0, Cosine Similarity: 2.5947206383492016e-08
Cluster predicted: 0, Cosine Similarity: 9.914813714484438e-08
Cluster predicted: 0, Cosine Similarity: 2.72928366240599e-07
Cluster predicted: 2, Cosine Similarity: 9.271214596040522e-08
Cluster predicted: 0, Cosine Similarity: 1.1109536911524387e-06
Cluster predicted: 0, Cosine Similarity: 1.9061542699816414e-07
Cluster predicted: 0, Cosine Similarity: 1.425063932414261e-07
Cluster predicted: 0, Cosine Similarity: 1.0162091268473006e-07
Cluster predicted: 2, Cosine Similarity: 2.9989541106001383e-07
Cluster predicted: 4, Cosine Similarity: 1.2559126549849253e-07
Cluster predicted: 0, Cosine Similarity: 7.392074552470262e-08
Cluster predicted: 0, Cosine Similarity: 8.750407420876627e-08
Cluster predicted: 0, Cosine Similarity: 5.0658024997041196e-08
Cluster predicted: 0, Cosine Similarity: 1.3769633655513758e-08
Cluster predicted: 0, Cosine Similarity: 1.4057559594160551e-07
Cluster predicted: 0, Cosine Similarity: 2.430594395086416e-06
Cluster predicted: 0, Cosine Similarity: 2.537982290995444e-08
Cluster predicted: 2, Cosine Similarity: 6.338483449219723e-08
Cluster predicted: 0, Cosine Similarity: 1.5854556056904556e-07
Cluster predicted: 4, Cosine Similarity: 2.549998946443921e-07
Cluster predicted: 2, Cosine Similarity: 1.8920846123204882e-07
Cluster predicted: 0, Cosine Similarity: 5.405385716805711e-08
Cluster predicted: 0, Cosine Similarity: 3.5143899013156954e-08
Cluster predicted: 3, Cosine Similarity: 3.348942811953748e-08
Cluster predicted: 4, Cosine Similarity: 1.8789727984636073e-07
Cluster predicted: 0, Cosine Similarity: 5.618789633565768e-08
Cluster predicted: 0, Cosine Similarity: 3.773523227224018e-08
Cluster predicted: 2, Cosine Similarity: 9.082006136029719e-08
Cluster predicted: 4, Cosine Similarity: 2.6646858297674214e-07
Cluster predicted: 0, Cosine Similarity: 1.294311691157901e-07
Cluster predicted: 0, Cosine Similarity: 5.199603370087402e-08
Cluster predicted: 0, Cosine Similarity: 5.8381214307345886e-08
Cluster predicted: 3, Cosine Similarity: 1.0853664078602776e-08
Cluster predicted: 2, Cosine Similarity: 4.3896363011164397e-07
Cluster predicted: 2, Cosine Similarity: 6.338483449219723e-08
Cluster predicted: 2, Cosine Similarity: 1.078488228722918e-07
Cluster predicted: 0, Cosine Similarity: 1.9124208927578934e-07
Cluster predicted: 4, Cosine Similarity: 1.1080313044864454e-07
Cluster predicted: 4, Cosine Similarity: 5.7061288583071246e-08
Cluster predicted: 4, Cosine Similarity: 2.7700782667672286e-08
Cluster predicted: 2, Cosine Similarity: 6.43308768477624e-08
Cluster predicted: 4, Cosine Similarity: 4.8414129927287775e-08
Cluster predicted: 4, Cosine Similarity: 4.252211205368894e-08
Cluster predicted: 2, Cosine Similarity: 7.189921524819454e-08
Cluster predicted: 2, Cosine Similarity: 3.3111480668424065e-08
Cluster predicted: 0, Cosine Similarity: 2.5151175919724267e-08
Cluster predicted: 3, Cosine Similarity: 3.380164559363408e-08
Cluster predicted: 0, Cosine Similarity: 1.283810863172974e-07
Cluster predicted: 3, Cosine Similarity: 2.0532404554352013e-08
Cluster predicted: 3, Cosine Similarity: 6.180261979960022e-08
Cluster predicted: 0, Cosine Similarity: 2.145894939520332e-07
Cluster predicted: 2, Cosine Similarity: 9.460423067153556e-08
Cluster predicted: 0, Cosine Similarity: 6.544386776941025e-08
Cluster predicted: 0, Cosine Similarity: 2.362008747480715e-07
Cluster predicted: 0, Cosine Similarity: 3.0877514323179867e-07
Cluster predicted: 0, Cosine Similarity: 5.5391865871889934e-08
Cluster predicted: 2, Cosine Similarity: 1.0122652671640253e-07
Cluster predicted: 0, Cosine Similarity: 1.2495984902383128e-07
Cluster predicted: 2, Cosine Similarity: 1.2771571133995963e-07
Cluster predicted: 0, Cosine Similarity: 4.058061775502608e-08
Cluster predicted: 2, Cosine Similarity: 6.054670764754633e-08
Cluster predicted: 0, Cosine Similarity: 9.232259923219743e-08
Cluster predicted: 2, Cosine Similarity: 4.635607298020261e-08
Cluster predicted: 0, Cosine Similarity: 7.498776510850291e-08
Cluster predicted: 0, Cosine Similarity: 8.972279752494927e-08
Cluster predicted: 3, Cosine Similarity: 6.881929592861269e-08
Cluster predicted: 0, Cosine Similarity: 1.0207820677621271e-07
Cluster predicted: 4, Cosine Similarity: 6.856317136261225e-08
Cluster predicted: 2, Cosine Similarity: 1.3528404985141407e-07
Cluster predicted: 4, Cosine Similarity: 1.0096097102785251e-07
Cluster predicted: 2, Cosine Similarity: 4.9194199935875815e-08
Cluster predicted: 4, Cosine Similarity: 3.365918943565305e-08
Cluster predicted: 0, Cosine Similarity: 2.7847517436452307e-07
Cluster predicted: 0, Cosine Similarity: 1.686568413816758e-07
Cluster predicted: 0, Cosine Similarity: 9.798796507531904e-08
Cluster predicted: 2, Cosine Similarity: 4.8248157580310647e-08
Cluster predicted: 4, Cosine Similarity: 5.913594280926304e-08
Cluster predicted: 2, Cosine Similarity: 6.054670764754633e-08
Cluster predicted: 0, Cosine Similarity: 6.293721854788714e-08
Cluster predicted: 4, Cosine Similarity: 7.701116322422763e-08
Cluster predicted: 3, Cosine Similarity: 4.226438132803878e-08
Cluster predicted: 0, Cosine Similarity: 4.855785940005575e-08
Cluster predicted: 0, Cosine Similarity: 1.785987535640743e-08
Cluster predicted: 4, Cosine Similarity: 2.0618743101774584e-07
Cluster predicted: 0, Cosine Similarity: 7.685928360423588e-08
Cluster predicted: 0, Cosine Similarity: 1.3150592947930306e-07
Cluster predicted: 2, Cosine Similarity: 5.108628453598385e-08
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 0, Cosine Similarity: 5.527330804167008e-08
Cluster predicted: 0, Cosine Similarity: 7.809567137417872e-08
Cluster predicted: 4, Cosine Similarity: 1.6185622053743742e-07
Cluster predicted: 4, Cosine Similarity: 1.622545541257736e-07
Cluster predicted: 2, Cosine Similarity: 6.243879224765436e-08
Cluster predicted: 0, Cosine Similarity: 8.18725820472821e-08
Cluster predicted: 0, Cosine Similarity: 1.2850811248465988e-07
Cluster predicted: 0, Cosine Similarity: 4.6137588050054745e-07
Cluster predicted: 3, Cosine Similarity: 5.276474668303166e-08
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 4.0648365118300944e-08
Cluster predicted: 3, Cosine Similarity: 2.0626891406028136e-07
Cluster predicted: 2, Cosine Similarity: 3.3111480668424065e-08
Cluster predicted: 0, Cosine Similarity: 3.196401124894166e-07
Cluster predicted: 0, Cosine Similarity: 3.476282051106949e-08
Cluster predicted: 4, Cosine Similarity: 7.18328263538126e-08
Cluster predicted: 4, Cosine Similarity: 6.994074175370457e-08
Cluster predicted: 0, Cosine Similarity: 5.44434040072872e-08
Cluster predicted: 3, Cosine Similarity: 9.982741744352097e-09
Cluster predicted: 2, Cosine Similarity: 1.1636320362828911e-07
Cluster predicted: 0, Cosine Similarity: 2.491406048132916e-08
Cluster predicted: 2, Cosine Similarity: 6.338483449219723e-08
Cluster predicted: 4, Cosine Similarity: 2.5841892470346295e-07
Cluster predicted: 4, Cosine Similarity: 5.900316490947688e-08
Cluster predicted: 0, Cosine Similarity: 1.2025988171959767e-07
Cluster predicted: 0, Cosine Similarity: 1.4596150421652965e-07
Cluster predicted: 0, Cosine Similarity: 1.4906941048220546e-07
Cluster predicted: 0, Cosine Similarity: 6.040177682464787e-07
Cluster predicted: 2, Cosine Similarity: 2.9327311468207995e-08
Cluster predicted: 0, Cosine Similarity: 1.4525862623848695e-07
Cluster predicted: 0, Cosine Similarity: 3.9691434805533277e-08
Cluster predicted: 0, Cosine Similarity: 6.737466506923084e-08
Cluster predicted: 0, Cosine Similarity: 3.719918193478833e-07
Cluster predicted: 0, Cosine Similarity: 7.608865837394063e-08
Cluster predicted: 3, Cosine Similarity: 2.172376056819303e-08
Cluster predicted: 0, Cosine Similarity: 3.111293610524868e-07
Cluster predicted: 0, Cosine Similarity: 3.7853789991437736e-08
Cluster predicted: 2, Cosine Similarity: 4.351794613555171e-08
Cluster predicted: 0, Cosine Similarity: 7.327714646176986e-08
Cluster predicted: 0, Cosine Similarity: 6.757790693701082e-08
Cluster predicted: 2, Cosine Similarity: 7.974001391075625e-05
Cluster predicted: 4, Cosine Similarity: 1.1958306700421417e-07
Cluster predicted: 3, Cosine Similarity: 1.7623852599513157e-08
Cluster predicted: 4, Cosine Similarity: 2.863022774413082e-08
Cluster predicted: 0, Cosine Similarity: 2.0248813692447243e-07
Cluster predicted: 4, Cosine Similarity: 2.706012940389968e-07
Cluster predicted: 0, Cosine Similarity: 2.762395145960994e-08
Cluster predicted: 0, Cosine Similarity: 9.989335714166714e-08
Cluster predicted: 0, Cosine Similarity: 7.027086112998404e-08
Cluster predicted: 0, Cosine Similarity: 3.166338269355862e-08
Cluster predicted: 0, Cosine Similarity: 1.1507721520143122e-07
Cluster predicted: 2, Cosine Similarity: 9.460423067153556e-08
Cluster predicted: 0, Cosine Similarity: 7.302309412704489e-08
Cluster predicted: 3, Cosine Similarity: 3.0868444911291704e-08
Cluster predicted: 2, Cosine Similarity: 6.906108840354364e-08
Cluster predicted: 4, Cosine Similarity: 7.103615917714023e-08
Cluster predicted: 0, Cosine Similarity: 5.8313467055093327e-08
Cluster predicted: 2, Cosine Similarity: 5.487045373619992e-08
Cluster predicted: 2, Cosine Similarity: 2.9894936881547096e-07
Cluster predicted: 1, Cosine Similarity: 0.00040222502318687514
Cluster predicted: 0, Cosine Similarity: 5.8957066229048394e-08
Cluster predicted: 0, Cosine Similarity: 1.4843427975641532e-07
Cluster predicted: 0, Cosine Similarity: 9.90465162109544e-08
Cluster predicted: 3, Cosine Similarity: 5.957601656447764e-08
Cluster predicted: 2, Cosine Similarity: 1.4758259980762745e-07
Cluster predicted: 1, Cosine Similarity: 8.388241778956029e-08
Cluster predicted: 0, Cosine Similarity: 2.377929353425401e-08
Cluster predicted: 2, Cosine Similarity: 9.384739678708343e-07
Cluster predicted: 0, Cosine Similarity: 1.2341859845221848e-07
Cluster predicted: 0, Cosine Similarity: 1.9081866886594412e-07
Cluster predicted: 3, Cosine Similarity: 9.02472715447189e-08
Cluster predicted: 0, Cosine Similarity: 8.392193706630025e-08
Cluster predicted: 2, Cosine Similarity: 2.2231994201149519e-07
Cluster predicted: 2, Cosine Similarity: 1.1920133058396232e-07
Cluster predicted: 0, Cosine Similarity: 2.6320663226719887e-07
Cluster predicted: 2, Cosine Similarity: 1.2298549956213378e-08
Cluster predicted: 1, Cosine Similarity: 7.264658869798879e-06
Cluster predicted: 0, Cosine Similarity: 1.3017638911261287e-07
Cluster predicted: 0, Cosine Similarity: 1.2860126497038493e-07
Cluster predicted: 3, Cosine Similarity: 7.61153243766799e-08
Cluster predicted: 0, Cosine Similarity: 2.349983607707884e-08
Cluster predicted: 0, Cosine Similarity: 6.998293522464394e-08
Cluster predicted: 0, Cosine Similarity: 7.582613770207303e-08
Cluster predicted: 4, Cosine Similarity: 5.679573289452122e-08
Cluster predicted: 4, Cosine Similarity: 5.467128694203183e-08
Cluster predicted: 3, Cosine Similarity: 2.2348195405363924e-08
Cluster predicted: 4, Cosine Similarity: 2.1443625652928233e-08
Cluster predicted: 2, Cosine Similarity: 1.5515093820805959e-07
Cluster predicted: 4, Cosine Similarity: 8.808151785721918e-08
Cluster predicted: 2, Cosine Similarity: 3.594960762409727e-08
Cluster predicted: 2, Cosine Similarity: 6.71690036924133e-08
Cluster predicted: 4, Cosine Similarity: 8.947568552741814e-08
Cluster predicted: 0, Cosine Similarity: 2.671867846970599e-07
Cluster predicted: 2, Cosine Similarity: 5.960066529198116e-08
Cluster predicted: 0, Cosine Similarity: 4.6474630699400166e-08
Cluster predicted: 4, Cosine Similarity: 9.52847172275284e-08
Cluster predicted: 0, Cosine Similarity: 3.15643023274248e-07
Cluster predicted: 0, Cosine Similarity: 5.0488656810898647e-08
Cluster predicted: 0, Cosine Similarity: 5.5883033533987714e-08
Cluster predicted: 0, Cosine Similarity: 5.551889192823012e-08
Cluster predicted: 2, Cosine Similarity: 1.4190634556321413e-08
Cluster predicted: 0, Cosine Similarity: 4.9447042460570856e-08
Cluster predicted: 0, Cosine Similarity: 1.1716891235025173e-07
Cluster predicted: 0, Cosine Similarity: 8.887595659423653e-08
Cluster predicted: 4, Cosine Similarity: 1.2474480659552256e-07
Cluster predicted: 4, Cosine Similarity: 1.3576536961323882e-07
Cluster predicted: 0, Cosine Similarity: 2.3695456319305919e-07
Cluster predicted: 4, Cosine Similarity: 9.749214924248406e-08
Cluster predicted: 2, Cosine Similarity: 1.778559536314006e-07
Cluster predicted: 3, Cosine Similarity: 6.559852661069954e-08
Cluster predicted: 0, Cosine Similarity: 3.1031639369238917e-07
Cluster predicted: 0, Cosine Similarity: 5.479060871671493e-08
Cluster predicted: 3, Cosine Similarity: 2.9430601511393206e-08
Cluster predicted: 0, Cosine Similarity: 6.137903130198907e-08
Cluster predicted: 0, Cosine Similarity: 6.505432093018015e-08
Cluster predicted: 0, Cosine Similarity: 3.6846049211725074e-08
Cluster predicted: 4, Cosine Similarity: 7.0156505715957e-08
Cluster predicted: 4, Cosine Similarity: 2.7149754466826437e-07
Cluster predicted: 0, Cosine Similarity: 9.056117000749708e-08
Cluster predicted: 4, Cosine Similarity: 3.363429357250425e-07
Cluster predicted: 0, Cosine Similarity: 4.660165686676265e-08
Cluster predicted: 0, Cosine Similarity: 2.6556931875809653e-08
Cluster predicted: 0, Cosine Similarity: 1.2101357016458536e-07
Cluster predicted: 2, Cosine Similarity: 8.608984991553825e-08
Cluster predicted: 3, Cosine Similarity: 2.990714387252069e-08
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 3, Cosine Similarity: 4.1639946490867885e-08
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 2, Cosine Similarity: 1.0879486522785697e-07
Cluster predicted: 4, Cosine Similarity: 1.6238733202555977e-07
Cluster predicted: 3, Cosine Similarity: 1.001067695405311e-07
Cluster predicted: 0, Cosine Similarity: 1.4090586386572568e-07
Cluster predicted: 0, Cosine Similarity: 1.1051274251272503e-07
Cluster predicted: 0, Cosine Similarity: 4.088548055669605e-08
Cluster predicted: 2, Cosine Similarity: 6.906108840354364e-08
Cluster predicted: 0, Cosine Similarity: 9.813192802798909e-08
Cluster predicted: 0, Cosine Similarity: 8.942640317144424e-08
Cluster predicted: 4, Cosine Similarity: 3.488738464785257e-08
Cluster predicted: 3, Cosine Similarity: 3.0572659959204884e-08
Cluster predicted: 2, Cosine Similarity: 5.24107437671617e-07
Cluster predicted: 2, Cosine Similarity: 7.28452576037597e-08
Cluster predicted: 0, Cosine Similarity: 7.804486090723373e-08
Cluster predicted: 3, Cosine Similarity: 3.00550363485641e-08
Cluster predicted: 2, Cosine Similarity: 7.757546915954094e-08
Cluster predicted: 0, Cosine Similarity: 2.1236230229870756e-07
Cluster predicted: 0, Cosine Similarity: 5.734806840518303e-08
Cluster predicted: 4, Cosine Similarity: 1.4442912543710662e-07
Cluster predicted: 0, Cosine Similarity: 4.4738606819194615e-08
Cluster predicted: 3, Cosine Similarity: 4.6890129112853174e-08
Cluster predicted: 4, Cosine Similarity: 4.794940744456966e-08
Cluster predicted: 0, Cosine Similarity: 1.1784638509482193e-07
Cluster predicted: 2, Cosine Similarity: 1.5562395938584217e-06
Cluster predicted: 0, Cosine Similarity: 5.610321218707526e-08
Cluster predicted: 0, Cosine Similarity: 1.0395819372011061e-07
Cluster predicted: 4, Cosine Similarity: 2.5123232261226036e-07
Cluster predicted: 0, Cosine Similarity: 8.974820275842177e-08
Cluster predicted: 0, Cosine Similarity: 6.021970602176907e-07
Cluster predicted: 0, Cosine Similarity: 1.0356017843271559e-07
Cluster predicted: 0, Cosine Similarity: 8.12205144251621e-08
Cluster predicted: 0, Cosine Similarity: 3.939758100868218e-07
Cluster predicted: 4, Cosine Similarity: 9.573284254216219e-08
Cluster predicted: 4, Cosine Similarity: 1.0974090747239984e-07
Cluster predicted: 4, Cosine Similarity: 3.426830789132751e-07
Cluster predicted: 3, Cosine Similarity: 5.513102618870391e-08
Cluster predicted: 2, Cosine Similarity: 3.4057523023989233e-08
Cluster predicted: 2, Cosine Similarity: 1.9488471514339523e-07
Cluster predicted: 0, Cosine Similarity: 2.1207437639336746e-07
Cluster predicted: 0, Cosine Similarity: 5.462970897873731e-08
Cluster predicted: 0, Cosine Similarity: 7.214237962571701e-08
Cluster predicted: 0, Cosine Similarity: 4.9616410646713405e-08
Cluster predicted: 4, Cosine Similarity: 7.072081165127031e-08
Cluster predicted: 2, Cosine Similarity: 3.963917263316574e-07
Cluster predicted: 0, Cosine Similarity: 1.1727053328414172e-07
Cluster predicted: 0, Cosine Similarity: 9.887714802481185e-08
Cluster predicted: 0, Cosine Similarity: 7.961998504946166e-08
Cluster predicted: 4, Cosine Similarity: 7.452157813059301e-08
Cluster predicted: 2, Cosine Similarity: 1.5893510740827566e-07
Cluster predicted: 4, Cosine Similarity: 8.278700036612463e-08
Cluster predicted: 3, Cosine Similarity: 5.3011234180111444e-08
Cluster predicted: 0, Cosine Similarity: 5.330863717123435e-08
Cluster predicted: 0, Cosine Similarity: 6.987199904218144e-07
Cluster predicted: 2, Cosine Similarity: 2.365105766788389e-08
Cluster predicted: 0, Cosine Similarity: 2.9724116945573087e-08
Cluster predicted: 3, Cosine Similarity: 6.646123262843417e-08
Cluster predicted: 0, Cosine Similarity: 2.4096858919353537e-07
Cluster predicted: 2, Cosine Similarity: 1.286617536955248e-07
Cluster predicted: 2, Cosine Similarity: 5.865462293641599e-08
Cluster predicted: 1, Cosine Similarity: 3.244925110834629e-07
Cluster predicted: 4, Cosine Similarity: 9.065408901332006e-08
Cluster predicted: 3, Cosine Similarity: 2.5281396087706298e-08
Cluster predicted: 2, Cosine Similarity: 1.7407178432016224e-07
Cluster predicted: 2, Cosine Similarity: 6.243879224765436e-08
Cluster predicted: 4, Cosine Similarity: 2.661034438355969e-07
Cluster predicted: 2, Cosine Similarity: 7.095317300365167e-08
Cluster predicted: 0, Cosine Similarity: 1.7030818122432123e-07
Cluster predicted: 0, Cosine Similarity: 2.038346140764702e-07
Cluster predicted: 4, Cosine Similarity: 6.645532268922949e-08
Cluster predicted: 2, Cosine Similarity: 1.2203945753963552e-07
Cluster predicted: 0, Cosine Similarity: 5.472709568854484e-07
Cluster predicted: 0, Cosine Similarity: 2.3406683591353783e-08
Cluster predicted: 2, Cosine Similarity: 1.5136676900784352e-07
Cluster predicted: 2, Cosine Similarity: 9.271214596040522e-08
Cluster predicted: 2, Cosine Similarity: 1.1380888944589884e-06
Cluster predicted: 0, Cosine Similarity: 4.6881114434960125e-08
Cluster predicted: 0, Cosine Similarity: 5.914337120049851e-08
Cluster predicted: 0, Cosine Similarity: 6.6138277299288e-08
Cluster predicted: 0, Cosine Similarity: 1.2101357027560766e-08
Cluster predicted: 2, Cosine Similarity: 3.973377682431334e-08
Cluster predicted: 0, Cosine Similarity: 1.410667636037033e-07
Cluster predicted: 0, Cosine Similarity: 1.8835436166320108e-07
Cluster predicted: 4, Cosine Similarity: 2.5466794995043784e-07
Cluster predicted: 4, Cosine Similarity: 1.0104395720134107e-07
Cluster predicted: 4, Cosine Similarity: 1.9038686482897305e-07
Cluster predicted: 0, Cosine Similarity: 1.0352630486210046e-07
Cluster predicted: 2, Cosine Similarity: 7.568338444841061e-08
Cluster predicted: 2, Cosine Similarity: 5.2978369136091885e-08
Cluster predicted: 3, Cosine Similarity: 9.477442430849692e-08
Cluster predicted: 2, Cosine Similarity: 1.3623009209595693e-07
Cluster predicted: 4, Cosine Similarity: 9.408971635149754e-08
Cluster predicted: 2, Cosine Similarity: 1.3244592289574086e-07
Cluster predicted: 0, Cosine Similarity: 3.3447676572873064e-07
Cluster predicted: 2, Cosine Similarity: 2.771903957476951e-07
Cluster predicted: 0, Cosine Similarity: 1.7425446008356715e-07
Cluster predicted: 4, Cosine Similarity: 3.012397864488037e-08
Cluster predicted: 4, Cosine Similarity: 2.7451824147206594e-08
Cluster predicted: 0, Cosine Similarity: 3.357131534986735e-07
Cluster predicted: 2, Cosine Similarity: 5.3924411491657054e-08
Cluster predicted: 0, Cosine Similarity: 9.206007844930753e-08
Cluster predicted: 0, Cosine Similarity: 1.0120596061202747e-07
Cluster predicted: 2, Cosine Similarity: 4.351794613555171e-08
Cluster predicted: 4, Cosine Similarity: 1.8781429367287217e-07
Cluster predicted: 0, Cosine Similarity: 3.3377388775068795e-07
Cluster predicted: 4, Cosine Similarity: 1.107865332805602e-07
Cluster predicted: 4, Cosine Similarity: 1.8588901418148396e-08
Cluster predicted: 0, Cosine Similarity: 7.32602096764623e-08
Cluster predicted: 0, Cosine Similarity: 1.2884684885694497e-07
Cluster predicted: 0, Cosine Similarity: 4.637300987653248e-08
Cluster predicted: 0, Cosine Similarity: 2.30222177655115e-07
Cluster predicted: 0, Cosine Similarity: 5.7923920215863234e-08
Cluster predicted: 2, Cosine Similarity: 3.4057523023989233e-08
Cluster predicted: 2, Cosine Similarity: 1.021725690719677e-06
Cluster predicted: 0, Cosine Similarity: 3.073185772528575e-08
Cluster predicted: 3, Cosine Similarity: 1.2250426240090206e-07
Cluster predicted: 4, Cosine Similarity: 2.1307528297320744e-07
Cluster predicted: 4, Cosine Similarity: 1.2159133222500174e-07
Cluster predicted: 2, Cosine Similarity: 6.243879224765436e-08
Cluster predicted: 0, Cosine Similarity: 3.2631321911225086e-07
Cluster predicted: 2, Cosine Similarity: 1.7123365747551134e-07
Cluster predicted: 0, Cosine Similarity: 1.0089262947321487e-07
Cluster predicted: 0, Cosine Similarity: 4.638994666184004e-08
Cluster predicted: 2, Cosine Similarity: 7.00071306480865e-08
Cluster predicted: 0, Cosine Similarity: 8.479418323048549e-08
Cluster predicted: 2, Cosine Similarity: 9.365818831597039e-08
Cluster predicted: 0, Cosine Similarity: 3.4306373286607794e-07
Cluster predicted: 0, Cosine Similarity: 2.1371724778784795e-07
Cluster predicted: 0, Cosine Similarity: 6.579107258986028e-08
Cluster predicted: 2, Cosine Similarity: 4.8248157580310647e-08
Cluster predicted: 2, Cosine Similarity: 6.243879224765436e-08
Cluster predicted: 2, Cosine Similarity: 4.5410030735659745e-08
Cluster predicted: 0, Cosine Similarity: 3.0511679038891515e-07
Cluster predicted: 0, Cosine Similarity: 3.578749807608972e-08
Cluster predicted: 4, Cosine Similarity: 2.9662575606881347e-07
Cluster predicted: 2, Cosine Similarity: 1.2393154213974356e-07
Cluster predicted: 0, Cosine Similarity: 7.651207889480816e-08
Cluster predicted: 3, Cosine Similarity: 2.481306993207255e-08
Cluster predicted: 2, Cosine Similarity: 1.5420489596351672e-07
Cluster predicted: 4, Cosine Similarity: 3.198784801483612e-07
Cluster predicted: 0, Cosine Similarity: 1.0812465112142178e-07
Cluster predicted: 2, Cosine Similarity: 5.7708580691873124e-08
Cluster predicted: 4, Cosine Similarity: 1.568272588459152e-07
Cluster predicted: 0, Cosine Similarity: 4.686417753863026e-08
Cluster predicted: 2, Cosine Similarity: 1.1730924598385428e-07
Cluster predicted: 0, Cosine Similarity: 7.41747978594276e-08
Cluster predicted: 0, Cosine Similarity: 2.2492095341775098e-07
Cluster predicted: 2, Cosine Similarity: 9.744235751618646e-08
Cluster predicted: 0, Cosine Similarity: 9.341502393844792e-08
Cluster predicted: 0, Cosine Similarity: 1.0520304982719608e-07
Cluster predicted: 4, Cosine Similarity: 2.391329394502151e-07
Cluster predicted: 4, Cosine Similarity: 8.218949998362035e-08
Cluster predicted: 0, Cosine Similarity: 4.715210344397036e-08
Cluster predicted: 0, Cosine Similarity: 6.822150599994359e-08
Cluster predicted: 4, Cosine Similarity: 7.734310780715958e-08
Cluster predicted: 4, Cosine Similarity: 1.422382905902353e-07
Cluster predicted: 0, Cosine Similarity: 4.084313853791599e-08
Cluster predicted: 1, Cosine Similarity: 2.2295063684119043e-07
Cluster predicted: 0, Cosine Similarity: 4.7854981422013054e-08
Cluster predicted: 2, Cosine Similarity: 7.757546915954094e-08
Cluster predicted: 0, Cosine Similarity: 4.9480916031185984e-08
Cluster predicted: 3, Cosine Similarity: 3.3415481937026925e-08
Cluster predicted: 2, Cosine Similarity: 4.257190377998654e-08
Cluster predicted: 0, Cosine Similarity: 6.727304424636316e-08
Cluster predicted: 3, Cosine Similarity: 1.8946668567387803e-08
Cluster predicted: 0, Cosine Similarity: 1.9445161647535514e-07
Cluster predicted: 0, Cosine Similarity: 8.885901969790666e-08
Cluster predicted: 0, Cosine Similarity: 4.9243800592790876e-08
Cluster predicted: 0, Cosine Similarity: 3.118915178346171e-07
Cluster predicted: 0, Cosine Similarity: 1.8596627016087552e-08
Cluster predicted: 0, Cosine Similarity: 4.135971146679296e-07
Cluster predicted: 3, Cosine Similarity: 3.71620911820969e-08
Cluster predicted: 0, Cosine Similarity: 2.9569145065799773e-07
Cluster predicted: 3, Cosine Similarity: 1.760742018852568e-08
Cluster predicted: 4, Cosine Similarity: 4.413204368614032e-08
Cluster predicted: 0, Cosine Similarity: 9.699716119193624e-08
Cluster predicted: 4, Cosine Similarity: 5.779156686536169e-08
Cluster predicted: 0, Cosine Similarity: 2.765782503022507e-08
Cluster predicted: 0, Cosine Similarity: 6.49611684444551e-08
Cluster predicted: 3, Cosine Similarity: 5.347134401922915e-08
Cluster predicted: 2, Cosine Similarity: 1.1257903442807304e-07
Cluster predicted: 4, Cosine Similarity: 2.0788034882368578e-07
Cluster predicted: 0, Cosine Similarity: 1.4880688981033785e-07
Cluster predicted: 3, Cosine Similarity: 2.3342361488154495e-08
Cluster predicted: 3, Cosine Similarity: 4.014458931944631e-08
Cluster predicted: 0, Cosine Similarity: 1.2234311042025325e-07
Cluster predicted: 4, Cosine Similarity: 6.741796221287899e-08
Cluster predicted: 0, Cosine Similarity: 9.526960553785102e-09
Cluster predicted: 4, Cosine Similarity: 1.715656020584433e-07
Cluster predicted: 1, Cosine Similarity: 1.0827454195228015e-05
Cluster predicted: 4, Cosine Similarity: 1.0540902961597709e-07
Cluster predicted: 0, Cosine Similarity: 3.9691434805533277e-08
Cluster predicted: 0, Cosine Similarity: 1.1547523048882624e-07
Cluster predicted: 0, Cosine Similarity: 1.8749058416922537e-08
Cluster predicted: 3, Cosine Similarity: 7.289455505876674e-08
Cluster predicted: 0, Cosine Similarity: 2.7717103945334998e-08
Cluster predicted: 2, Cosine Similarity: 7.568338444841061e-08
Cluster predicted: 2, Cosine Similarity: 8.135963835975701e-08
Cluster predicted: 0, Cosine Similarity: 3.042953546916749e-07
Cluster predicted: 0, Cosine Similarity: 2.17799021151599e-07
Cluster predicted: 0, Cosine Similarity: 7.888153980006862e-07
Cluster predicted: 0, Cosine Similarity: 5.658506471162283e-07
Cluster predicted: 0, Cosine Similarity: 7.980967746679113e-07
Cluster predicted: 0, Cosine Similarity: 6.52999048167402e-08
Cluster predicted: 4, Cosine Similarity: 8.562512732179783e-08
Cluster predicted: 4, Cosine Similarity: 8.323512568075842e-08
Cluster predicted: 0, Cosine Similarity: 4.731300329297028e-08
Cluster predicted: 0, Cosine Similarity: 3.872603615562298e-08
Cluster predicted: 4, Cosine Similarity: 1.2507675117845451e-07
Cluster predicted: 0, Cosine Similarity: 1.6060338414725095e-07
Cluster predicted: 0, Cosine Similarity: 1.2422309736415116e-07
Cluster predicted: 0, Cosine Similarity: 5.023460447617367e-08
Cluster predicted: 0, Cosine Similarity: 1.8589852290862297e-07
Cluster predicted: 2, Cosine Similarity: 1.0028048447185967e-07
Cluster predicted: 3, Cosine Similarity: 3.4516459135325306e-08
Cluster predicted: 0, Cosine Similarity: 1.0483890822143849e-07
Cluster predicted: 3, Cosine Similarity: 5.6125192271494484e-08
Cluster predicted: 0, Cosine Similarity: 4.0826201641586124e-08
Cluster predicted: 0, Cosine Similarity: 2.9413326341209967e-07
Cluster predicted: 0, Cosine Similarity: 4.540761111559988e-08
Cluster predicted: 0, Cosine Similarity: 9.536275813459838e-08
Cluster predicted: 0, Cosine Similarity: 2.3247477465293542e-07
Cluster predicted: 0, Cosine Similarity: 4.475554360450218e-08
Cluster predicted: 0, Cosine Similarity: 1.4416620142121417e-07
Cluster predicted: 0, Cosine Similarity: 7.464056039907518e-08
Cluster predicted: 4, Cosine Similarity: 7.918540079199232e-08
Cluster predicted: 3, Cosine Similarity: 3.40563492962076e-08
Cluster predicted: 0, Cosine Similarity: 1.696476453760809e-07
Cluster predicted: 2, Cosine Similarity: 1.3339196514028373e-07
Cluster predicted: 2, Cosine Similarity: 9.64963152716436e-08
Cluster predicted: 3, Cosine Similarity: 4.9453598660598175e-08
Cluster predicted: 3, Cosine Similarity: 1.1115762343916202e-07
Cluster predicted: 3, Cosine Similarity: 6.209840475168704e-08
Cluster predicted: 0, Cosine Similarity: 4.9794247169998584e-08
Cluster predicted: 4, Cosine Similarity: 1.6165705374326933e-07
Cluster predicted: 0, Cosine Similarity: 3.2865896892886326e-08
Cluster predicted: 0, Cosine Similarity: 3.15278881890535e-08
Cluster predicted: 4, Cosine Similarity: 9.598180095160558e-08
Cluster predicted: 1, Cosine Similarity: 3.613566262550272e-06
Cluster predicted: 2, Cosine Similarity: 9.838839987175163e-08
Cluster predicted: 4, Cosine Similarity: 7.930158141267185e-08
Cluster predicted: 0, Cosine Similarity: 6.320820766791968e-08
Cluster predicted: 0, Cosine Similarity: 1.0198505429048765e-07
Cluster predicted: 0, Cosine Similarity: 1.5996825342146082e-07
Cluster predicted: 2, Cosine Similarity: 1.3623009209595693e-07
Cluster predicted: 4, Cosine Similarity: 8.424755693070551e-08
Cluster predicted: 4, Cosine Similarity: 1.4389801394898427e-07
Cluster predicted: 2, Cosine Similarity: 6.527691909230526e-08
Cluster predicted: 0, Cosine Similarity: 3.069629036511756e-07
Cluster predicted: 0, Cosine Similarity: 1.5332055203209904e-07
Cluster predicted: 2, Cosine Similarity: 2.743522687920219e-07
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 3, Cosine Similarity: 1.7640285121522936e-08
Cluster predicted: 0, Cosine Similarity: 7.072815522146669e-08
Cluster predicted: 2, Cosine Similarity: 4.6545281473520106e-07
Cluster predicted: 2, Cosine Similarity: 1.5136676911886582e-08
Cluster predicted: 2, Cosine Similarity: 5.203232689154902e-08
Cluster predicted: 3, Cosine Similarity: 5.9074825364824335e-08
Cluster predicted: 0, Cosine Similarity: 1.2860126497038493e-07
Cluster predicted: 0, Cosine Similarity: 1.3828065692500502e-07
Cluster predicted: 4, Cosine Similarity: 4.074620807426044e-08
Cluster predicted: 0, Cosine Similarity: 1.1834602120508464e-07
Cluster predicted: 0, Cosine Similarity: 5.176399924256003e-07
Cluster predicted: 0, Cosine Similarity: 4.4840227753084605e-08
Cluster predicted: 0, Cosine Similarity: 4.214727356011139e-08
Cluster predicted: 4, Cosine Similarity: 7.11689369659041e-08
Cluster predicted: 2, Cosine Similarity: 1.986688843436113e-07
Cluster predicted: 4, Cosine Similarity: 8.155880515392511e-08
Cluster predicted: 4, Cosine Similarity: 8.423095976262118e-08
Cluster predicted: 1, Cosine Similarity: 1.986688846766782e-08
Cluster predicted: 0, Cosine Similarity: 6.692583942591313e-08
Cluster predicted: 2, Cosine Similarity: 6.71690036924133e-08
Cluster predicted: 3, Cosine Similarity: 8.485741265218394e-08
Cluster predicted: 2, Cosine Similarity: 3.878773457977047e-08
Cluster predicted: 3, Cosine Similarity: 2.1501921798616763e-08
Cluster predicted: 2, Cosine Similarity: 1.8637033438739792e-07
Cluster predicted: 0, Cosine Similarity: 2.1558876628358092e-07
Cluster predicted: 0, Cosine Similarity: 3.154482497436106e-08
Cluster predicted: 4, Cosine Similarity: 8.86956155188301e-08
Cluster predicted: 4, Cosine Similarity: 1.1707688474249522e-07
Cluster predicted: 2, Cosine Similarity: 7.00071306480865e-08
Cluster predicted: 2, Cosine Similarity: 4.351794613555171e-08
Cluster predicted: 0, Cosine Similarity: 2.207714333568589e-08
Cluster predicted: 2, Cosine Similarity: 1.7974803823150864e-07
Cluster predicted: 4, Cosine Similarity: 9.296110448087092e-08
Cluster predicted: 3, Cosine Similarity: 3.917507196415926e-08
Cluster predicted: 0, Cosine Similarity: 9.066279094138707e-08
Cluster predicted: 4, Cosine Similarity: 9.906888642774447e-08
Cluster predicted: 0, Cosine Similarity: 1.3825525169153252e-07
Cluster predicted: 0, Cosine Similarity: 2.111174457475329e-08
Cluster predicted: 3, Cosine Similarity: 2.342452398718109e-08
Cluster predicted: 4, Cosine Similarity: 4.753447657712684e-08
Cluster predicted: 0, Cosine Similarity: 3.34332802554016e-08
Cluster predicted: 0, Cosine Similarity: 6.507125771548772e-08
Cluster predicted: 0, Cosine Similarity: 1.2604380539293913e-07
Cluster predicted: 4, Cosine Similarity: 1.0003152595139397e-07
Cluster predicted: 4, Cosine Similarity: 2.1659389615180658e-08
Cluster predicted: 3, Cosine Similarity: 1.6498226673711258e-08
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 4, Cosine Similarity: 1.6713414074942534e-07
Cluster predicted: 0, Cosine Similarity: 4.130043262939864e-08
Cluster predicted: 2, Cosine Similarity: 5.014024218041868e-08
Cluster predicted: 0, Cosine Similarity: 1.1049580572741746e-07
Cluster predicted: 3, Cosine Similarity: 5.578832606989437e-08
Cluster predicted: 2, Cosine Similarity: 1.1068694982796501e-07
Cluster predicted: 0, Cosine Similarity: 3.527092506949714e-08
Cluster predicted: 4, Cosine Similarity: 4.6123711738843554e-08
Cluster predicted: 4, Cosine Similarity: 4.630628125390501e-08
Cluster predicted: 2, Cosine Similarity: 5.581649609176509e-08
Cluster predicted: 0, Cosine Similarity: 2.589300855948551e-07
Cluster predicted: 0, Cosine Similarity: 1.6443110517627701e-07
Cluster predicted: 4, Cosine Similarity: 1.9176443522006537e-07
Cluster predicted: 0, Cosine Similarity: 1.282286550274847e-07
Cluster predicted: 2, Cosine Similarity: 1.0595673827218377e-07
Cluster predicted: 0, Cosine Similarity: 5.016685722392111e-08
Cluster predicted: 2, Cosine Similarity: 2.365105765678166e-07
Cluster predicted: 0, Cosine Similarity: 7.685928360423588e-08
Cluster predicted: 4, Cosine Similarity: 1.070355585275351e-07
Cluster predicted: 0, Cosine Similarity: 3.5016872845794467e-08
Cluster predicted: 0, Cosine Similarity: 3.745577470404271e-08
Cluster predicted: 0, Cosine Similarity: 1.9638241377517573e-07
Cluster predicted: 4, Cosine Similarity: 1.0315180587472383e-07
Cluster predicted: 0, Cosine Similarity: 1.9526458372443045e-07
Cluster predicted: 0, Cosine Similarity: 1.30913140772293e-07
Cluster predicted: 0, Cosine Similarity: 5.7195637004348043e-08
Cluster predicted: 0, Cosine Similarity: 2.370646524640918e-07
Cluster predicted: 0, Cosine Similarity: 1.139763220470158e-07
Cluster predicted: 4, Cosine Similarity: 5.017343669422303e-07
Cluster predicted: 0, Cosine Similarity: 1.578511510391678e-08
Cluster predicted: 4, Cosine Similarity: 6.861296308890985e-08
Cluster predicted: 2, Cosine Similarity: 3.594960762409727e-08
Cluster predicted: 0, Cosine Similarity: 2.1663884908207365e-07
Cluster predicted: 2, Cosine Similarity: 1.0311861142753287e-07
Cluster predicted: 0, Cosine Similarity: 2.437716327685635e-07
Cluster predicted: 4, Cosine Similarity: 9.208145113070998e-08
Cluster predicted: 3, Cosine Similarity: 8.541611751233802e-08
Cluster predicted: 0, Cosine Similarity: 1.5219425353318883e-07
Cluster predicted: 4, Cosine Similarity: 4.033127731783992e-08
Cluster predicted: 4, Cosine Similarity: 1.354500220873689e-07
Cluster predicted: 4, Cosine Similarity: 7.256310463610305e-08
Cluster predicted: 0, Cosine Similarity: 7.766378251616857e-08
Cluster predicted: 0, Cosine Similarity: 2.707350477137993e-08
Cluster predicted: 1, Cosine Similarity: 0.48605995804225044
Cluster predicted: 4, Cosine Similarity: 1.0660403038098565e-07
Cluster predicted: 0, Cosine Similarity: 3.6913796574999935e-08
Cluster predicted: 3, Cosine Similarity: 2.78037842749157e-08
Cluster predicted: 0, Cosine Similarity: 2.2830831669651275e-08
Cluster predicted: 0, Cosine Similarity: 6.900059967840377e-08
Cluster predicted: 2, Cosine Similarity: 1.5420489596351672e-07
Cluster predicted: 0, Cosine Similarity: 3.1727742622056354e-07
Cluster predicted: 0, Cosine Similarity: 4.086854377138849e-08
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 0, Cosine Similarity: 8.283798069719239e-08
Cluster predicted: 4, Cosine Similarity: 2.1212924061764227e-07
Cluster predicted: 2, Cosine Similarity: 1.8258616507615955e-07
Cluster predicted: 4, Cosine Similarity: 1.1130104748957592e-07
Cluster predicted: 0, Cosine Similarity: 1.7885280589879926e-08
Cluster predicted: 2, Cosine Similarity: 1.2203945753963552e-07
Cluster predicted: 0, Cosine Similarity: 5.0488656810898647e-08
Cluster predicted: 0, Cosine Similarity: 2.0391082977688768e-07
Cluster predicted: 2, Cosine Similarity: 7.095317300365167e-08
Cluster predicted: 4, Cosine Similarity: 7.744269125975478e-08
Cluster predicted: 0, Cosine Similarity: 2.788223791849731e-07
Cluster predicted: 0, Cosine Similarity: 4.014872889701593e-08
Cluster predicted: 0, Cosine Similarity: 1.1542442002188125e-07
Cluster predicted: 0, Cosine Similarity: 5.5222497685747385e-08
Cluster predicted: 0, Cosine Similarity: 7.34719198813849e-08
Cluster predicted: 0, Cosine Similarity: 3.506768331273946e-08
Cluster predicted: 4, Cosine Similarity: 1.1332590998947012e-07
Cluster predicted: 0, Cosine Similarity: 5.950412542699013e-07
Cluster predicted: 2, Cosine Similarity: 4.067981917987851e-08
Cluster predicted: 2, Cosine Similarity: 2.7246018419191387e-07
Cluster predicted: 2, Cosine Similarity: 3.5192773795156285e-07
Cluster predicted: 2, Cosine Similarity: 9.271214596040522e-08
Cluster predicted: 4, Cosine Similarity: 1.9217936608750819e-07
Cluster predicted: 2, Cosine Similarity: 5.2978369136091885e-08
Cluster predicted: 1, Cosine Similarity: 1.0043815823479463e-07
Cluster predicted: 0, Cosine Similarity: 8.427761022389291e-08
Cluster predicted: 4, Cosine Similarity: 1.1677813449573193e-07
Cluster predicted: 0, Cosine Similarity: 1.2335931964813085e-07
Cluster predicted: 4, Cosine Similarity: 2.0731604288837246e-07
Cluster predicted: 3, Cosine Similarity: 1.887272238487725e-08
Cluster predicted: 4, Cosine Similarity: 3.28243485836488e-07
Cluster predicted: 3, Cosine Similarity: 1.561908802294454e-08
Cluster predicted: 0, Cosine Similarity: 2.2780021313728582e-08
Cluster predicted: 0, Cosine Similarity: 1.2506146984669897e-07
Cluster predicted: 0, Cosine Similarity: 6.674800279160564e-08
Cluster predicted: 3, Cosine Similarity: 1.4365288558693123e-07
Cluster predicted: 0, Cosine Similarity: 8.231293924243488e-08
Cluster predicted: 3, Cosine Similarity: 2.891297778973012e-08
Cluster predicted: 0, Cosine Similarity: 2.0516415433213808e-07
Cluster predicted: 2, Cosine Similarity: 3.121939606831603e-08
Cluster predicted: 4, Cosine Similarity: 1.111350751425988e-07
Cluster predicted: 0, Cosine Similarity: 9.930056854567937e-08
Cluster predicted: 4, Cosine Similarity: 1.927436720228215e-07
Cluster predicted: 0, Cosine Similarity: 4.815137577551809e-08
Cluster predicted: 3, Cosine Similarity: 3.7827607268781094e-08
Cluster predicted: 0, Cosine Similarity: 2.4020643241140505e-07
Cluster predicted: 0, Cosine Similarity: 1.4867986364297536e-07
Cluster predicted: 0, Cosine Similarity: 2.8567332244211485e-07
Cluster predicted: 4, Cosine Similarity: 6.872914370958938e-08
Cluster predicted: 0, Cosine Similarity: 1.066426794649189e-07
Cluster predicted: 0, Cosine Similarity: 1.9383342253487257e-07
Cluster predicted: 4, Cosine Similarity: 9.110221432795385e-08
Cluster predicted: 4, Cosine Similarity: 1.6064462249332223e-07
Cluster predicted: 4, Cosine Similarity: 1.321637699280842e-07
Cluster predicted: 0, Cosine Similarity: 1.2269878368886822e-07
Cluster predicted: 1, Cosine Similarity: 0.17615406856414495
Cluster predicted: 3, Cosine Similarity: 2.5872965991879937e-08
Cluster predicted: 0, Cosine Similarity: 2.3722555231309173e-07
Cluster predicted: 4, Cosine Similarity: 6.511094674532814e-08
Cluster predicted: 0, Cosine Similarity: 9.371141829195295e-08
Cluster predicted: 0, Cosine Similarity: 7.963692194579153e-08
Cluster predicted: 0, Cosine Similarity: 6.431756927050003e-08
Cluster predicted: 0, Cosine Similarity: 7.663656447221001e-07
Cluster predicted: 2, Cosine Similarity: 7.00071306480865e-08
Cluster predicted: 0, Cosine Similarity: 6.645160843810061e-08
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[123]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">yTpred</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cosDmat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cosDmat</span><span class="p">))]</span>
<span class="n">yTpred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">yTpred</span><span class="p">)</span>
<span class="n">c</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">yTpred</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted cluster&quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Predicted cluster 0 949
Predicted cluster 1 33
Predicted cluster 2 460
Predicted cluster 3 188
Predicted cluster 4 370
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&#160;[&#160;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>


 



<script type="text/javascript" src="/d2l/common/math/MathML.js?v=20.21.2.27566-324 "></script><script type="text/javascript">document.addEventListener('DOMContentLoaded', function() { D2LMathML.DesktopInit('https://s.brightspace.com/lib/mathjax/2.7.4/MathJax.js?config=MML_HTMLorMML','https://s.brightspace.com/lib/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML','130'); });</script></body></html>