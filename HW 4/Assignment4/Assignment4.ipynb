{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) PCA for Reduced Dimensionality in Clustering [Dataset: segmentation_data.zip]\n",
    "For this problem you will use an image segmentation data set for clustering. You will experiment with using PCA as an approach to reduce dimensionality and noise in the data. You will compare the results of clustering the data with and without PCA using the provided image class assignments as the ground truth. The data set is divided into three files. The file \"segmentation_data.txt\" contains data about images with each line corresponding to one image. Each image is represented by 19 features (these are the columns in the data and correspond to the feature names in the file \"segmentation_names.txt\". The file \"segmentation_classes.txt\" contains the class labels (the type of image) and a numeric class label for each of the corresponding images in the data file. After clustering the image data, you will use the class labels to measure completeness and homogeneity of the generated clusters. The data set used in this problem is based on the Image Segmentation data set at the UCI Machine Learning Repository.\n",
    "\n",
    "Your tasks in this problem are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load in the image data matrix (with rows as images and columns as features). Also load in the numeric class labels from the segmentation class file. Using your favorite method (e.g., sklearn's min-max scaler), perform min-max normalization on the data matrix so that each feature is scaled to [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('segmentation_data.txt', sep = ',', header = None)\n",
    "df_classes =  pd.read_csv('segmentation_classes.txt', sep = '\\t', header = None)\n",
    "df_name = pd.read_csv('segmentation_names.txt', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>12.925926</td>\n",
       "      <td>10.888889</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>18.666668</td>\n",
       "      <td>-6.111111</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>17.222221</td>\n",
       "      <td>18.666668</td>\n",
       "      <td>0.508139</td>\n",
       "      <td>1.910864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.720082</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.750309</td>\n",
       "      <td>13.740741</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>10.333334</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>-6.222222</td>\n",
       "      <td>-10.222222</td>\n",
       "      <td>16.444445</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>0.463329</td>\n",
       "      <td>1.941465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>2.195113</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.520234</td>\n",
       "      <td>12.259259</td>\n",
       "      <td>10.333334</td>\n",
       "      <td>9.333334</td>\n",
       "      <td>17.111110</td>\n",
       "      <td>-5.777778</td>\n",
       "      <td>-8.777778</td>\n",
       "      <td>14.555555</td>\n",
       "      <td>17.111110</td>\n",
       "      <td>0.480149</td>\n",
       "      <td>1.987902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>1.254621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>12.703704</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.111110</td>\n",
       "      <td>-5.111111</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>16.222221</td>\n",
       "      <td>18.111110</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>1.875362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.691215</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.005540</td>\n",
       "      <td>15.592592</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>11.777778</td>\n",
       "      <td>21.111110</td>\n",
       "      <td>-5.111111</td>\n",
       "      <td>-11.444445</td>\n",
       "      <td>16.555555</td>\n",
       "      <td>21.111110</td>\n",
       "      <td>0.442661</td>\n",
       "      <td>1.863654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1   2    3    4         5         6         7         8   \\\n",
       "0  110.0  189.0   9  0.0  0.0  1.000000  0.666667  1.222222  1.186342   \n",
       "1   86.0  187.0   9  0.0  0.0  1.111111  0.720082  1.444444  0.750309   \n",
       "2  225.0  244.0   9  0.0  0.0  3.388889  2.195113  3.000000  1.520234   \n",
       "3   47.0  232.0   9  0.0  0.0  1.277778  1.254621  1.000000  0.894427   \n",
       "4   97.0  186.0   9  0.0  0.0  1.166667  0.691215  1.166667  1.005540   \n",
       "\n",
       "          9          10         11         12        13         14         15  \\\n",
       "0  12.925926  10.888889   9.222222  18.666668 -6.111111 -11.111111  17.222221   \n",
       "1  13.740741  11.666667  10.333334  19.222221 -6.222222 -10.222222  16.444445   \n",
       "2  12.259259  10.333334   9.333334  17.111110 -5.777778  -8.777778  14.555555   \n",
       "3  12.703704  11.000000   9.000000  18.111110 -5.111111 -11.111111  16.222221   \n",
       "4  15.592592  13.888889  11.777778  21.111110 -5.111111 -11.444445  16.555555   \n",
       "\n",
       "          16        17        18  \n",
       "0  18.666668  0.508139  1.910864  \n",
       "1  19.222221  0.463329  1.941465  \n",
       "2  17.111110  0.480149  1.987902  \n",
       "3  18.111110  0.500966  1.875362  \n",
       "4  21.111110  0.442661  1.863654  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REGION-CENTROID-COL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REGION-CENTROID-ROW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REGION-PIXEL-COUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHORT-LINE-DENSITY-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SHORT-LINE-DENSITY-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0   REGION-CENTROID-COL\n",
       "1   REGION-CENTROID-ROW\n",
       "2    REGION-PIXEL-COUNT\n",
       "3  SHORT-LINE-DENSITY-5\n",
       "4  SHORT-LINE-DENSITY-2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_name.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>CEMENT</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>CEMENT</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>CEMENT</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>CEMENT</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>CEMENT</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1\n",
       "0      GRASS  0\n",
       "1      GRASS  0\n",
       "2      GRASS  0\n",
       "3      GRASS  0\n",
       "4      GRASS  0\n",
       "...      ... ..\n",
       "2095  CEMENT  3\n",
       "2096  CEMENT  3\n",
       "2097  CEMENT  3\n",
       "2098  CEMENT  3\n",
       "2099  CEMENT  3\n",
       "\n",
       "[2100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43083004, 0.74166667, 0.        , ..., 0.12371135, 0.50813884,\n",
       "        0.83184923],\n",
       "       [0.33596838, 0.73333333, 0.        , ..., 0.12739322, 0.46332908,\n",
       "        0.83698646],\n",
       "       [0.88537549, 0.97083333, 0.        , ..., 0.11340205, 0.48014903,\n",
       "        0.84478233],\n",
       "       ...,\n",
       "       [0.50197628, 0.625     , 0.        , ..., 0.07216495, 0.5409177 ,\n",
       "        0.17591546],\n",
       "       [0.58893281, 0.6125    , 0.        , ..., 0.08100147, 0.50308645,\n",
       "        0.18478933],\n",
       "       [0.48616601, 0.62916667, 0.        , ..., 0.09646539, 0.4799313 ,\n",
       "        0.17037463]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "norm = MinMaxScaler()\n",
    "df_N = norm.fit_transform(df_data)\n",
    "df_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using the Kmeans implementation in scikit-learn, perform clustering on the image data (use K = 7 in your clustering so that later we can compare the clusters to the 7 pre-assigned image classes). Print the cluster centroids (use some formatting so that they are visually understandable). To evaluate your clusters, first perform Silhouette analysis on the clusters (compute Silhouette values for all instances in the data, and then compute the overall mean Silhouette value; optionally, you can provide a visaulization of the Silhouettes). Next, compare your 7 clusters to the 7 pre-assigned classes by computing the Completeness and Homogeneity values of the generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=7,max_iter=1000,verbose=0)\n",
    "kmeans.fit(df_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 6, 6, 5], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = kmeans.predict(df_N)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "...       ...\n",
       "2095        2\n",
       "2096        5\n",
       "2097        6\n",
       "2098        6\n",
       "2099        5\n",
       "\n",
       "[2100 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clusters, columns=[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(REGION-CENTROID-COL,)</th>\n",
       "      <th>(REGION-CENTROID-ROW,)</th>\n",
       "      <th>(REGION-PIXEL-COUNT,)</th>\n",
       "      <th>(SHORT-LINE-DENSITY-5,)</th>\n",
       "      <th>(SHORT-LINE-DENSITY-2,)</th>\n",
       "      <th>(VEDGE-MEAN,)</th>\n",
       "      <th>(VEDGE-SD,)</th>\n",
       "      <th>(HEDGE-MEAN,)</th>\n",
       "      <th>(HEDGE-SD,)</th>\n",
       "      <th>(INTENSITY-MEAN,)</th>\n",
       "      <th>(RAWRED-MEAN,)</th>\n",
       "      <th>(RAWBLUE-MEAN,)</th>\n",
       "      <th>(RAWGREEN-MEAN,)</th>\n",
       "      <th>(EXRED-MEAN,)</th>\n",
       "      <th>(EXBLUE-MEAN,)</th>\n",
       "      <th>(EXGREEN-MEAN,)</th>\n",
       "      <th>(VALUE-MEAN,)</th>\n",
       "      <th>(SATURATION-MEAN,)</th>\n",
       "      <th>(HUE-MEAN,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (REGION-CENTROID-COL,)  (REGION-CENTROID-ROW,)  (REGION-PIXEL-COUNT,)  \\\n",
       "0                    0.51                    0.81                   0.00   \n",
       "1                    0.54                    0.15                   0.00   \n",
       "2                    0.25                    0.46                   0.00   \n",
       "3                    0.75                    0.53                   0.00   \n",
       "4                    0.30                    0.53                   0.00   \n",
       "5                    0.26                    0.39                   0.00   \n",
       "6                    0.77                    0.43                   0.00   \n",
       "\n",
       "   (SHORT-LINE-DENSITY-5,)  (SHORT-LINE-DENSITY-2,)  (VEDGE-MEAN,)  \\\n",
       "0                     0.08                     0.01           0.05   \n",
       "1                     0.03                     0.00           0.03   \n",
       "2                     0.03                     0.01           0.04   \n",
       "3                     0.04                     0.04           0.11   \n",
       "4                     0.05                     0.05           0.10   \n",
       "5                     0.07                     0.02           0.08   \n",
       "6                     0.01                     0.02           0.04   \n",
       "\n",
       "   (VEDGE-SD,)  (HEDGE-MEAN,)  (HEDGE-SD,)  (INTENSITY-MEAN,)  (RAWRED-MEAN,)  \\\n",
       "0         0.00           0.05         0.00               0.11            0.09   \n",
       "1         0.00           0.03         0.00               0.82            0.78   \n",
       "2         0.00           0.03         0.00               0.03            0.02   \n",
       "3         0.02           0.11         0.02               0.30            0.28   \n",
       "4         0.01           0.08         0.01               0.40            0.37   \n",
       "5         0.00           0.06         0.00               0.15            0.14   \n",
       "6         0.00           0.02         0.00               0.04            0.04   \n",
       "\n",
       "   (RAWBLUE-MEAN,)  (RAWGREEN-MEAN,)  (EXRED-MEAN,)  (EXBLUE-MEAN,)  \\\n",
       "0             0.09              0.14           0.68            0.08   \n",
       "1             0.89              0.79           0.27            0.67   \n",
       "2             0.04              0.02           0.77            0.22   \n",
       "3             0.35              0.27           0.59            0.45   \n",
       "4             0.47              0.35           0.50            0.57   \n",
       "5             0.19              0.12           0.72            0.34   \n",
       "6             0.06              0.03           0.78            0.22   \n",
       "\n",
       "   (EXGREEN-MEAN,)  (VALUE-MEAN,)  (SATURATION-MEAN,)  (HUE-MEAN,)  \n",
       "0             0.82           0.13                0.41         0.89  \n",
       "1             0.29           0.89                0.21         0.13  \n",
       "2             0.51           0.04                0.80         0.18  \n",
       "3             0.31           0.35                0.30         0.16  \n",
       "4             0.21           0.47                0.30         0.16  \n",
       "5             0.36           0.19                0.41         0.20  \n",
       "6             0.49           0.06                0.54         0.24  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=df_name)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57202831 0.56064636 0.46584172 0.49522097 0.57151852 0.56037254\n",
      " 0.54213796 0.41254281 0.51968142 0.46733032 0.48661677 0.49108474\n",
      " 0.58463969 0.56222519 0.37949564 0.53083575 0.54919193 0.42227935\n",
      " 0.41618572 0.40389039]\n"
     ]
    }
   ],
   "source": [
    "# Code from Course Jupyter notebook\n",
    "from sklearn import metrics\n",
    "silhouettes = metrics.silhouette_samples(df_N, clusters)\n",
    "print(silhouettes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.332066475286672\n"
     ]
    }
   ],
   "source": [
    "# Code from Course Jupyter notebook\n",
    "print(silhouettes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouettes(data, clusters, metric='euclidean'):\n",
    "    \n",
    "    from matplotlib import cm\n",
    "    from sklearn.metrics import silhouette_samples\n",
    "\n",
    "    cluster_labels = np.unique(clusters)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = metrics.silhouette_samples(data, clusters, metric='euclidean')\n",
    "    c_ax_lower, c_ax_upper = 0, 0\n",
    "    cticks = []\n",
    "    for i, k in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[clusters == k]\n",
    "        c_silhouette_vals.sort()\n",
    "        c_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(float(i) / n_clusters)\n",
    "        pl.barh(range(c_ax_lower, c_ax_upper), c_silhouette_vals, height=1.0, \n",
    "                      edgecolor='none', color=color)\n",
    "\n",
    "        cticks.append((c_ax_lower + c_ax_upper) / 2)\n",
    "        c_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    pl.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "    pl.yticks(cticks, cluster_labels)\n",
    "    pl.ylabel('Cluster')\n",
    "    pl.xlabel('Silhouette coefficient')\n",
    "\n",
    "    pl.tight_layout()\n",
    "    #pl.savefig('images/11_04.png', dpi=300)\n",
    "    pl.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAayklEQVR4nO3de5hkdX3n8feX+1VABpEFOkMMYlxXB9OrkWhkBBVR0A0oamDFGMegUVk1KyA+7ipBjTHibsA4ooIalVlYd1mCyMWZJSoYZmAYbgsq4jKsN7xFkYiD3/3jnGaKpruruqZOnUu9X8/Tz6nLqfp9uqZ7Pv07deqcyEwkSWqareoOIEnSXCwoSVIjWVCSpEayoCRJjWRBSZIaaZu6A/RasmRJLl26tO4YUnfcfnuxPOigenNIpXXr1t2bmXsNsm6jCmrp0qWsXbu27hhSdxx6aLFcs6bOFNJDIuI7g67rJj5JUiNZUJKkRrKgJEmN1Kj3oCSN2Ekn1Z1AGpoFJXXZccfVnUAampv4pC67++7iS2ohZ1BSl51wQrF0N3O1kDMoSVIjWVCSpEZyE18bHBx1J1BbfaNcNvFn6AZPlqqFWVCSqmMJaQtYUFKXPWYMY1hCqogFJXXZblvwWItHNbOgpC77l3K5w4DrW0pqEAtK6rKZz+geiOWj1rGgpC6ZXUKeD0otZkFJbeWMSB1nQUlNZglpgllQUt0sIWlOFpQ0LnUU0emnj39MaUQsKKkqTZgZHX543QmkoVlQ0pZqQhHNZ/36YrlsWb05pCFUWlARsTtwLvAkIIE/ycxrqhxTqkyTi2g+J59cLN3NXC1U9Qzqw8BlmXlsRGwH7FTxeNLotLGQpA6prKAiYjfgD4ETATLzAeCBqsaTRsJSkhqjyhnUAcAPgU9GxFOAdcCbM/O+3pUiYgWwAmBqaqrCONI8LCWpkao8o+42wFOBj2TmwcB9wCmzV8rMlZk5nZnTe+21V4VxpNIN+fAvSY1U5QxqI7AxM79eXr+QOQpKGotJLaIzz6w7gTS0ygoqM78XEXdHxEGZeTtwGHBrVeNJD5nUMprLIYfUnUAaWtV78b0R+PtyD747gVdXPJ4mkYU0v699rVhaVGqhSgsqM9cD01WOoQlkIQ3utNOKpZ+DUgt5JAk1n4UkTSQLSs1hEUnqYUGpXpaSpHlYUBofy0jSIlhQGg3Lp5nOOqvuBNLQLCjNzcLpBk+zoRazoCaJpTN5rryyWHriQrWQBdU2lowW44wziqUFpRayoEbt4Bj9c1pKkibQZBVUFeUhSarEZBVUW2cil1usGtJPyuU4foae19LfLzXWZBWUpNGylFQhC0rqsjeN4DksIdXEgpK6bP8hHmMhqSEsKKnLri2Xvz/P/ZaRGsyCkrrsonLZW1CWklrCgpK6bo9nw/PW1J1CWrSt6g4gqSLPy6KcpJZyBiV1hZvu1DEWlNQ2FpEmhAUlNd2WFNKnPz26HNKYWVBSE41qlrT/MB+EkprBgpKaZNSb7y64oFged9xon1caAwtKaooq3lv6yEeKpQWlFqq0oCLiLuDnwIPApsycrnI8qXXc4UGa1zhmUMsz894xjCM1m2UkLYqb+KQqWEbSFqu6oBK4PCIS+Ghmrpy9QkSsAFYATE1NVRxHqoiFJI1c1QX1zMy8JyIeA1wREf8nM6/uXaEsrZUA09PT/par+dpURhdeWHcCaWiVFlRm3lMufxARXwCeBly98KOkhmlTIc22ZEndCaShVVZQEbEzsFVm/ry8/Dzg3VWNJ41Em8toLuedVyxPPLHOFNJQqpxB7Q18ISJmxvlsZl5W4XjS4LpWRPOxoNRilRVUZt4JPKWq55cWNCkFJHWYu5mrOywlqVMsKLWXhSR1mgWl5rOIpIlkQal+FlB1Lr207gTS0CwoVc8Cqs9OO9WdQBqaBaXRsISa6ZxziuXrX19vDmkIFpQGYwG106pVxdKCUgtZUJqbhSSpZhbUJLF0JLWIBdU1lpCkjrCgmubyeORtlo6kCWRBNc0cZXQZy2sIok5YM/MHT/t/ho5gdd0RNGYWlKTaWDpaiAUlddjSv/6/ANz1tqmakxQsJC2GBSV12GMu+REw3oKyhDQqFpSkoVhEqpoFJWlelpDqZEFJeoiFpCaxoKQOe3DHrfuuYympqSwoqcPWffHJD7tuGalNKi+oiNgaWAvck5kvqno8SZtZSGqzccyg3gzcBjxqDGNJE2neInrPe4rlO985vjDSiFRaUBGxH/BC4C+Bt1Q5ljQJFj0juuqqYmlBqYWqnkGdBfxHYNf5VoiIFcAKgKmpZnzaXaqLm+SkzSorqIh4EfCDzFwXEYfOt15mrgRWAkxPT3vYbk0My0haWJUzqD8Ajo6II4EdgEdFxGcy8/gKx5QaxyKShlNZQWXmqcCpAOUM6m2Wk7qucWW05551J5CG5uegpEVoXAH1c9FFdSeQhjaWgsrMNcCacYwljULrikjqIGdQmggTWzinnlos3/veenNIQ7Cg1BkTW0ILueaauhNIQ7Og1GiWjjS5LChVyoKRNCwLSn1ZMpLqYEEJsIQ6a7/96k4gDS0ym3N0oenp6Vy7dm3dMRpnOZfVHUEaq9UcUXcEVSQi1mXm9CDrOoOSVDkLR8PoW1DlCQfflJkfGkMeSSP0hpP/DoCzz/qzsY1pGWlU+hZUZj4YEa8ALCipZX5n/Z1jG8ti0qgNuonvqxHxt8AFwH0zN2bm9ZWkktQaFpOqMmhBLSuX7+65LYHnjDaOpDawlDQOAxVUZi6vOoik5rKQVIeBCioi9gbOBP5VZr4gIp4IPCMzP15pOklbZOPj9130YywjNcWgm/jOAz4JvKO8fgfF+1EWlNRgH1z55kfcZgGpLQYtqCWZuSoiTgXIzE0R8WCFuSSNmMWkthm0oO6LiD0pdowgIn4f+FllqSRtsdUcAStWFFdWrqw3jDSEQQvqLcDFwOMi4qvAXsBLK0slaUEDz4buuKPaIFKFBi2oW4BnAwcBAdwObFVVKEmbuWlOk2rQgromM59KUVQARMT1wFMrSSVNEAtImtuCBRURjwX2BXaMiIMpZk8AjwJ26vPYHYCrge3LcS7MzHdtcWKpJSweacv0m0E9HzgR2A/4IJsL6ufAaX0e+yvgOZn5i4jYFvhKRHwxM6/dgrxSozS+hJYt67+O1FALFlRmng+cHxHHZOZFi3niLE409Yvy6rblV3NOPiUtUuPLaC5nnVV3Amlog74HtV9EPIpi5vQxiveeTsnMyxd6UHmqjnXA7wBnZ+bX51hnBbACYGpqahHRpWq1spCkDhm0oP4kMz8cEc8H9gROAD4NLFhQmfkgsCwidge+EBFPysybZ62zElgJxRl1F/sNSFui8yV0/PHF8jOfqTeHNIRBC2rmvacjgU9l5i0REQs9oFdm/jQiVgNHADf3W18atc4X0Xw2bqw7gTS0QQtqXURcDhwAnBoRuwK/WegBEbEX8OuynHYEngu8f4vSSoswsaUkdcSgBfUainNC3ZmZvywPe/TqPo/Zh2IHi60pPtS7KjMvGT6qtDALSeqWQQvqmeXyyYNu2cvMDcDBw4SS5mMJSZNj0IL6i57LOwBPo9g7zzPqqjKW0Qg84xl1J5CGNugZdY/qvR4R+wN+wEIjZymN2HvfW3cCaWiDzqBm2wj87iiDaDJYQJIGNegp3/8rm48CsRXFDhPXVxVK7WPxNNQxxxTLixZ1IBipEQadQa3tubwJ+FxmfrWCPGo4i6hlfvSjuhNIQxv0Pajzqw6iZrCAJDVFv9Nt3MQCB3jNzCePPJHGwiKS1HT9ZlB/BOwN3D3r9v2B71WSSCNlEUlqq34F9SHg1Mz8Tu+N5ZHNPwQcNeejNHYWkeZ02GF1J5CG1q+g9s7Mm2bfmJk3RcTSShJpIBaSBvLOd9adQBpav4LafYH7dhxlkEm2nMsWvN8ykjSJ+hXU2oh4bWZ+rPfGiPhTikMdaQT6FVB8dkxB1DmXvv8FABz59i/WnES98pV1J2iHfgV1MsWJBv+YzYU0DWwH/Lsqg0nacjv++v66IwgLaVgLFlRmfh84JCKWA08qb/6HzPxy5ckkqaUspNEY9IO6q4HVFWeRpNaylEZv2IPFStJEsojGx4KSOuySg19Ud4TWsojqZ0FJHfbBF76t7gitYik1iwUlaeJZTM1kQUkdtvqMQwFYfvqaWnM0kaXUfBaUpE6ziNrLgpLUCRZR91RWUBGxP/ApitN1JLAyMz9c1XiSusnimVxVzqA2AW/NzOsjYldgXURckZm3VjimpBawdDSIygoqM78LfLe8/POIuA3YF7CgpDFZ9fSXjX1My0ejMpb3oMpzRx0MfH2O+1YAKwCmpqbGEUeaGB957usrfX7LSFWqvKAiYhfgIuDkzPzn2fdn5kpgJcD09HRWnUeaJDv+6pcA3L/9TkM93gJSnSotqIjYlqKc/j4z/3uVY0l6pEs/cCQw/+egLCA1WZV78QXwceC2zPybqsaRVJizbFYucJ/UcFXOoP4AOAG4KSLWl7edlpmXVjim1GkWjSZJlXvxfQWIqp5f6jKLSPJIElItLCCpPwtKGqOxF9OJJ455QGl0LChphBo3M7Kg1GIWlDSkxpXRXO69t1guWVJvDmkIFpS0gFaU0EKOPbZYrllTawxpGBaUJl7rS0jqKAtKnWXxSO1mQanxLBppMllQqoWlI6kfC6oN/rzuAKMXHfyemuhlD5wEwKpH1xxErZc/Hv+YFpTUYau2O67uCGqxOkqplwUlddh+v7kbgI1b7V9zErVB3YU0mwUlddin7zsBgOW7rqk3iBqraaXUy4KSpAnQ5CKajwUlSR3VxlLqtVXdASRJo9f2cgJnUJLUGV0opV4WlNRhH9z+rXVHUMW6Vkq9LCipwy7Z7qi6I2gEulxCC7GgpA57/IO3A3DH1gfVnESLNaml1MuCkjrso798HeDnoNrEYtqssr34IuITEfGDiLi5qjEkqQvyx5u/tFmVu5mfBxxR4fNLUqtZSgurrKAy82rAl16S5mAx9Vf7e1ARsQJYATA1NVVzGkkaPctoOLUXVGauBFYCTE9PZ81xpE45Y4fT644w0SymLVN7QUmqzlXbHl53hIljKY2OBSV12FM2rQfgxm2W1Zyk2yylalRWUBHxOeBQYElEbATelZkfr2o8SY901v0nA34OatQspPGorKAy8xVVPbckjZulNH5u4pOkeVhK9bKgJE00S6i5LChJE8MyahcLSuqw03Y8s+4IY2P5dI8FJXXYNdscUneEkbKEJosFJXXYMzZ9DWhWUVkyGpQFJXXYmfefBlTzOSiLRlWzoKQJZ9GoqSwoqWMeVjiHlretqSGItIUsKKlGzl6k+VlQbfCT/1x3AlUkotrnX81dACwPf4YmXea76o6waBaU1GEnc0TdEVSjNpZSLwtK6rAb2afuCKpB24tphgUlddhhfAuAq3hczUlUha4U0XwsKKnDTudqwILqgq6X0VwsKElqmEkso7lYUJJUMwtpbhaUJNXAUurPgpKkMbGUFseCkjrsdRxVd4SJZiFtGQtK6rA7WFJ3hIlkMY2GBSV12Iu4HYBLOKjmJN1nKY1epQUVEUcAHwa2Bs7NzPdVOZ6kh3srxQkLLahqWErVqqygImJr4GzgucBG4LqIuDgzb61qTEmqmqU0PlXOoJ4GfDMz7wSIiM8DLwYsKEmtYSHVp8qC2he4u+f6RuDps1eKiBXACoCpqakK40jS4Cym+tW+k0RmrgRWAkxPT2fNcSRNMEupWaosqHuA/Xuu71feJmlMTuCP6o7QeJZSc1VZUNcBB0bEARTF9HLglRWOJ2mWjexWd4TGsZDao7KCysxNEfHnwJcodjP/RGbeUtV4kh7pZdwMwCqeVHOS+llM7VPpe1CZeSlwaZVjSJrfSVwHTGZBWUjtV/tOEpI0KpZSt1hQklrNUuouC0pSq1hIk8OCktQ4lpDAgpI67VheVneEviwjzceCkjrsR+xcd4SHsYy0GBaU1GGv4gYAzufgsY5rEWkULCipw05kPTD6grKANA4WlDQhLBW1jQXVAv7HoqEduhqAXOPPkNpnq7oDSJI0FwtKktRIbuKTuuxSj9Ws9rKgpC7baae6E0hDcxOf1GXnnFN8SS1kQUldtmpV8SW1kAUlSWokC0qS1EgWlCSpkSwoSVIjRWbWneEhEfFD4DtjGGoJcO8YxhmVNuVtU1ZoV942ZYV25W1TVmhX3tlZfysz9xrkgY0qqHGJiLWZOV13jkG1KW+bskK78rYpK7Qrb5uyQrvybklWN/FJkhrJgpIkNdKkFtTKugMsUpvytikrtCtvm7JCu/K2KSu0K+/QWSfyPShJUvNN6gxKktRwFpQkqZEmoqAi4tERcUVEfKNc7jHPepdFxE8j4pJxZyzHPyIibo+Ib0bEKXPcv31EXFDe//WIWDr+lA9l6Zf1DyPi+ojYFBHH1pFxVp5+ed8SEbdGxIaIuCoifquOnGWWfln/LCJuioj1EfGViHhiHTl78iyYt2e9YyIiI6K23aMHeG1PjIgflq/t+oj40zpylln6vq4R8bLy5/aWiPjsuDPOytLvtf1Qz+t6R0T8tO+TZmbnv4C/Ak4pL58CvH+e9Q4DjgIuqSHj1sC3gN8GtgNuBJ44a53XA39XXn45cEFNr+cgWZcCTwY+BRxb87//IHmXAzuVl09q+Gv7qJ7LRwOXNfm1LdfbFbgauBaYbmpW4ETgb+t6PReZ9UDgBmCP8vpjmpx31vpvBD7R73knYgYFvBg4v7x8PvCSuVbKzKuAn48r1CxPA76ZmXdm5gPA5yly9+r9Pi4EDouIGGPGGX2zZuZdmbkB+E0N+WYbJO/qzPxlefVaYL8xZ5wxSNZ/7rm6M1Dnnk6D/NwCvAd4P/Av4ww3y6BZm2CQrK8Fzs7MnwBk5g/GnLHXYl/bVwCf6/ekk1JQe2fmd8vL3wP2rjPMPPYF7u65vrG8bc51MnMT8DNgz7GkmydHaa6sTbLYvK8BvlhpovkNlDUi3hAR36LYOvCmMWWbS9+8EfFUYP/M/IdxBpvDoD8Hx5Sbei+MiP3HE+0RBsn6eODxEfHViLg2Io4YW7pHGvh3rNx8fgDw5X5P2plTvkfElcBj57jrHb1XMjMjwn3rNaeIOB6YBp5dd5aFZObZwNkR8UrgdOBVNUeaU0RsBfwNxaazNvhfwOcy81cR8TqKLRbPqTnTfLah2Mx3KMWM/+qI+DeZ2f+9nXq9HLgwMx/st2JnCiozD5/vvoj4fkTsk5nfjYh9gDqnwvO5B+j9a22/8ra51tkYEdsAuwE/Gk+8OXPMmCtrkwyUNyIOp/iD5tmZ+asxZZttsa/t54GPVJpoYf3y7go8CVhTbo1+LHBxRBydmWvHlrLQ97XNzN7fp3MpZqh1GOTnYCPw9cz8NfDtiLiDorCuG0/Eh1nMz+3LgTcM8qSTsonvYjb/hfkq4H/WmGU+1wEHRsQBEbEdxT/ixbPW6f0+jgW+nOU7jmM2SNYm6Zs3Ig4GPgocXfO2/EGyHthz9YXAN8aYb7YF82bmzzJzSWYuzcylFO/v1VFOfbMClH/AzjgauG2M+XoN8jv2PyhmT0TEEopNfneOM2SPgf5PiIgnAHsA1wz0rHXuqTLGPUz2BK6i+EW+Enh0efs0cG7Pev8I/BC4n+Kvk+ePOeeRwB0Ue8O8o7zt3RS/0AA7AP8N+CbwT8Bv1/ia9sv6b8vX8D6KWd4tNf8M9Mt7JfB9YH35dXGDs34YuKXMuRr4101+bWetu4aa9uIb8LV9b/na3li+tk9ocNag2Hx6K3AT8PKm/xwA/wl436DP6aGOJEmNNCmb+CRJLWNBSZIayYKSJDWSBSVJaiQLSpLUSBaUWiUi3lEeuXlDeVTkp5e3nztzVO+IuCsilkTE0oi4ueI8S8ujOcxcXxYRR1Y55gJZ9oriKPc3RMSzIuKlEXFbRKyOiOmI+C99Hn9pROw+5Ngvqfuo6uqezhxJQt0XEc8AXgQ8NYtD0SyhOHIymVnXaRGWAq8EZk51sIzi83WX1pDlMOCmmdciIi4DXpuZXynvX/DDsZm5JcX6EuASis/kSCPhDEptsg9wb5aHIcrMezPz/wFExJp5zjO0dUR8rJx1XR4RO5brLysPsLkhIr4Q5TnCep+nnIXdVV7eOiI+EBHXlY95Xfn87wOeVc7m3k7xwcTjyuvHRcTOEfGJiPincmYz5xGeI+LtUZzj6caIeF+fjI+L4txl6yLiHyPiCRGxjOKwPC8ux34X8Ezg42XuQ6M8z1lE7BIRnyzH2xARx5S331WWPhFxfJl5fUR8NCK2Lm//RUT8ZZnz2ojYOyIOoTjqwgfK9R835L+v9HB1fvLYL78W8wXsQnH0hDuAcyiOmTdz3xrKIxQAdwFLKGY3m4Bl5e2rgOPLyxtmHk9RKmfN8TxLgLvKyyuA08vL21PMRg6gONTMJT05TqTnfELAmT1j7l5m33nW9/UC4GtsPh/Vo/tkvAo4sLz8dIpDXs01du/38lBOitNenNWz3h6zXrffpTho6rbl7ecA/768nMBR5eW/6nlNzqPm83751b0vN/GpNTLzFxHxe8CzKE4weEFEnJKZ5y3wsG9n5vry8jpgaUTsBuyemf+7vP18ikNILeR5wJNj89mBd6M4MOcDAzzu6Ih4W3l9B2CKhx/j7XDgk1mejyozfzxfxojYBTikvDzz+O37ZJjtcIpjpVGO95NZ9x8G/B5wXTnGjmw+wPIDFJvyoHg9n7vIsaWBWVBqlSwO0b+G4ujYN1EcPPe8BR7Se1TyByn+s13IJjZv+t6h5/YA3piZX+pdOSIO7fN8ARyTmbf3WW9QWwE/zcxlI3q+uQRwfmaeOsd9v87MmeOjPYj/h6hCvgel1oiIg2YdyXsZ8J3FPk9m/gz4SUQ8q7zpBGBmpnIXxewBiiPGz/gScFJEbFtmeXxE7ExxBuZde9abff1LwBujnIpEcdT02a4AXh0RO5XrPHq+jFmcTffbEfHSct2IiKcs6gUoxnvodAcz7231uAo4NiIeM5MnipPMLWT29y1tMQtKbbILcH5E3BoRG4AnUhwdeRivonhTfwNF0b27vP2vKYroBor3Y2acS7GH2vVR7Lr+UYrZwwbgwXKngf9AcQTsJ87sJEFxqvNtgQ0RcUt5/WEy8zKKUxOsjYj1wMzmwPky/jHwmoi4keLI24s9bfkZwB4RcXP5HMtn5bmV4iSIl5djX0Gxg8pCPg/8RbkjiDtJaCQ8mrkkqZGcQUmSGsmCkiQ1kgUlSWokC0qS1EgWlCSpkSwoSVIjWVCSpEb6/77sS4ynV1rGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_silhouettes(df_N, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6131870124853009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "print(completeness_score(df_classes.iloc[:, 1],clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6115021163370862\n"
     ]
    }
   ],
   "source": [
    "print(homogeneity_score(df_classes.iloc[:, 1],clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) Perform PCA on the normalized image data matrix. You may use the linear algebra package in Numpy or the Decomposition module in scikit-learn (the latter is much more efficient). Analyze the principal components to determine the number, r, of PCs needed to capture at least 95% of variance in the data. Provide a plot of PC variances. Then use these r components as features to transform the data into a reduced dimension space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08  0.    0.   -0.   -0.   -0.    0.   -0.    0.    0.    0.    0.01  0.01 -0.01  0.    0.\n",
      "   0.01 -0.01  0.  ]\n",
      " [ 0.    0.06  0.    0.    0.    0.   -0.    0.   -0.   -0.03 -0.03 -0.03 -0.03  0.02 -0.02  0.02\n",
      "  -0.03  0.    0.04]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.  ]\n",
      " [-0.    0.    0.    0.02 -0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.    0.   -0.    0.\n",
      "  -0.   -0.    0.  ]\n",
      " [-0.    0.    0.   -0.    0.01  0.    0.    0.    0.   -0.   -0.   -0.   -0.   -0.    0.   -0.\n",
      "  -0.    0.   -0.  ]\n",
      " [-0.    0.    0.   -0.    0.    0.01  0.    0.    0.   -0.   -0.    0.   -0.   -0.    0.   -0.\n",
      "   0.   -0.   -0.  ]\n",
      " [ 0.   -0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.    0.   -0.    0.    0.\n",
      "   0.    0.   -0.  ]\n",
      " [-0.    0.    0.   -0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.   -0.    0.   -0.\n",
      "   0.   -0.   -0.  ]\n",
      " [ 0.   -0.    0.   -0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   -0.    0.   -0.\n",
      "   0.   -0.   -0.  ]\n",
      " [ 0.   -0.03  0.   -0.   -0.   -0.   -0.    0.    0.    0.07  0.07  0.08  0.07 -0.04  0.04 -0.03\n",
      "   0.08 -0.04 -0.02]\n",
      " [ 0.   -0.03  0.   -0.   -0.   -0.   -0.    0.    0.    0.07  0.07  0.07  0.06 -0.04  0.04 -0.03\n",
      "   0.07 -0.04 -0.02]\n",
      " [ 0.01 -0.03  0.   -0.   -0.    0.    0.    0.    0.    0.08  0.07  0.08  0.07 -0.05  0.05 -0.03\n",
      "   0.08 -0.04 -0.03]\n",
      " [ 0.01 -0.03  0.   -0.   -0.   -0.    0.    0.    0.    0.07  0.06  0.07  0.06 -0.04  0.04 -0.02\n",
      "   0.07 -0.04 -0.02]\n",
      " [-0.01  0.02  0.    0.   -0.   -0.   -0.   -0.   -0.   -0.04 -0.04 -0.05 -0.04  0.04 -0.03  0.02\n",
      "  -0.05  0.02  0.01]\n",
      " [ 0.   -0.02  0.   -0.    0.    0.    0.    0.    0.    0.04  0.04  0.05  0.04 -0.03  0.04 -0.03\n",
      "   0.05 -0.02 -0.03]\n",
      " [ 0.    0.02  0.    0.   -0.   -0.    0.   -0.   -0.   -0.03 -0.03 -0.03 -0.02  0.02 -0.03  0.04\n",
      "  -0.03  0.01  0.04]\n",
      " [ 0.01 -0.03  0.   -0.   -0.    0.    0.    0.    0.    0.08  0.07  0.08  0.07 -0.05  0.05 -0.03\n",
      "   0.08 -0.04 -0.03]\n",
      " [-0.01  0.    0.   -0.    0.   -0.    0.   -0.   -0.   -0.04 -0.04 -0.04 -0.04  0.02 -0.02  0.01\n",
      "  -0.04  0.05 -0.  ]\n",
      " [ 0.    0.04  0.    0.   -0.   -0.   -0.   -0.   -0.   -0.02 -0.02 -0.03 -0.02  0.01 -0.03  0.04\n",
      "  -0.03 -0.    0.07]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "meanVals = np.mean(df_N, axis=0)\n",
    "meanRemoved = df_N - meanVals #remove mean\n",
    "covMat = np.cov(meanRemoved, rowvar=0)\n",
    "\n",
    "np.set_printoptions(precision=2,suppress=True,linewidth=100)\n",
    "print(covMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la\n",
    "eigVals,eigVects = la.eig(np.mat(covMat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48 0.1  0.08 0.04 0.03 0.02 0.01 0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(eigVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03 -0.35  0.93  0.04 -0.01  0.03  0.01  0.03 -0.   -0.02 -0.01  0.01 -0.    0.    0.   -0.\n",
      "  -0.   -0.    0.  ]\n",
      " [-0.19 -0.38 -0.12 -0.66 -0.47  0.14 -0.24  0.22  0.06  0.07  0.06 -0.04 -0.01 -0.01  0.    0.\n",
      "  -0.   -0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    1.  ]\n",
      " [-0.01 -0.03 -0.04 -0.03  0.06  0.6   0.72  0.32  0.06  0.08  0.01 -0.01 -0.   -0.    0.    0.\n",
      "   0.    0.    0.  ]\n",
      " [-0.    0.02  0.01 -0.1  -0.09 -0.44  0.28  0.32 -0.77  0.05  0.07 -0.    0.01  0.   -0.   -0.\n",
      "  -0.    0.    0.  ]\n",
      " [ 0.    0.02  0.01 -0.12 -0.03 -0.42  0.3   0.11  0.44 -0.34  0.55  0.28 -0.11 -0.01 -0.   -0.\n",
      "  -0.   -0.    0.  ]\n",
      " [ 0.    0.01  0.01 -0.01 -0.01 -0.18  0.12  0.07  0.18 -0.14  0.05 -0.77  0.55  0.01 -0.    0.\n",
      "  -0.    0.    0.  ]\n",
      " [ 0.01  0.   -0.   -0.14 -0.03 -0.33  0.19  0.16  0.29 -0.02 -0.77  0.29  0.21 -0.01 -0.   -0.\n",
      "   0.   -0.    0.  ]\n",
      " [ 0.    0.    0.   -0.02 -0.01 -0.16  0.09  0.07  0.13 -0.05 -0.24 -0.49 -0.8   0.01  0.   -0.\n",
      "  -0.    0.    0.  ]\n",
      " [ 0.38 -0.11 -0.06  0.09 -0.05  0.02 -0.08  0.16  0.01 -0.05  0.    0.   -0.   -0.21  0.74 -0.02\n",
      "   0.11  0.36  0.  ]\n",
      " [ 0.36 -0.11 -0.07  0.09  0.01  0.04 -0.13  0.24  0.   -0.11 -0.    0.01 -0.   -0.21 -0.06  0.5\n",
      "  -0.62  0.1   0.  ]\n",
      " [ 0.41 -0.07 -0.04  0.03 -0.08  0.03 -0.04  0.07 -0.01 -0.06 -0.01 -0.   -0.   -0.22 -0.16 -0.81\n",
      "  -0.16 -0.08  0.  ]\n",
      " [ 0.36 -0.16 -0.08  0.14 -0.06 -0.01 -0.08  0.18  0.04  0.02  0.02  0.   -0.   -0.21 -0.53  0.27\n",
      "   0.69 -0.39  0.  ]\n",
      " [-0.24  0.06  0.    0.01  0.39  0.13 -0.31  0.5  -0.05 -0.42 -0.04  0.03 -0.    0.08 -0.2  -0.11\n",
      "  -0.06  0.37  0.  ]\n",
      " [ 0.26  0.18  0.08 -0.27 -0.18  0.04  0.18 -0.4  -0.08 -0.08 -0.04 -0.02  0.   -0.08 -0.3   0.1\n",
      "  -0.2   0.62  0.  ]\n",
      " [-0.18 -0.34 -0.13  0.43 -0.11 -0.2   0.03  0.14  0.17  0.55  0.1   0.    0.    0.05 -0.13 -0.08\n",
      "  -0.24  0.43  0.  ]\n",
      " [ 0.41 -0.1  -0.06  0.04 -0.1   0.02 -0.03  0.06 -0.   -0.07 -0.01  0.02  0.    0.89 -0.   -0.\n",
      "  -0.   -0.    0.  ]\n",
      " [-0.2   0.31  0.08  0.42 -0.74  0.12 -0.03  0.13 -0.01 -0.3  -0.06  0.03  0.   -0.01 -0.    0.\n",
      "  -0.    0.    0.  ]\n",
      " [-0.17 -0.64 -0.24  0.2   0.01  0.02  0.18 -0.35 -0.18 -0.5  -0.12  0.02  0.   -0.04 -0.    0.\n",
      "  -0.   -0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(eigVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48 0.1  0.08 0.04 0.03 0.02 0.01 0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "[60.71 13.2  10.12  4.54  3.55  1.99  1.89  1.62  1.07  0.71  0.39  0.16  0.05  0.    0.    0.\n",
      "  0.    0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "eigValInd = np.argsort(eigVals)  #sort, sort goes smallest to largest\n",
    "eigValInd = eigValInd[::-1]   #reverse\n",
    "sortedEigVals = eigVals[eigValInd]\n",
    "print(sortedEigVals)\n",
    "total = sum(sortedEigVals)\n",
    "varPercentage = sortedEigVals/total*100\n",
    "print(varPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 60.714233968533236\n",
      "2 73.91121320168925\n",
      "3 84.03498614256189\n",
      "4 88.57852534332582\n",
      "5 92.12588648109565\n",
      "6 94.1139219796062\n",
      "7 96.00589227704954\n",
      "8 97.62130108194513\n",
      "9 98.68690193362029\n",
      "10 99.3982394514953\n",
      "11 99.79044297671408\n",
      "12 99.94794328616065\n",
      "13 99.99685835864892\n",
      "14 99.9999999999999\n",
      "15 99.99999999999993\n",
      "16 99.99999999999994\n",
      "17 99.99999999999996\n",
      "18 99.99999999999997\n",
      "19 99.99999999999997\n"
     ]
    }
   ],
   "source": [
    "pca = 0\n",
    "for i in range(0,len(varPercentage)):\n",
    "    pca = pca + varPercentage[i]\n",
    "    print(i+1,pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, On 7th PC the variance is greater then 95%. So, 7 Principle components are needed to capture at least 95% of variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(1, 11), varPercentage[:10], marker='^')\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69 -0.53 -0.25 ... -0.08 -0.05 -0.05]\n",
      " [-0.67 -0.51 -0.34 ... -0.04 -0.06 -0.04]\n",
      " [-0.71 -0.77  0.16 ... -0.17 -0.04 -0.06]\n",
      " ...\n",
      " [-0.51  0.13  0.08 ... -0.03  0.03 -0.11]\n",
      " [-0.48  0.09  0.16 ...  0.    0.   -0.09]\n",
      " [-0.44  0.11  0.05 ...  0.02  0.21  0.15]]\n"
     ]
    }
   ],
   "source": [
    "topNfeat = 7\n",
    "topEigValInd = eigValInd[:topNfeat]  #cut off unwanted dimensions\n",
    "reducedEigVects = eigVects[:,topEigValInd]   #reorganize eig vects largest to smallest\n",
    "reducedDT = np.dot(meanRemoved, reducedEigVects)    #transform data into new dimensions\n",
    "print(reducedDT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Perform Kmeans again, but this time on the lower dimensional transformed data. Then compare Silhouette values as well as completeness and Homogeneity values of the new clusters. Compare these results with those obtained on the full data in part b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=7,max_iter=1000,verbose=0)\n",
    "kmeans.fit(reducedDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 6, 6, 2], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = kmeans.predict(reducedDT)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster\n",
       "0           5\n",
       "1           5\n",
       "2           5\n",
       "3           5\n",
       "4           5\n",
       "...       ...\n",
       "2095        0\n",
       "2096        2\n",
       "2097        6\n",
       "2098        6\n",
       "2099        2\n",
       "\n",
       "[2100 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clusters, columns=[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.41</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6\n",
       "0 -0.60  0.36 -0.11  0.13 -0.13  0.02 -0.04\n",
       "1  1.41 -0.09 -0.04  0.17 -0.03  0.01 -0.02\n",
       "2 -0.21  0.25 -0.15 -0.06  0.13  0.01  0.03\n",
       "3  0.44  0.10 -0.16 -0.23 -0.05  0.01  0.02\n",
       "4  0.18 -0.04  0.27 -0.18  0.03 -0.02  0.00\n",
       "5 -0.62 -0.64 -0.20  0.09 -0.07 -0.01  0.04\n",
       "6 -0.51  0.06  0.34  0.07  0.08 -0.01 -0.03"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.332066475286672\n"
     ]
    }
   ],
   "source": [
    "print(silhouettes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouettes(data, clusters, metric='euclidean'):\n",
    "    \n",
    "    from matplotlib import cm\n",
    "    from sklearn.metrics import silhouette_samples\n",
    "\n",
    "    cluster_labels = np.unique(clusters)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = metrics.silhouette_samples(data, clusters, metric='euclidean')\n",
    "    c_ax_lower, c_ax_upper = 0, 0\n",
    "    cticks = []\n",
    "    for i, k in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[clusters == k]\n",
    "        c_silhouette_vals.sort()\n",
    "        c_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(float(i) / n_clusters)\n",
    "        pl.barh(range(c_ax_lower, c_ax_upper), c_silhouette_vals, height=1.0, \n",
    "                      edgecolor='none', color=color)\n",
    "\n",
    "        cticks.append((c_ax_lower + c_ax_upper) / 2)\n",
    "        c_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    pl.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "    pl.yticks(cticks, cluster_labels)\n",
    "    pl.ylabel('Cluster')\n",
    "    pl.xlabel('Silhouette coefficient')\n",
    "\n",
    "    pl.tight_layout()\n",
    "    #pl.savefig('images/11_04.png', dpi=300)\n",
    "    pl.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAabUlEQVR4nO3df5wkdX3n8dcHBPkhArILesBmiQEM8XQhcxpQ4yKISABNQFEDJ8a4Hhc1RPEExId3ioBJjHAXNKyoYIwKB+cdQUQQ2RAVkAWW5YcHKuKxxB/gr1Mx6uInf1QN27Yz0z0zXV1VXa/n4zGP6q6p7u97e2f2vVVd/a3ITCRJaprN6g4gSdJMLChJUiNZUJKkRrKgJEmNZEFJkhrpMXUH6LVkyZJcvnx53TGkyXf33cVy773rzSEBN99880OZubR/faMKavny5axdu7buGNLkW7myWK5ZU2cKCYCI+MZM6z3EJ0lqJAtKktRIFpQkqZEa9R6UpDE54YS6E0gDWVBSFx1zTN0JpIE8xCd10f33F19Sg7kHJXXRcccVS08zV4O5ByVJaiQLSpLUSB7ia6p9o+4EmmRfKZdN/zm71QuqdpkFJWl8LBzNgwUlddHOYxzLUtICWVBSF21f0fNaRhohC0rqon8tl1sNub3FoxpYUFIXTX9Gd89yaQGpgSwoqQv6C8jrQakFLChpErlHpAlgQUltYemoYywoqaksJHWcBSXVqa4SOu20esaV5sGCksalSXtEBx9cdwJpIAtKqkqTCqnfunXFcsWKenNIc6i0oCJiB+B84KlAAn+SmddXOaY0dk0uotmceGKx9DRzNVjVe1DnAFdm5tERsSWwTcXjSePTxmKSWqSygoqI7YHfB44HyMyfAz+vajxppCwfqXZVXrBwD+BB4MMRcWtEnB8R2/ZvFBGrImJtRKx98MEHK4wjDclykhqhyoJ6DLAf8P7M3Bf4CXBy/0aZuTozpzJzaunSpRXGkfrcmjN/SWqEKt+D2gBsyMwby/uXMENBSWNj+Wxyxhl1J5AGqqygMvNbEXF/ROydmXcDBwF3VTWeNCuL6dcdcEDdCaSBqj6L7/XAP5Rn8N0LvKri8aSCpTS3L36xWFpUarBKCyoz1wFTVY4hPcpSGt6ppxZLPwelBnMmCbWXhSRNNAtK7WIpSZ1hQal5LCFJWFCqm2UkaRYWlEbP0mm+s8+uO4E0kAWlwSycyeNlNtQCFlQXWDDq99nPFksvXKgGs6CaZN/YdNtSUZVOP71YWlBqMAtqlHoLRpK0KN0oKItDklqnGwXVxsNlV1mqqtD3y6U/Z5PtkBb+29ejGwUlSV3S8mKaZkFJXfSGugNo5CaklHpZUFIX7V53AC3KBJbRTCwoqYtuKJe/V2sKDasjhdTPgpK66NJyaUE1R0dLaC4WlCTVyWKalQUlSeNmKQ3FgpKkcbCU5s2CkqQqWEiLZkFJXfTmugNMKEtppCwoqYt2rjvAhLGYKlFpQUXEfcCPgEeAjZk5VeV4koa0plyurDFD21lKlRvHHtSBmfnQGMaRNKxPlcuVdYZoIUtprDzEJ0mDWEy1qLqgErgqIhI4LzNX928QEauAVQDLli2rOI4kzcEiapSqC+rZmflAROwMXB0R/zczr+vdoCyt1QBTU1P+dEgaP4upkSotqMx8oFx+JyI+CTwDuG7uR0nSmFhMjVZZQUXEtsBmmfmj8vYhwDuqGk/SPJxWd4Axs4haqco9qF2AT0bE9Dgfy8wrKxxP0rC2rzvAGFhKrVdZQWXmvcDTq3p+SYtwVbk8pNYU1bGcJoKnmUtddHW5nKSCspQmjgUlqb0spYlmQUlqDwupUywoSc1hAamHBSWpHpaRBrCgpC5655jHs4y0ABaU1EVbVfjclpFGxIKSuugfy+URi3gOi0gVs6CkLpqeEbO/oCwdNYgF1VBXHrKy7giaYM8441YAvnTIvn3fOXD8YSpwKNfWHUEjYEFJagVLp3ssKEmNZCHJgpJUK4tIs7GgpA760pr+957Gy1LSMCwoSWNjMWk+LCipg5b/9f8D4L6Tlo1lPItJC2FBSR208+XfBaorKAtJo2BBSVo0C0lVsKAkDcUS0rhZUJJmZCGpbhaU1EGPbL05YAmp2SwoacIMVTqfrj6HtFgWlNRy7gVpUlVeUBGxObAWeCAzD696PGkSjbyE3lleUvdtbxvt80ojNI49qD8Hvgw8fgxjSa02tr2ha64plhaUGqzSgoqI3YA/AN4FvLHKsaQ28vCcNLuq96DOBv4LsN1sG0TEKmAVwLJl45l2RRoXC0hauMoKKiIOB76TmTdHxMrZtsvM1cBqgKmpKa83rdayjKTRqnIP6lnAkRFxGLAV8PiI+GhmHlvhmNJYtL6Mdtqp7gTSQJUVVGaeApwCUO5BnWQ5qU1aX0JzufTSuhNIA/k5KIkJLyOppcZSUJm5BlgzjrGkuVhEpVNOKZZnnllvDmkO7kFp4llKM7j++roTSANZUGo1y0eaXBaUGsnikWRBqTaWkKS5WFCqjAXUYLvtVncCaSALSkOxbCbMRz9adwJpIAtKgAUkqXkGFlR5Pac3ZOZ7x5BHY2IhddyJJxbLs8+uN4c0h4EFlZmPRMTLAQuq5SwlPWrduroTSAMNe4jvCxHxt8BFwE+mV2bmLZWk0tAsHUmTatiCWlEu39GzLoHnjTaOpr2btwy53ZUVJ9Ekei/fA+AvJuTn51oOrTuCKjBUQWXmgVUHkSSwbLTJUAUVEbsAZwD/LjNfGBH7APtn5gcrTSepEhv22rXuCBaRBhr2EN8FwIeBt5b376F4P8qCklroPav/vJZxLSXNx7AFtSQzL46IUwAyc2NEPFJhLkktYvGoCsMW1E8iYieKEyOIiN8DflhZKkmVetOqc4D570lZRBqnYQvqjcBlwJMj4gvAUuAllaWSVKnd7nlgqO0sJNVp2IK6E3gusDcQwN3AZlWFklQfS0lNMWxBXZ+Z+1EUFQARcQuwXyWpJI2FZaQmm7OgIuKJwK7A1hGxL8XeE8DjgW0GPHYr4DrgseU4l2Tm2xedWNKiFKV0Vt0xpIEG7UG9ADge2A14D5sK6kfAqQMe+zPgeZn544jYAvh8RHw6M29YRF5JQ5pz72jFitm/JzXEnAWVmRcCF0bEUZl56XyeODMT+HF5d4vyKxeUUtJA8zpc5yzmaoFh34PaLSIeT7Hn9AGK955Ozsyr5npQeamOm4HfAs7NzBsXE1ZSwfeO1AXDFtSfZOY5EfECYCfgOODvgTkLKjMfAVZExA7AJyPiqZl5R+82EbEKWAWwbNmy+eaXJlLlBXTsscXSK+uqwYYtqOn3ng4DPpKZd0ZEzPWAXpn5g4i4FjgUuKPve6uB1QBTU1MeAlRn1LoXtGFDfWNLQxq2oG6OiKuAPYBTImI74JdzPSAilgK/KMtpa+D5wLsXlVZqKQ/JSfM3bEG9muKaUPdm5sPltEevGvCYJ1GcYLE5xYd6L87MyxceVWoHy0gajWEL6tnl8mnDHtnLzPXAvgsJJTWZBSSNx7AF9eae21sBz6A4O88r6qqVOl8y++9fdwJpoGGvqHtE7/2I2B3wgxRqlM6XznyceWbdCaSBht2D6rcB+O1RBpGGYQlJ3THsJd//B5tmgdiM4oSJW6oKpcllwTTEUUcVy0vnNUGMNFbD7kGt7bm9Efh4Zn6hgjxqKItlwnz3u3UnkAYa9j2oC6sOovpYPpKaaNDlNm5njgleM/NpI0/UcQdyJWBpSNKgPag/AnYB7u9bvzvwrUoSddx0McXHag6iiXbtd4rlgTX/nOUr6h1fzTaooN4LnJKZ3+hdWc5s/l7giBkfJanRrvmdg2ob21LSsAYV1C6ZeXv/ysy8PSKWV5JIUuVO/8O3VT6GRaTFGlRQO8zxva1HGURS+1lKGqVBBbU2Il6TmR/oXRkRf0ox1ZGkFrri3S8E4LC3fHpBj7eINA6DCupEigsN/jGbCmkK2BL4wyqDSarO1r/46by2t5BUhzkLKjO/DRwQEQcCTy1XfyozP1d5Mkm1sZDUBMN+UPda4NqKs0iqmcWkJlnoZLGSWm7lzhaSms2Ckjrk0UL6l8NrzSENw4KSJtyMe0knnTT2HNJ8WVDShPGwnSaFBSVNiHkV08qVxXLNmgqSSKNhQUkt5Z6SJl1lBRURuwMfoZgNPYHVmXlOVeNJk85CUtdUuQe1EXhTZt4SEdsBN0fE1Zl5V4VjSq1nEUmFygoqM78JfLO8/aOI+DKwK2BBSSXLSJrdWN6DKi/NsS9w4wzfWwWsAli2bNk44ki1aFQZvfSldSeQBorMWa/oPpoBIh4H/BPwrsz8X3NtOzU1lWvXrq00T1t4Rd32alQRSS0QETdn5lT/+kr3oCJiC+BS4B8GlZPUVq0spIcfLpbbbFNvDmkOVZ7FF8AHgS9n5t9UNY40Lq0sotkcdlix9HNQarAq96CeBRwH3B4R68p1p2bmFRWOKY3ERJWR1FJVnsX3eSCqen5psSwhqdmcSUITywKS2s2C0kSwjKTJY0GpdSyjETj++LoTSANZUGocC2gMLCi1gAWlWlhCNXvooWK5ZEm9OaQ5WFCqnGXUQEcfXSz9HJQazILSglg6kqpmQWlOFpGkulhQHWb5SGoyC2rCWDqSJoUF1WCWjSpzwgl1J5AGsqCa6nUQr6s7hCbXMcXCntIC5feqH8OCkjpot1/eD8CGzXavOYnaZhzFNM2Ckjro739yHAAHbrem3iBqvHEWUj8LSpL0qDoLqZ8FJUkd1qRC6mdBSVIHNbmYpllQktQhbSimaRaU1EHveeyb6o6gMWtTMU2zoKQOunzLI+qOoDFoYyn1sqCkDtrrkbsBuGfzvWtOolFpexnNxIKSOui8h18L+DmotpvEUuplQUlSy0x6MU3brKonjogPRcR3IuKOqsaQpC7J73WnnKDCggIuAA6t8PklqRO6VkzTKjvEl5nXRcTyqp5fkiZdF0upl+9BSR10+lan1R1Bs+h6KfWqvaAiYhWwCmDZsmU1p5G64ZotDq47gvpYTL+uyveghpKZqzNzKjOnli5dWnccqROevnEdT9+4ru4YnTf93pLlNLPa96Akjd/ZPz0R8HNQdbCMhlflaeYfB64H9o6IDRHx6qrGkqSmc09p/qo8i+/lVT23JDWdZbR4HuKTpBGwkEbPgpKkebKMxsOCkjro1K3PqDtCq1hI9bCgpA66/jEH1B2hESyeZrOgpA7af+MXgckuKsun/SwoqYPO+OmpQLs/B2UBTT4LStLYWS4ahgUlaV4sF42LBSV11MpnQa6pO4U0OwtKarkF7dGsHHUKafQsqKb6/n+rO4FaImL+j3k6TwHgtvDnrC6Zb687QuNZUFIH3caT6o7QKZbRwlhQUgcdxNcAuIYn15xkcllKi2dBSR10GtcBFtSoWUqjZUFJ0jxZRONhQUnSABZSPSwoSephGTWHBSWp8yylZrKgpA56LUfUHaExLKfmsqCkDrqHJXVHqIVl1C4WlNRBh3M3AJezd81JqmMZtV+lBRURhwLnAJsD52fmWVWOJ2k4b6K4YGHbCsrS6ZbKCioiNgfOBZ4PbABuiojLMvOuqsaU1C4WjuZS5R7UM4CvZua9ABHxCeBFgAUldYglpIWqsqB2Be7vub8BeGb/RhGxClgFsGzZsgrjSKqaZaRRqv0kicxcDawGmJqayprjSJqF5aNxq7KgHgB277m/W7lOUs2O448evW3xqKmqLKibgD0jYg+KYnoZ8IoKx5M0AwtIbVVZQWXmxoh4HfAZitPMP5SZd1Y1ntQliy6diy4qlsccs/gwUkUqfQ8qM68ArqhyDGkSVb7X8/73F0sLSg1W+0kSUld56E2amwUlVcwikhbGgpIWyQKSqmFBSUOyiKTxsqCkHp0poUsuqTuBNJAFpU7qTBHNZkk3rweldrGgNJE6X0CDXHBBsTz++DpTSHOyoNRIFkzFLCi1gAWlylk2khbCgmoo/1GX1HWb1R1AkqSZWFCSpEbyEJ/URVc4h7Oaz4KSumibbepOIA3kIT6pi973vuJLajALSuqiiy8uvqQGs6AkSY1kQUmSGsmCkiQ1kgUlSWqkyMy6MzwqIh4EvlHhEEuAhyp8/lFqS9a25ASzVqEtOcGsVRhVzt/IzKX9KxtVUFWLiLWZOVV3jmG0JWtbcoJZq9CWnGDWKlSd00N8kqRGsqAkSY3UtYJaXXeAeWhL1rbkBLNWoS05waxVqDRnp96DkiS1R9f2oCRJLWFBSZIaaaILKiKeEBFXR8RXyuWOs2x3ZUT8ICIuH3O+QyPi7oj4akScPMP3HxsRF5XfvzEilo8zX1+WQVl/PyJuiYiNEXF0HRl7sgzK+saIuCsi1kfENRHxGw3N+Z8i4vaIWBcRn4+IferIWWaZM2vPdkdFREZEbadID/G6Hh8RD5av67qI+NMm5iy3eWn5s3pnRHxs3Bl7cgx6Td/b83reExE/GMnAmTmxX8BfAieXt08G3j3LdgcBRwCXjzHb5sDXgN8EtgRuA/bp2+Y/A39X3n4ZcFFNr+MwWZcDTwM+Ahxd49/5MFkPBLYpb59Qx+s6ZM7H99w+Eriyqa9pud12wHXADcBUU7MCxwN/W0e+eebcE7gV2LG8v3NTs/Zt/3rgQ6MYe6L3oIAXAReWty8EXjzTRpl5DfCjcYUqPQP4ambem5k/Bz5BkbdXb/5LgIMiIsaYcdrArJl5X2auB35ZQ75ew2S9NjMfLu/eAOw25owwXM7/33N3W6CuM5qG+VkFeCfwbuBfxxmuz7BZ6zZMztcA52bm9wEy8ztjzjhtvq/py4GPj2LgSS+oXTLzm+XtbwG71Bmmz67A/T33N5TrZtwmMzcCPwR2Gku6WXKUZsraFPPN+mrg05UmmtlQOSPizyLiaxRHA94wpmz9BmaNiP2A3TPzU+MMNoNh//6PKg/xXhIRu48n2q8YJudewF4R8YWIuCEiDh1bul819O9Uebh8D+Bzoxi49Zd8j4jPAk+c4Vtv7b2TmRkRnlOvR0XEscAU8Ny6s8wmM88Fzo2IVwCnAa+sOdKviYjNgL+hOHTWBv8IfDwzfxYRr6U4SvG8mjPN5DEUh/lWUuzlXxcR/z4zR/P+TjVeBlySmY+M4slaX1CZefBs34uIb0fEkzLzmxHxJKCuXeSZPAD0/s9tt3LdTNtsiIjHANsD3x1PvBlzTJspa1MMlTUiDqb4T8xzM/NnY8rWa76v6SeA91eaaHaDsm4HPBVYUx6BfiJwWUQcmZlrx5ayMPB1zcze36HzKfZOx22Yv/8NwI2Z+Qvg6xFxD0Vh3TSeiI+az8/qy4A/G9XAk36I7zI2/Y/zlcD/qTFLv5uAPSNij4jYkuIv9rK+bXrzHw18Lst3IcdsmKxNMTBrROwLnAccWeNx/WFy7tlz9w+Ar4wxX685s2bmDzNzSWYuz8zlFO/r1VFOA7MClP9ZnXYk8OUx5ps2zO/U/6bYeyIillAc8rt3nCFLQ/3+R8RTgB2B60c2ch1nhYzx7JOdgGsofrE/CzyhXD8FnN+z3T8DDwI/pfhfywvGlO8w4B6KM2TeWq57B8UvN8BWwP8Evgp8CfjNGl/LQVn/Q/na/YRiL+/OBmf9LPBtYF35dVlDc54D3FlmvBb4naa+pn3brqGms/iGfF3PLF/X28rX9SkNzRkUh07vAm4HXtbU17S8/1+Bs0Y5rlMdSZIaadIP8UmSWsqCkiQ1kgUlSWokC0qS1EgWlCSpkSwotVJEvLWc4Xl9OYPyM8v150/P+h0R90XEkohYHhF3VJxneTnbw/T9FRFxWJVjzpFlaRSz398aEc+JiJdExJcj4tqImIqI/z7g8VdExA4LHPvFdc66rsnS+pkk1D0RsT9wOLBfFtPVLKGYZZnMrOXSCRSzub8CmL4kwgqKz9tdUUOWg4Dbp1+LiLgSeE1mfr78/pwfoM3MxRTri4HLKT67Iy2Ke1BqoycBD2U5RVFmPpSZ/wIQEWtmuRbR5hHxgXKv66qI2LrcfkU5Eef6iPhklNcM632eci/svvL25hHxVxFxU/mY15bPfxbwnHJv7i0UH2I8prx/TERsGxEfiogvlXs2M84GHRFvieIaULdFxFkDMj45imuZ3RwR/xwRT4mIFRRT97yoHPvtwLOBD5a5V0Z53bOIeFxEfLgcb31EHFWuv68sfSLi2DLzuog4LyI2L9f/OCLeVea8ISJ2iYgDKGZm+Kty+ycv8O9XKtT1yWS//FroF/A4itkV7gHeRzGf3vT31lDOYgDcByyh2LvZCKwo118MHFveXj/9eIpSOXuG51kC3FfeXgWcVt5+LMXeyB4UU9Jc3pPjeHquOQSc0TPmDmX2bfv+XC8Evsima1U9YUDGa4A9y9vPpJgKa6axe/8sj+akuDTG2T3b7dj3uv02xcSqW5Tr3wf8x/J2AkeUt/+y5zW5gBqvB+bXZH15iE+tk5k/jojfBZ5DcfHBiyLi5My8YI6HfT0z15W3bwaWR8T2wA6Z+U/l+gspppaayyHA02LTVYO3p5jA8+dDPO7IiDipvL8VsIxfnQfuYODDWV6rKjO/N1vGiHgccEB5e/rxjx2Qod/BFPOqUY73/b7vHwT8LnBTOcbWbJpw+ecUh/KgeD2fP8+xpYEsKLVSFtP5r6GYQft2ikl1L5jjIb0zlj9C8Y/tXDay6RD4Vj3rA3h9Zn6md+OIWDng+QI4KjPvHrDdsDYDfpCZK0b0fDMJ4MLMPGWG7/0iM6fnSXsE/y1RBXwPSq0TEXv3zfS9AvjGfJ8nM38IfD8inlOuOg6Y3lO5j2LvAYqZ5Kd9BjghIrYos+wVEdtSXJF5u57t+u9/Bnh9lLsiUcyo3u9q4FURsU25zRNmy5jF1Xa/HhEvKbeNiHj6vF6AYrxHL40w/d5Wj2uAoyNi5+k8UVyQbi79f25pwSwotdHjgAsj4q6IWA/sQzGT8kK8kuJN/fUURfeOcv1fUxTRrRTvx0w7n+IMtVuiOHX9PIq9h/XAI+VJA39BMUv2PtMnSVBcDn0LYH1E3Fne/xWZeSXFZQzWRsQ6YPpw4GwZ/xh4dUTcRjE793wvbX46sGNE3FE+x4F9ee6iuEjiVeXYV1OcoDKXTwBvLk8E8SQJLYqzmUuSGsk9KElSI1lQkqRGsqAkSY1kQUmSGsmCkiQ1kgUlSWokC0qS1Ej/BvuuPYwDANa5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_silhouettes(reducedDT, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6107955063694607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "print(completeness_score(df_classes.iloc[:, 1],clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6091364049733291\n"
     ]
    }
   ],
   "source": [
    "print(homogeneity_score(df_classes.iloc[:, 1],clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before PCA\n",
    "##### completeness_score:0.6116744999910891 \n",
    "##### homogeneity_score : 0.6099656393147241\n",
    "\n",
    "\n",
    "#### After PCA\n",
    "##### completeness_score:0.6107955063694607 \n",
    "##### homogeneity_score :0.6091364049733291 \n",
    "\n",
    "The performance of the model is almost same as the scores before and after PCA are similarly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Item-Based Joke Recommendation [Dataset: jokes.zip]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem you will use a modified version of the item-based recommender algorithm from Ch. 14 of Machine Learning in Action and use it on joke ratings data based on Jester Online Joke Recommender System. The modified version of the code is provided in the module itemBasedRec.py. Most of the module will be used as is, but you will add some additional functionality.\n",
    "\n",
    "The data set contains two files. The file \"modified_jester_data.csv\" contains the ratings on 100 jokes by 1000 users (each row is a user profile). The ratings have been normalized to be between 1 and 21 (a 20-point scale), with 1 being the lowest rating. A zero indicated a missing rating. The file \"jokes.csv\" contains the joke ids mapped to the actual text of the jokes.\n",
    "\n",
    "Your tasks in this problem are the following (please also see comments for the function stubs in the provided module):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load in the joke ratings data and the joke text data into appropriate data structures. Use the \"recommend\" function to provide top 5 joke recommendations for at least 2 users. Use both standard item-based collaborative filtering (based on the rating prediction function \"standEst\") and the SVD-based version of the item-based CF (using \"svdEst\" as the prediction engine) to generate these recommendations for the two users and note the differences. You should show the text of the recommended jokes as well as the predicted ratings for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>...</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.83</td>\n",
       "      <td>7.46</td>\n",
       "      <td>11.44</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.68</td>\n",
       "      <td>2.31</td>\n",
       "      <td>10.13</td>\n",
       "      <td>4.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>...</td>\n",
       "      <td>7.46</td>\n",
       "      <td>4.11</td>\n",
       "      <td>10.32</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.82</td>\n",
       "      <td>7.65</td>\n",
       "      <td>11.05</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1.15</td>\n",
       "      <td>18.72</td>\n",
       "      <td>19.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.84</td>\n",
       "      <td>14.16</td>\n",
       "      <td>20.17</td>\n",
       "      <td>4.79</td>\n",
       "      <td>2.84</td>\n",
       "      <td>9.30</td>\n",
       "      <td>20.27</td>\n",
       "      <td>12.41</td>\n",
       "      <td>5.81</td>\n",
       "      <td>6.58</td>\n",
       "      <td>...</td>\n",
       "      <td>18.23</td>\n",
       "      <td>9.88</td>\n",
       "      <td>10.90</td>\n",
       "      <td>5.32</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.65</td>\n",
       "      <td>13.14</td>\n",
       "      <td>10.95</td>\n",
       "      <td>12.31</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.21</td>\n",
       "      <td>7.46</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.26</td>\n",
       "      <td>10.71</td>\n",
       "      <td>5.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.40</td>\n",
       "      <td>...</td>\n",
       "      <td>15.37</td>\n",
       "      <td>10.71</td>\n",
       "      <td>15.17</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.01</td>\n",
       "      <td>16.15</td>\n",
       "      <td>16.15</td>\n",
       "      <td>14.01</td>\n",
       "      <td>17.41</td>\n",
       "      <td>16.15</td>\n",
       "      <td>19.93</td>\n",
       "      <td>13.52</td>\n",
       "      <td>14.01</td>\n",
       "      <td>19.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9   ...    90  \\\n",
       "0  3.18 19.79  1.34  2.84  3.48  2.50  1.15 15.17  2.02  6.24  ... 13.82   \n",
       "1 15.08 10.71 17.36 15.37  8.62  1.34 10.27  5.66 19.88 20.22  ... 13.82   \n",
       "2  0.00  0.00  0.00  0.00 20.03 20.27 20.03 20.27  0.00  0.00  ...  0.00   \n",
       "3  0.00 19.35  0.00  0.00 12.80 19.16  8.18 17.21  0.00 12.84  ...  0.00   \n",
       "4 19.50 15.61  6.83  5.61 12.36 12.60 18.04 15.61 10.56 16.73  ... 16.19   \n",
       "5  4.83  7.46 11.44  2.50  3.91  6.68  2.31 10.13  4.35  9.20  ...  7.46   \n",
       "6  0.00  0.00  0.00  0.00 19.59  1.15 18.72 19.79  0.00  0.00  ...  0.00   \n",
       "7 17.84 14.16 20.17  4.79  2.84  9.30 20.27 12.41  5.81  6.58  ... 18.23   \n",
       "8  7.21  7.46  1.58  4.11  2.26 10.71  5.71  2.07  3.14  9.40  ... 15.37   \n",
       "9 14.01 16.15 16.15 14.01 17.41 16.15 19.93 13.52 14.01 19.16  ...  0.00   \n",
       "\n",
       "     91    92    93    94    95    96    97    98    99  \n",
       "0  0.00  0.00  0.00  0.00  0.00  5.37  0.00  0.00  0.00  \n",
       "1  6.05 10.71 18.86 10.81  8.86 14.06 11.34  6.68 12.07  \n",
       "2  0.00  0.00 20.08  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "3  0.00  0.00 11.53  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "4 16.58 15.27 16.19 16.73 12.55 14.11 17.55 12.80 12.60  \n",
       "5  4.11 10.32  8.04  8.82  7.65 11.05  1.92  5.95  7.55  \n",
       "6  0.00  0.00  0.00  0.00 13.33  0.00  0.00  0.00  0.00  \n",
       "7  9.88 10.90  5.32  7.84  7.65 13.14 10.95 12.31 11.00  \n",
       "8 10.71 15.17 10.71 10.71 10.71 10.71 10.71  7.60  6.05  \n",
       "9 15.47  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = pd.read_csv(\"modified_jester_data.csv\", header=None)\n",
    "rating.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A man visits the doctor. The doctor says \"I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This couple had an excellent relationship goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Q. What's 200 feet long and has 4 teeth? A. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Q. What's the difference between a man and a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Q. What's O. J. Simpson's Internet address? A....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill &amp; Hillary are on a trip back to Arkansas....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>How many feminists does it take to screw in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Q. Did you hear about the dyslexic devil worsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A country guy goes into a city bar that has a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Two cannibals are eating a clown one turns to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  A man visits the doctor. The doctor says \"I ha...\n",
       "1  1  This couple had an excellent relationship goin...\n",
       "2  2  Q. What's 200 feet long and has 4 teeth? A. Th...\n",
       "3  3  Q. What's the difference between a man and a t...\n",
       "4  4  Q. What's O. J. Simpson's Internet address? A....\n",
       "5  5  Bill & Hillary are on a trip back to Arkansas....\n",
       "6  6  How many feminists does it take to screw in a ...\n",
       "7  7  Q. Did you hear about the dyslexic devil worsh...\n",
       "8  8  A country guy goes into a city bar that has a ...\n",
       "9  9  Two cannibals are eating a clown one turns to ..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke= pd.read_csv(\"jokes.csv\", header=None)\n",
    "joke.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Example output for \"print_most_similar_jokes\":\\n\\nSelected joke: \\n\\nQ. What\\'s the difference between a man and a toilet? A. A toilet doesn\\'t follow you around after you use it.\\n\\nTop 5 Recommended jokes are :\\n\\nQ: What\\'s the difference between a Lawyer and a Plumber? A: A Plumber works to unclog the system. \\n_______________\\nWhat do you call an American in the finals of the world cup? \"Hey Beer Man!\" \\n_______________\\nQ. What\\'s 200 feet long and has 4 teeth? <P>A. The front row at a Willie Nelson Concert. \\n_______________\\nA country guy goes into a city bar that has a dress code and the maitred\\' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He\\'s got jumper cables in the trunk! So he wrapsthem around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d\\' is reluctant but says to the guy \"Okay you\\'re a pretty resourceful fellow you can come in... but just don\\'t start anything\"!   \\n_______________\\nWhat do you get when you run over a parakeet with a lawnmower? <P>Shredded tweet. \\n_______________\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the code is provided in the module itemBasedRec.py\n",
    "\n",
    "from numpy import *\n",
    "from numpy import linalg as la\n",
    "import numpy as np\n",
    "\n",
    "def euclidSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsonSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosineSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)\n",
    "\n",
    "\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0: continue\n",
    "        overLap = nonzero(logical_and(data[:,item]>0, data[:,j]>0))[0]\n",
    "        if len(overLap) == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = simMeas(data[overLap,item], data[overLap,j])\n",
    "        #print('the %d and %d similarity is: %f' % (item, j, similarity))\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "    \n",
    "def svdEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    k = 4 #number of dimension for SVD\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    Sig_k = mat(eye(k)*Sigma[:k]) #arrange Sig_k into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:k] * Sig_k.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T, xformedItems[j,:].T)\n",
    "        #print('the %d and %d similarity is: %f' % (item, j, similarity))\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "\n",
    "def recommend(dataMat, user, N=3, simMeas=pearsonSim, estMethod=standEst):\n",
    "    unratedItems = nonzero(dataMat[user,:].A==0)[1] #find unrated items \n",
    "    if len(unratedItems) == 0: return 'you rated everything'\n",
    "    itemScores = []\n",
    "    for item in unratedItems:\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        itemScores.append((item, estimatedScore))\n",
    "    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function performs evaluation on a single user based on the test_ratio\n",
    "# For example, with test_ratio = 0.2, a randomly selected 20 percent of rated \n",
    "# items by the user are withheld as test data. The remaining part of the user\n",
    "# profile is used as input for the estimation functions to predict the \n",
    "# withheld ratings and compute the error for this user\n",
    "\n",
    "def cross_validate_user(dataMat, user, test_ratio, estMethod=standEst, simMeas=pearsonSim):\n",
    "    dataMat = np.array(dataMat)\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = int(test_ratio * len(rated_items_by_user))\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    dataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating estimation below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "    # Compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # Estimate rating on the withheld item\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        error_u = error_u + abs(estimatedScore - original_user_profile[item])\t\n",
    "    \n",
    "    # Now restore ratings of the withheld items to the user profile\n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "    \n",
    "    # Return sum of absolute errors and the count of test cases for this user\n",
    "    # Note that these will have to be accumulated for each user to compute MAE\n",
    "    return error_u, count_u\n",
    "    \n",
    "# def test(dataMat, test_ratio, estMethod, simMeas=pearsonSim):\n",
    "#     # Write this function to iterate over all users and for each perform evaluation by calling\n",
    "#     # the above cross_validate_user function on each user. MAE will be the ratio of total error \n",
    "#     # across all test cases to the total number of test cases, across all users\n",
    "#     return # MAE\n",
    "\n",
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsonSim):\n",
    "    # Write this function to find the k most similar jokes (based on user ratings) to a queryJoke\n",
    "    # The queryJoke is a joke id as given in the 'jokes.csv' file (an corresponding to the a column in dataMat)\n",
    "    # You must compare ratings for the queryJoke (the column in dataMat corresponding to the joke), to all\n",
    "    # other joke rating vectors and return the top k. Note that this is the same as performing KNN on the \n",
    "    # columns of dataMat. The function must retrieve the text of the joke from 'jokes.csv' file and print both\n",
    "    # the queryJoke text as well as the text of the returned top-k jokes.\n",
    "    return\n",
    "\n",
    "def load_jokes(file):\n",
    "    jokes = np.genfromtxt(file, delimiter=',', dtype=str)\n",
    "    jokes = np.array(jokes[:,1])\n",
    "    return jokes\n",
    "\n",
    "def get_joke_text(jokes, id):\n",
    "    return jokes[id]\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# dataMat = np.genfromtxt('modified_jester_data.csv',delimiter=',')\n",
    "# dataMat = np.mat(dataMat)\n",
    "\n",
    "# MAE = test(dataMat, 0.2, svdEst, pearsonSim)\n",
    "# MAE = test(dataMat, 0.2, standEst, pearsonSim)\n",
    "\n",
    "# jokes = load_jokes('jokes.csv')\n",
    "# print_most_similar_jokes(dataMat, jokes, 3, 5, pearsonSim)\n",
    "\n",
    "''' Example output for \"print_most_similar_jokes\":\n",
    "\n",
    "Selected joke: \n",
    "\n",
    "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n",
    "\n",
    "Top 5 Recommended jokes are :\n",
    "\n",
    "Q: What's the difference between a Lawyer and a Plumber? A: A Plumber works to unclog the system. \n",
    "_______________\n",
    "What do you call an American in the finals of the world cup? \"Hey Beer Man!\" \n",
    "_______________\n",
    "Q. What's 200 feet long and has 4 teeth? <P>A. The front row at a Willie Nelson Concert. \n",
    "_______________\n",
    "A country guy goes into a city bar that has a dress code and the maitred' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He's got jumper cables in the trunk! So he wrapsthem around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d' is reluctant but says to the guy \"Okay you're a pretty resourceful fellow you can come in... but just don't start anything\"!   \n",
    "_______________\n",
    "What do you get when you run over a parakeet with a lawnmower? <P>Shredded tweet. \n",
    "_______________\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMat = np.mat(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Recommended jokes for user 46 using prediction engine: standEst\n",
      "\n",
      "\n",
      "\n",
      "Joke number :   71 \n",
      " On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      " RATING (Predicted) =  16.92732138621851 \n",
      "\n",
      "Joke number :   79 \n",
      " Hillary Bill Clinton and the Pope are sitting together on an airplane. Bill says \"I could throw one thousand dollar bill out of this plane and make one person very happy.\"Hillary says \"I could throw 10 hundred dollar bills out of the plane and make 10 people very happy.\"The Pope chips in and says \"I could throw Bill out of the airplane and make the whole country happy.\" \n",
      " RATING (Predicted) =  16.88799094921455 \n",
      "\n",
      "Joke number :   75 \n",
      " There once was a man and a woman that both  got in  a terrible car wreck. Both of their vehicles  were completely destroyed buy fortunately no one  was   hurt.  In thankfulness the woman said to the man 'We are both okay so we should celebrate. I have   a  bottle of wine in my car let's open it.' So the woman got the bottle out of the car and  handed it to the man. The man took a really big drink and handed the woman the bottle. The  woman  closed the bottle and put it down. The man  asked  'Aren't you going to take a drink?' The woman cleverly replied 'No I think I'll  just  wait for the cops to get here.' \n",
      " RATING (Predicted) =  16.88126756849171 \n",
      "\n",
      "Joke number :   78 \n",
      " Q: Ever wonder why the IRS calls it Form 1040?A: Because for every $50 that you earn you get 10 and they get 40. \n",
      " RATING (Predicted) =  16.8727795316786 \n",
      "\n",
      "Joke number :   97 \n",
      " Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn? \n",
      " RATING (Predicted) =  16.871602726950943 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 5 Recommended jokes for user 51 using prediction engine: standEst\n",
      "\n",
      "\n",
      "\n",
      "Joke number :   71 \n",
      " On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      " RATING (Predicted) =  13.186153679007786 \n",
      "\n",
      "Joke number :   99 \n",
      " Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen. \n",
      " RATING (Predicted) =  13.183611139199664 \n",
      "\n",
      "Joke number :   5 \n",
      " Bill & Hillary are on a trip back to Arkansas. They're almost out of gas so Bill pulls into a service station on the outskirts of town. The attendant runs out of the station to serve them when Hillary realizes it's an old boyfriend from high school. She and the attendant chat as he gases up their car and cleans the windows. Then they all say good-bye. As Bill pulls the car onto the road he turns to Hillary and says 'Now aren't you glad you married me and not him ? You could've been the wife of a grease monkey !' To which Hillary replied 'No Bill. If I would have married him you'd be pumping gas and he would be the President !'  \n",
      " RATING (Predicted) =  13.178263848027646 \n",
      "\n",
      "Joke number :   97 \n",
      " Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn? \n",
      " RATING (Predicted) =  13.169856206590653 \n",
      "\n",
      "Joke number :   82 \n",
      " What a woman says:\"This place is a mess!  C'monYou and I need to clean upYour stuff is lying on the floor and you'll have no clothes to wear if we don't do laundry right now!\"What a man hears:blah blah blah blah C'mon blah blah blah blah you and I blah blah blah blah on the floor blah blah blah blah no clothes blah blah blah blah RIGHT NOW! \n",
      " RATING (Predicted) =  13.158633895709965 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 5 Recommended jokes for user 73 using prediction engine: standEst\n",
      "\n",
      "\n",
      "\n",
      "Joke number :   71 \n",
      " On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      " RATING (Predicted) =  17.597599106001336 \n",
      "\n",
      "Joke number :   99 \n",
      " Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen. \n",
      " RATING (Predicted) =  17.591249143847207 \n",
      "\n",
      "Joke number :   82 \n",
      " What a woman says:\"This place is a mess!  C'monYou and I need to clean upYour stuff is lying on the floor and you'll have no clothes to wear if we don't do laundry right now!\"What a man hears:blah blah blah blah C'mon blah blah blah blah you and I blah blah blah blah on the floor blah blah blah blah no clothes blah blah blah blah RIGHT NOW! \n",
      " RATING (Predicted) =  17.561488534067472 \n",
      "\n",
      "Joke number :   97 \n",
      " Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn? \n",
      " RATING (Predicted) =  17.548459271208415 \n",
      "\n",
      "Joke number :   79 \n",
      " Hillary Bill Clinton and the Pope are sitting together on an airplane. Bill says \"I could throw one thousand dollar bill out of this plane and make one person very happy.\"Hillary says \"I could throw 10 hundred dollar bills out of the plane and make 10 people very happy.\"The Pope chips in and says \"I could throw Bill out of the airplane and make the whole country happy.\" \n",
      " RATING (Predicted) =  17.518419695126337 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l =[46,51,73] \n",
    "N = 5\n",
    "jokeA = np.array(joke.iloc[:,1])\n",
    "for u in l:\n",
    "    Recom = recommend(dataMat,u,N,pearsonSim,standEst)\n",
    "    if(Recom=='you rated everything'):\n",
    "        break;\n",
    "    print(\"\\nTop 5 Recommended jokes for user\",u,\"using prediction engine: standEst\")\n",
    "    print('\\n\\n')\n",
    "    for i in range(len(Recom)):\n",
    "        print(\"Joke number :  \",Recom[i][0],\"\\n\",jokeA[Recom[i][0]],'\\n RATING (Predicted) = ',Recom[i][1],'\\n')\n",
    "    print('\\n\\n')\n",
    "# u =51\n",
    "# N = 5\n",
    "# jokes_arr = np.array(joke.iloc[:,1])\n",
    "# recom_jokes = recommend(dataMat,u,N,pearsonSim,svdEst)\n",
    "# print(\"Top 5 Recommended jokes for user#\",u)\n",
    "    \n",
    "# for i in range(len(recom_jokes)):\n",
    "#     print(\"Joke number :  \",recom_jokes[i][0],\"\\n\",jokes_arr[recom_jokes[i][0]],'\\n RATING (Predicted) = ',recom_jokes[i][1],'\\n')\n",
    "# print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Recommended jokes for user 46 using prediction engine: svdEst\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Joke number :   78 \n",
      " Q: Ever wonder why the IRS calls it Form 1040?A: Because for every $50 that you earn you get 10 and they get 40. \n",
      " RATING (Predicted) =  16.68007133244043 \n",
      "\n",
      "Joke number :   79 \n",
      " Hillary Bill Clinton and the Pope are sitting together on an airplane. Bill says \"I could throw one thousand dollar bill out of this plane and make one person very happy.\"Hillary says \"I could throw 10 hundred dollar bills out of the plane and make 10 people very happy.\"The Pope chips in and says \"I could throw Bill out of the airplane and make the whole country happy.\" \n",
      " RATING (Predicted) =  16.663007158414107 \n",
      "\n",
      "Joke number :   72 \n",
      " Q: What is the difference between George  Washington Richard Nixon and Bill Clinton? A: Washington couldn't tell a lie Nixon couldn't   tell the truth andClinton doesn't know the difference. \n",
      " RATING (Predicted) =  16.59510236923362 \n",
      "\n",
      "Joke number :   99 \n",
      " Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen. \n",
      " RATING (Predicted) =  16.55476846684357 \n",
      "\n",
      "Joke number :   71 \n",
      " On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      " RATING (Predicted) =  16.51146744130765 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 5 Recommended jokes for user 51 using prediction engine: svdEst\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Joke number :   71 \n",
      " On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\" \n",
      " RATING (Predicted) =  13.110636635081374 \n",
      "\n",
      "Joke number :   79 \n",
      " Hillary Bill Clinton and the Pope are sitting together on an airplane. Bill says \"I could throw one thousand dollar bill out of this plane and make one person very happy.\"Hillary says \"I could throw 10 hundred dollar bills out of the plane and make 10 people very happy.\"The Pope chips in and says \"I could throw Bill out of the airplane and make the whole country happy.\" \n",
      " RATING (Predicted) =  13.071905385196708 \n",
      "\n",
      "Joke number :   77 \n",
      " Q: What's the difference between the government  and  the Mafia? A: One of them is organized. \n",
      " RATING (Predicted) =  13.0569827316643 \n",
      "\n",
      "Joke number :   78 \n",
      " Q: Ever wonder why the IRS calls it Form 1040?A: Because for every $50 that you earn you get 10 and they get 40. \n",
      " RATING (Predicted) =  13.04811208784985 \n",
      "\n",
      "Joke number :   76 \n",
      " If pro- is the opposite of con- then congress must be the opposite of progress. \n",
      " RATING (Predicted) =  13.040399264729492 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 5 Recommended jokes for user 73 using prediction engine: svdEst\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Joke number :   23 \n",
      " What do you get when you run over a parakeet with a lawnmower? Shredded tweet. \n",
      " RATING (Predicted) =  17.652271859284433 \n",
      "\n",
      "Joke number :   22 \n",
      " Q: What is the Australian word for a boomerang that won't   come back? A: A stick \n",
      " RATING (Predicted) =  17.575650270467047 \n",
      "\n",
      "Joke number :   8 \n",
      " A country guy goes into a city bar that has a dress code and the maitred' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He's got jumper cables in the trunk! So he wraps them around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d' is reluctant but says to the guy \"Okay you're a pretty resourceful fellow you can come in... but just don't start anything\"!   \n",
      " RATING (Predicted) =  17.41052080380861 \n",
      "\n",
      "Joke number :   78 \n",
      " Q: Ever wonder why the IRS calls it Form 1040?A: Because for every $50 that you earn you get 10 and they get 40. \n",
      " RATING (Predicted) =  17.39657487089361 \n",
      "\n",
      "Joke number :   99 \n",
      " Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen. \n",
      " RATING (Predicted) =  17.386810843930892 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l =[46,51,73]\n",
    "N = 5\n",
    "jokeA = np.array(joke.iloc[:,1])\n",
    "for u in l:\n",
    "    Recom = recommend(dataMat,u,N,pearsonSim,svdEst)\n",
    "    if(Recom=='you rated everything'):\n",
    "        break;\n",
    "    print(\"\\nTop 5 Recommended jokes for user\",u,\"using prediction engine: svdEst\")\n",
    "    print('\\n\\n\\n')\n",
    "    for i in range(len(Recom)):\n",
    "        print(\"Joke number :  \",Recom[i][0],\"\\n\",jokeA[Recom[i][0]],'\\n RATING (Predicted) = ',Recom[i][1],'\\n')\n",
    "    print('\\n\\n')\n",
    "\n",
    "# u =51\n",
    "# N = 5\n",
    "# jokes_arr = np.array(joke.iloc[:,1])\n",
    "# recom_jokes = recommend(dataMat,u,N,pearsonSim,svdEst)\n",
    "# print(\"Top 5 Recommended jokes for user#\",u)\n",
    "    \n",
    "# for i in range(len(recom_jokes)):\n",
    "#     print(\"Joke number :  \",recom_jokes[i][0],\"\\n\",jokes_arr[recom_jokes[i][0]],'\\n RATING (Predicted) = ',recom_jokes[i][1],'\\n')\n",
    "# print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Complete the definition for the function \"test\". This function iterates over all users and for each performs evaluation (by calling the provided \"cross_validate_user\" function), and returns the error information necessary to compute Mean Absolute Error (MAE). Use this function to perform evaluation (with 20% test-ratio for each user) comparing MAE results using the rating prediction function \"standEst\" with results using the \"svdEst\" prediction function. [Note: See comments provided in the module for hints on accomplishing these tasks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totCount = 0\n",
    "# totError = 0\n",
    "# for u in range(M):\n",
    "#     err_u = 0\n",
    "#     rateCount_u = 0\n",
    "#     for j in range(N):\n",
    "#         if (Ratings[u,j] > 0): ### Only use known ratings computing error\n",
    "#             rateCount_u += 1\n",
    "#             err_u += abs(np.dot(fP[u],fQ[j]) - Ratings[u,j])\n",
    "#     print(\"Mean Absolute Error for User %d = %0.3f\" %(u, err_u/rateCount_u))\n",
    "#     totCount += rateCount_u\n",
    "#     totError += err_u\n",
    "# print\n",
    "# print(\"Overall Mean Absolute Error = %0.3f\" %(totError/totCount))\n",
    "\n",
    "\n",
    "\n",
    "import itemBasedRec as ibr\n",
    "\n",
    "def test(dataMat, test_ratio, estMethod, simMeas=pearsonSim):\n",
    "    # Write this function to iterate over all users and for each perform evaluation by calling\n",
    "    # the above cross_validate_user function on each user. MAE will be the ratio of total error \n",
    "    # across all test cases to the total number of test cases, across all users\n",
    "    \n",
    "    # Must keep running count of MAE and number of test cases\n",
    "    totCount = 0\n",
    "    totError = 0\n",
    "\n",
    "    # Iterate through \n",
    "    for i in range(len(dataMat)):\n",
    "        err_u, rateCount_u = ibr.cross_validate_user(dataMat, i, test_ratio, eval(estMethod), simMeas)\n",
    "        totError += err_u\n",
    "        totCount += rateCount_u\n",
    "        print(\"Mean Absolute Error for User %d = %0.3f\" %(i, err_u/rateCount_u)) \n",
    "    print(\"\\n\\n\\n\\nOverall Mean Absolute Error = %0.3f\" %(totError/totCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 0 = 6.292\n",
      "Mean Absolute Error for User 1 = 4.286\n",
      "Mean Absolute Error for User 2 = 1.165\n",
      "Mean Absolute Error for User 3 = 4.874\n",
      "Mean Absolute Error for User 4 = 3.857\n",
      "Mean Absolute Error for User 5 = 2.786\n",
      "Mean Absolute Error for User 6 = 1.770\n",
      "Mean Absolute Error for User 7 = 4.522\n",
      "Mean Absolute Error for User 8 = 3.197\n",
      "Mean Absolute Error for User 9 = 2.119\n",
      "Mean Absolute Error for User 10 = 3.049\n",
      "Mean Absolute Error for User 11 = 2.241\n",
      "Mean Absolute Error for User 12 = 1.242\n",
      "Mean Absolute Error for User 13 = 3.971\n",
      "Mean Absolute Error for User 14 = 5.143\n",
      "Mean Absolute Error for User 15 = 3.048\n",
      "Mean Absolute Error for User 16 = 3.749\n",
      "Mean Absolute Error for User 17 = 2.705\n",
      "Mean Absolute Error for User 18 = 2.718\n",
      "Mean Absolute Error for User 19 = 5.029\n",
      "Mean Absolute Error for User 20 = 1.460\n",
      "Mean Absolute Error for User 21 = 3.577\n",
      "Mean Absolute Error for User 22 = 3.144\n",
      "Mean Absolute Error for User 23 = 5.166\n",
      "Mean Absolute Error for User 24 = 2.659\n",
      "Mean Absolute Error for User 25 = 2.349\n",
      "Mean Absolute Error for User 26 = 4.297\n",
      "Mean Absolute Error for User 27 = 4.860\n",
      "Mean Absolute Error for User 28 = 3.123\n",
      "Mean Absolute Error for User 29 = 1.691\n",
      "Mean Absolute Error for User 30 = 3.423\n",
      "Mean Absolute Error for User 31 = 2.431\n",
      "Mean Absolute Error for User 32 = 2.337\n",
      "Mean Absolute Error for User 33 = 3.936\n",
      "Mean Absolute Error for User 34 = 2.391\n",
      "Mean Absolute Error for User 35 = 5.378\n",
      "Mean Absolute Error for User 36 = 2.578\n",
      "Mean Absolute Error for User 37 = 1.817\n",
      "Mean Absolute Error for User 38 = 0.182\n",
      "Mean Absolute Error for User 39 = 5.634\n",
      "Mean Absolute Error for User 40 = 3.290\n",
      "Mean Absolute Error for User 41 = 3.681\n",
      "Mean Absolute Error for User 42 = 3.360\n",
      "Mean Absolute Error for User 43 = 3.754\n",
      "Mean Absolute Error for User 44 = 2.409\n",
      "Mean Absolute Error for User 45 = 5.045\n",
      "Mean Absolute Error for User 46 = 3.293\n",
      "Mean Absolute Error for User 47 = 4.821\n",
      "Mean Absolute Error for User 48 = 2.630\n",
      "Mean Absolute Error for User 49 = 6.559\n",
      "Mean Absolute Error for User 50 = 2.674\n",
      "Mean Absolute Error for User 51 = 1.824\n",
      "Mean Absolute Error for User 52 = 4.432\n",
      "Mean Absolute Error for User 53 = 6.350\n",
      "Mean Absolute Error for User 54 = 4.640\n",
      "Mean Absolute Error for User 55 = 4.476\n",
      "Mean Absolute Error for User 56 = 4.220\n",
      "Mean Absolute Error for User 57 = 3.686\n",
      "Mean Absolute Error for User 58 = 4.673\n",
      "Mean Absolute Error for User 59 = 7.117\n",
      "Mean Absolute Error for User 60 = 2.854\n",
      "Mean Absolute Error for User 61 = 3.296\n",
      "Mean Absolute Error for User 62 = 3.479\n",
      "Mean Absolute Error for User 63 = 3.051\n",
      "Mean Absolute Error for User 64 = 1.725\n",
      "Mean Absolute Error for User 65 = 5.273\n",
      "Mean Absolute Error for User 66 = 1.970\n",
      "Mean Absolute Error for User 67 = 2.406\n",
      "Mean Absolute Error for User 68 = 5.563\n",
      "Mean Absolute Error for User 69 = 2.287\n",
      "Mean Absolute Error for User 70 = 2.431\n",
      "Mean Absolute Error for User 71 = 2.214\n",
      "Mean Absolute Error for User 72 = 1.502\n",
      "Mean Absolute Error for User 73 = 3.215\n",
      "Mean Absolute Error for User 74 = 4.902\n",
      "Mean Absolute Error for User 75 = 6.540\n",
      "Mean Absolute Error for User 76 = 6.562\n",
      "Mean Absolute Error for User 77 = 2.462\n",
      "Mean Absolute Error for User 78 = 4.884\n",
      "Mean Absolute Error for User 79 = 3.137\n",
      "Mean Absolute Error for User 80 = 2.439\n",
      "Mean Absolute Error for User 81 = 5.650\n",
      "Mean Absolute Error for User 82 = 5.594\n",
      "Mean Absolute Error for User 83 = 6.244\n",
      "Mean Absolute Error for User 84 = 2.086\n",
      "Mean Absolute Error for User 85 = 2.514\n",
      "Mean Absolute Error for User 86 = 2.820\n",
      "Mean Absolute Error for User 87 = 3.374\n",
      "Mean Absolute Error for User 88 = 4.655\n",
      "Mean Absolute Error for User 89 = 3.910\n",
      "Mean Absolute Error for User 90 = 3.860\n",
      "Mean Absolute Error for User 91 = 4.421\n",
      "Mean Absolute Error for User 92 = 6.287\n",
      "Mean Absolute Error for User 93 = 3.268\n",
      "Mean Absolute Error for User 94 = 3.211\n",
      "Mean Absolute Error for User 95 = 5.366\n",
      "Mean Absolute Error for User 96 = 4.940\n",
      "Mean Absolute Error for User 97 = 5.919\n",
      "Mean Absolute Error for User 98 = 2.435\n",
      "Mean Absolute Error for User 99 = 1.513\n",
      "Mean Absolute Error for User 100 = 5.457\n",
      "Mean Absolute Error for User 101 = 5.034\n",
      "Mean Absolute Error for User 102 = 3.386\n",
      "Mean Absolute Error for User 103 = 2.245\n",
      "Mean Absolute Error for User 104 = 2.599\n",
      "Mean Absolute Error for User 105 = 6.815\n",
      "Mean Absolute Error for User 106 = 4.930\n",
      "Mean Absolute Error for User 107 = 0.954\n",
      "Mean Absolute Error for User 108 = 2.016\n",
      "Mean Absolute Error for User 109 = 3.375\n",
      "Mean Absolute Error for User 110 = 1.534\n",
      "Mean Absolute Error for User 111 = 1.741\n",
      "Mean Absolute Error for User 112 = 4.049\n",
      "Mean Absolute Error for User 113 = 4.096\n",
      "Mean Absolute Error for User 114 = 4.354\n",
      "Mean Absolute Error for User 115 = 5.945\n",
      "Mean Absolute Error for User 116 = 2.763\n",
      "Mean Absolute Error for User 117 = 3.202\n",
      "Mean Absolute Error for User 118 = 6.159\n",
      "Mean Absolute Error for User 119 = 5.594\n",
      "Mean Absolute Error for User 120 = 2.895\n",
      "Mean Absolute Error for User 121 = 1.504\n",
      "Mean Absolute Error for User 122 = 3.760\n",
      "Mean Absolute Error for User 123 = 5.844\n",
      "Mean Absolute Error for User 124 = 4.403\n",
      "Mean Absolute Error for User 125 = 3.468\n",
      "Mean Absolute Error for User 126 = 2.273\n",
      "Mean Absolute Error for User 127 = 2.107\n",
      "Mean Absolute Error for User 128 = 3.894\n",
      "Mean Absolute Error for User 129 = 3.730\n",
      "Mean Absolute Error for User 130 = 5.598\n",
      "Mean Absolute Error for User 131 = 2.651\n",
      "Mean Absolute Error for User 132 = 4.542\n",
      "Mean Absolute Error for User 133 = 5.310\n",
      "Mean Absolute Error for User 134 = 4.934\n",
      "Mean Absolute Error for User 135 = 7.587\n",
      "Mean Absolute Error for User 136 = 2.614\n",
      "Mean Absolute Error for User 137 = 6.272\n",
      "Mean Absolute Error for User 138 = 2.702\n",
      "Mean Absolute Error for User 139 = 4.708\n",
      "Mean Absolute Error for User 140 = 5.061\n",
      "Mean Absolute Error for User 141 = 5.496\n",
      "Mean Absolute Error for User 142 = 6.112\n",
      "Mean Absolute Error for User 143 = 1.982\n",
      "Mean Absolute Error for User 144 = 2.735\n",
      "Mean Absolute Error for User 145 = 2.602\n",
      "Mean Absolute Error for User 146 = 2.347\n",
      "Mean Absolute Error for User 147 = 1.779\n",
      "Mean Absolute Error for User 148 = 2.731\n",
      "Mean Absolute Error for User 149 = 3.353\n",
      "Mean Absolute Error for User 150 = 5.396\n",
      "Mean Absolute Error for User 151 = 6.248\n",
      "Mean Absolute Error for User 152 = 3.771\n",
      "Mean Absolute Error for User 153 = 4.965\n",
      "Mean Absolute Error for User 154 = 3.865\n",
      "Mean Absolute Error for User 155 = 4.357\n",
      "Mean Absolute Error for User 156 = 3.839\n",
      "Mean Absolute Error for User 157 = 4.073\n",
      "Mean Absolute Error for User 158 = 4.190\n",
      "Mean Absolute Error for User 159 = 3.668\n",
      "Mean Absolute Error for User 160 = 5.460\n",
      "Mean Absolute Error for User 161 = 5.958\n",
      "Mean Absolute Error for User 162 = 3.790\n",
      "Mean Absolute Error for User 163 = 3.082\n",
      "Mean Absolute Error for User 164 = 5.696\n",
      "Mean Absolute Error for User 165 = 4.488\n",
      "Mean Absolute Error for User 166 = 2.046\n",
      "Mean Absolute Error for User 167 = 2.253\n",
      "Mean Absolute Error for User 168 = 4.165\n",
      "Mean Absolute Error for User 169 = 5.510\n",
      "Mean Absolute Error for User 170 = 7.700\n",
      "Mean Absolute Error for User 171 = 2.733\n",
      "Mean Absolute Error for User 172 = 2.606\n",
      "Mean Absolute Error for User 173 = 2.307\n",
      "Mean Absolute Error for User 174 = 3.096\n",
      "Mean Absolute Error for User 175 = 8.971\n",
      "Mean Absolute Error for User 176 = 3.841\n",
      "Mean Absolute Error for User 177 = 4.643\n",
      "Mean Absolute Error for User 178 = 1.427\n",
      "Mean Absolute Error for User 179 = 4.228\n",
      "Mean Absolute Error for User 180 = 2.782\n",
      "Mean Absolute Error for User 181 = 3.739\n",
      "Mean Absolute Error for User 182 = 1.623\n",
      "Mean Absolute Error for User 183 = 3.223\n",
      "Mean Absolute Error for User 184 = 4.569\n",
      "Mean Absolute Error for User 185 = 4.243\n",
      "Mean Absolute Error for User 186 = 2.946\n",
      "Mean Absolute Error for User 187 = 3.266\n",
      "Mean Absolute Error for User 188 = 3.925\n",
      "Mean Absolute Error for User 189 = 2.907\n",
      "Mean Absolute Error for User 190 = 4.086\n",
      "Mean Absolute Error for User 191 = 3.240\n",
      "Mean Absolute Error for User 192 = 3.494\n",
      "Mean Absolute Error for User 193 = 4.531\n",
      "Mean Absolute Error for User 194 = 3.824\n",
      "Mean Absolute Error for User 195 = 1.918\n",
      "Mean Absolute Error for User 196 = 3.031\n",
      "Mean Absolute Error for User 197 = 2.676\n",
      "Mean Absolute Error for User 198 = 3.565\n",
      "Mean Absolute Error for User 199 = 5.114\n",
      "Mean Absolute Error for User 200 = 4.063\n",
      "Mean Absolute Error for User 201 = 4.697\n",
      "Mean Absolute Error for User 202 = 3.270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 203 = 2.772\n",
      "Mean Absolute Error for User 204 = 5.353\n",
      "Mean Absolute Error for User 205 = 3.925\n",
      "Mean Absolute Error for User 206 = 5.121\n",
      "Mean Absolute Error for User 207 = 2.630\n",
      "Mean Absolute Error for User 208 = 4.587\n",
      "Mean Absolute Error for User 209 = 2.269\n",
      "Mean Absolute Error for User 210 = 4.833\n",
      "Mean Absolute Error for User 211 = 5.586\n",
      "Mean Absolute Error for User 212 = 4.734\n",
      "Mean Absolute Error for User 213 = 2.491\n",
      "Mean Absolute Error for User 214 = 2.665\n",
      "Mean Absolute Error for User 215 = 3.302\n",
      "Mean Absolute Error for User 216 = 2.739\n",
      "Mean Absolute Error for User 217 = 3.318\n",
      "Mean Absolute Error for User 218 = 4.914\n",
      "Mean Absolute Error for User 219 = 1.925\n",
      "Mean Absolute Error for User 220 = 3.560\n",
      "Mean Absolute Error for User 221 = 1.753\n",
      "Mean Absolute Error for User 222 = 4.937\n",
      "Mean Absolute Error for User 223 = 1.051\n",
      "Mean Absolute Error for User 224 = 2.160\n",
      "Mean Absolute Error for User 225 = 6.839\n",
      "Mean Absolute Error for User 226 = 2.527\n",
      "Mean Absolute Error for User 227 = 4.019\n",
      "Mean Absolute Error for User 228 = 2.674\n",
      "Mean Absolute Error for User 229 = 4.761\n",
      "Mean Absolute Error for User 230 = 7.397\n",
      "Mean Absolute Error for User 231 = 2.768\n",
      "Mean Absolute Error for User 232 = 2.718\n",
      "Mean Absolute Error for User 233 = 5.022\n",
      "Mean Absolute Error for User 234 = 5.982\n",
      "Mean Absolute Error for User 235 = 3.118\n",
      "Mean Absolute Error for User 236 = 5.465\n",
      "Mean Absolute Error for User 237 = 2.226\n",
      "Mean Absolute Error for User 238 = 4.333\n",
      "Mean Absolute Error for User 239 = 6.421\n",
      "Mean Absolute Error for User 240 = 2.391\n",
      "Mean Absolute Error for User 241 = 0.529\n",
      "Mean Absolute Error for User 242 = 3.148\n",
      "Mean Absolute Error for User 243 = 2.870\n",
      "Mean Absolute Error for User 244 = 4.280\n",
      "Mean Absolute Error for User 245 = 2.510\n",
      "Mean Absolute Error for User 246 = 7.245\n",
      "Mean Absolute Error for User 247 = 3.109\n",
      "Mean Absolute Error for User 248 = 3.256\n",
      "Mean Absolute Error for User 249 = 1.607\n",
      "Mean Absolute Error for User 250 = 4.854\n",
      "Mean Absolute Error for User 251 = 2.157\n",
      "Mean Absolute Error for User 252 = 4.565\n",
      "Mean Absolute Error for User 253 = 2.914\n",
      "Mean Absolute Error for User 254 = 4.400\n",
      "Mean Absolute Error for User 255 = 3.879\n",
      "Mean Absolute Error for User 256 = 4.243\n",
      "Mean Absolute Error for User 257 = 3.524\n",
      "Mean Absolute Error for User 258 = 2.242\n",
      "Mean Absolute Error for User 259 = 5.288\n",
      "Mean Absolute Error for User 260 = 6.915\n",
      "Mean Absolute Error for User 261 = 4.601\n",
      "Mean Absolute Error for User 262 = 2.876\n",
      "Mean Absolute Error for User 263 = 4.600\n",
      "Mean Absolute Error for User 264 = 3.542\n",
      "Mean Absolute Error for User 265 = 3.180\n",
      "Mean Absolute Error for User 266 = 2.666\n",
      "Mean Absolute Error for User 267 = 6.110\n",
      "Mean Absolute Error for User 268 = 1.961\n",
      "Mean Absolute Error for User 269 = 6.950\n",
      "Mean Absolute Error for User 270 = 3.448\n",
      "Mean Absolute Error for User 271 = 3.724\n",
      "Mean Absolute Error for User 272 = 2.909\n",
      "Mean Absolute Error for User 273 = 2.094\n",
      "Mean Absolute Error for User 274 = 2.930\n",
      "Mean Absolute Error for User 275 = 7.257\n",
      "Mean Absolute Error for User 276 = 2.523\n",
      "Mean Absolute Error for User 277 = 6.222\n",
      "Mean Absolute Error for User 278 = 5.252\n",
      "Mean Absolute Error for User 279 = 3.816\n",
      "Mean Absolute Error for User 280 = 4.117\n",
      "Mean Absolute Error for User 281 = 3.017\n",
      "Mean Absolute Error for User 282 = 2.662\n",
      "Mean Absolute Error for User 283 = 4.200\n",
      "Mean Absolute Error for User 284 = 0.997\n",
      "Mean Absolute Error for User 285 = 4.759\n",
      "Mean Absolute Error for User 286 = 2.379\n",
      "Mean Absolute Error for User 287 = 2.760\n",
      "Mean Absolute Error for User 288 = 3.184\n",
      "Mean Absolute Error for User 289 = 3.192\n",
      "Mean Absolute Error for User 290 = 4.788\n",
      "Mean Absolute Error for User 291 = 1.323\n",
      "Mean Absolute Error for User 292 = 4.202\n",
      "Mean Absolute Error for User 293 = 0.857\n",
      "Mean Absolute Error for User 294 = 6.698\n",
      "Mean Absolute Error for User 295 = 1.475\n",
      "Mean Absolute Error for User 296 = 5.204\n",
      "Mean Absolute Error for User 297 = 5.196\n",
      "Mean Absolute Error for User 298 = 3.763\n",
      "Mean Absolute Error for User 299 = 3.765\n",
      "Mean Absolute Error for User 300 = 1.014\n",
      "Mean Absolute Error for User 301 = 3.196\n",
      "Mean Absolute Error for User 302 = 3.547\n",
      "Mean Absolute Error for User 303 = 6.214\n",
      "Mean Absolute Error for User 304 = 3.144\n",
      "Mean Absolute Error for User 305 = 4.454\n",
      "Mean Absolute Error for User 306 = 3.520\n",
      "Mean Absolute Error for User 307 = 4.002\n",
      "Mean Absolute Error for User 308 = 2.661\n",
      "Mean Absolute Error for User 309 = 4.555\n",
      "Mean Absolute Error for User 310 = 3.313\n",
      "Mean Absolute Error for User 311 = 4.245\n",
      "Mean Absolute Error for User 312 = 1.702\n",
      "Mean Absolute Error for User 313 = 2.378\n",
      "Mean Absolute Error for User 314 = 4.621\n",
      "Mean Absolute Error for User 315 = 5.201\n",
      "Mean Absolute Error for User 316 = 4.566\n",
      "Mean Absolute Error for User 317 = 1.849\n",
      "Mean Absolute Error for User 318 = 5.565\n",
      "Mean Absolute Error for User 319 = 2.460\n",
      "Mean Absolute Error for User 320 = 5.352\n",
      "Mean Absolute Error for User 321 = 2.852\n",
      "Mean Absolute Error for User 322 = 5.056\n",
      "Mean Absolute Error for User 323 = 2.613\n",
      "Mean Absolute Error for User 324 = 5.152\n",
      "Mean Absolute Error for User 325 = 5.060\n",
      "Mean Absolute Error for User 326 = 4.345\n",
      "Mean Absolute Error for User 327 = 5.554\n",
      "Mean Absolute Error for User 328 = 4.509\n",
      "Mean Absolute Error for User 329 = 4.853\n",
      "Mean Absolute Error for User 330 = 2.037\n",
      "Mean Absolute Error for User 331 = 2.990\n",
      "Mean Absolute Error for User 332 = 3.030\n",
      "Mean Absolute Error for User 333 = 3.673\n",
      "Mean Absolute Error for User 334 = 4.934\n",
      "Mean Absolute Error for User 335 = 3.794\n",
      "Mean Absolute Error for User 336 = 2.654\n",
      "Mean Absolute Error for User 337 = 5.632\n",
      "Mean Absolute Error for User 338 = 2.836\n",
      "Mean Absolute Error for User 339 = 3.913\n",
      "Mean Absolute Error for User 340 = 3.358\n",
      "Mean Absolute Error for User 341 = 1.222\n",
      "Mean Absolute Error for User 342 = 4.272\n",
      "Mean Absolute Error for User 343 = 3.477\n",
      "Mean Absolute Error for User 344 = 4.374\n",
      "Mean Absolute Error for User 345 = 3.733\n",
      "Mean Absolute Error for User 346 = 3.933\n",
      "Mean Absolute Error for User 347 = 2.085\n",
      "Mean Absolute Error for User 348 = 3.094\n",
      "Mean Absolute Error for User 349 = 5.596\n",
      "Mean Absolute Error for User 350 = 4.401\n",
      "Mean Absolute Error for User 351 = 2.038\n",
      "Mean Absolute Error for User 352 = 2.143\n",
      "Mean Absolute Error for User 353 = 2.503\n",
      "Mean Absolute Error for User 354 = 5.181\n",
      "Mean Absolute Error for User 355 = 3.717\n",
      "Mean Absolute Error for User 356 = 5.246\n",
      "Mean Absolute Error for User 357 = 1.473\n",
      "Mean Absolute Error for User 358 = 3.614\n",
      "Mean Absolute Error for User 359 = 2.695\n",
      "Mean Absolute Error for User 360 = 4.082\n",
      "Mean Absolute Error for User 361 = 2.152\n",
      "Mean Absolute Error for User 362 = 2.943\n",
      "Mean Absolute Error for User 363 = 5.918\n",
      "Mean Absolute Error for User 364 = 4.131\n",
      "Mean Absolute Error for User 365 = 4.121\n",
      "Mean Absolute Error for User 366 = 3.926\n",
      "Mean Absolute Error for User 367 = 5.980\n",
      "Mean Absolute Error for User 368 = 2.948\n",
      "Mean Absolute Error for User 369 = 4.565\n",
      "Mean Absolute Error for User 370 = 5.948\n",
      "Mean Absolute Error for User 371 = 3.511\n",
      "Mean Absolute Error for User 372 = 4.521\n",
      "Mean Absolute Error for User 373 = 4.102\n",
      "Mean Absolute Error for User 374 = 4.377\n",
      "Mean Absolute Error for User 375 = 3.487\n",
      "Mean Absolute Error for User 376 = 2.973\n",
      "Mean Absolute Error for User 377 = 3.618\n",
      "Mean Absolute Error for User 378 = 5.185\n",
      "Mean Absolute Error for User 379 = 0.959\n",
      "Mean Absolute Error for User 380 = 2.897\n",
      "Mean Absolute Error for User 381 = 4.987\n",
      "Mean Absolute Error for User 382 = 5.353\n",
      "Mean Absolute Error for User 383 = 3.099\n",
      "Mean Absolute Error for User 384 = 2.737\n",
      "Mean Absolute Error for User 385 = 1.332\n",
      "Mean Absolute Error for User 386 = 4.707\n",
      "Mean Absolute Error for User 387 = 4.496\n",
      "Mean Absolute Error for User 388 = 3.916\n",
      "Mean Absolute Error for User 389 = 6.882\n",
      "Mean Absolute Error for User 390 = 5.792\n",
      "Mean Absolute Error for User 391 = 3.702\n",
      "Mean Absolute Error for User 392 = 6.756\n",
      "Mean Absolute Error for User 393 = 3.634\n",
      "Mean Absolute Error for User 394 = 3.656\n",
      "Mean Absolute Error for User 395 = 4.147\n",
      "Mean Absolute Error for User 396 = 2.717\n",
      "Mean Absolute Error for User 397 = 2.692\n",
      "Mean Absolute Error for User 398 = 2.503\n",
      "Mean Absolute Error for User 399 = 1.625\n",
      "Mean Absolute Error for User 400 = 1.944\n",
      "Mean Absolute Error for User 401 = 3.292\n",
      "Mean Absolute Error for User 402 = 3.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 403 = 2.183\n",
      "Mean Absolute Error for User 404 = 1.611\n",
      "Mean Absolute Error for User 405 = 4.677\n",
      "Mean Absolute Error for User 406 = 7.495\n",
      "Mean Absolute Error for User 407 = 3.133\n",
      "Mean Absolute Error for User 408 = 2.411\n",
      "Mean Absolute Error for User 409 = 3.131\n",
      "Mean Absolute Error for User 410 = 3.472\n",
      "Mean Absolute Error for User 411 = 2.986\n",
      "Mean Absolute Error for User 412 = 1.378\n",
      "Mean Absolute Error for User 413 = 3.472\n",
      "Mean Absolute Error for User 414 = 2.512\n",
      "Mean Absolute Error for User 415 = 2.732\n",
      "Mean Absolute Error for User 416 = 5.020\n",
      "Mean Absolute Error for User 417 = 4.570\n",
      "Mean Absolute Error for User 418 = 3.334\n",
      "Mean Absolute Error for User 419 = 4.782\n",
      "Mean Absolute Error for User 420 = 3.834\n",
      "Mean Absolute Error for User 421 = 2.007\n",
      "Mean Absolute Error for User 422 = 4.012\n",
      "Mean Absolute Error for User 423 = 6.471\n",
      "Mean Absolute Error for User 424 = 3.298\n",
      "Mean Absolute Error for User 425 = 4.207\n",
      "Mean Absolute Error for User 426 = 3.311\n",
      "Mean Absolute Error for User 427 = 5.924\n",
      "Mean Absolute Error for User 428 = 0.960\n",
      "Mean Absolute Error for User 429 = 3.486\n",
      "Mean Absolute Error for User 430 = 4.369\n",
      "Mean Absolute Error for User 431 = 3.313\n",
      "Mean Absolute Error for User 432 = 4.842\n",
      "Mean Absolute Error for User 433 = 3.153\n",
      "Mean Absolute Error for User 434 = 4.502\n",
      "Mean Absolute Error for User 435 = 1.748\n",
      "Mean Absolute Error for User 436 = 2.231\n",
      "Mean Absolute Error for User 437 = 3.054\n",
      "Mean Absolute Error for User 438 = 6.191\n",
      "Mean Absolute Error for User 439 = 7.285\n",
      "Mean Absolute Error for User 440 = 3.923\n",
      "Mean Absolute Error for User 441 = 3.635\n",
      "Mean Absolute Error for User 442 = 3.703\n",
      "Mean Absolute Error for User 443 = 4.640\n",
      "Mean Absolute Error for User 444 = 1.371\n",
      "Mean Absolute Error for User 445 = 3.578\n",
      "Mean Absolute Error for User 446 = 5.092\n",
      "Mean Absolute Error for User 447 = 5.484\n",
      "Mean Absolute Error for User 448 = 3.612\n",
      "Mean Absolute Error for User 449 = 2.862\n",
      "Mean Absolute Error for User 450 = 4.429\n",
      "Mean Absolute Error for User 451 = 3.010\n",
      "Mean Absolute Error for User 452 = 4.109\n",
      "Mean Absolute Error for User 453 = 5.978\n",
      "Mean Absolute Error for User 454 = 2.484\n",
      "Mean Absolute Error for User 455 = 7.637\n",
      "Mean Absolute Error for User 456 = 4.124\n",
      "Mean Absolute Error for User 457 = 3.952\n",
      "Mean Absolute Error for User 458 = 2.686\n",
      "Mean Absolute Error for User 459 = 4.140\n",
      "Mean Absolute Error for User 460 = 6.919\n",
      "Mean Absolute Error for User 461 = 2.045\n",
      "Mean Absolute Error for User 462 = 5.939\n",
      "Mean Absolute Error for User 463 = 8.442\n",
      "Mean Absolute Error for User 464 = 1.674\n",
      "Mean Absolute Error for User 465 = 4.664\n",
      "Mean Absolute Error for User 466 = 2.336\n",
      "Mean Absolute Error for User 467 = 1.637\n",
      "Mean Absolute Error for User 468 = 2.168\n",
      "Mean Absolute Error for User 469 = 3.463\n",
      "Mean Absolute Error for User 470 = 5.294\n",
      "Mean Absolute Error for User 471 = 3.430\n",
      "Mean Absolute Error for User 472 = 3.747\n",
      "Mean Absolute Error for User 473 = 2.911\n",
      "Mean Absolute Error for User 474 = 4.616\n",
      "Mean Absolute Error for User 475 = 0.979\n",
      "Mean Absolute Error for User 476 = 5.876\n",
      "Mean Absolute Error for User 477 = 3.697\n",
      "Mean Absolute Error for User 478 = 3.117\n",
      "Mean Absolute Error for User 479 = 4.390\n",
      "Mean Absolute Error for User 480 = 5.358\n",
      "Mean Absolute Error for User 481 = 1.843\n",
      "Mean Absolute Error for User 482 = 5.376\n",
      "Mean Absolute Error for User 483 = 1.050\n",
      "Mean Absolute Error for User 484 = 3.161\n",
      "Mean Absolute Error for User 485 = 1.363\n",
      "Mean Absolute Error for User 486 = 5.110\n",
      "Mean Absolute Error for User 487 = 2.797\n",
      "Mean Absolute Error for User 488 = 6.407\n",
      "Mean Absolute Error for User 489 = 2.821\n",
      "Mean Absolute Error for User 490 = 2.968\n",
      "Mean Absolute Error for User 491 = 4.981\n",
      "Mean Absolute Error for User 492 = 5.140\n",
      "Mean Absolute Error for User 493 = 3.075\n",
      "Mean Absolute Error for User 494 = 4.054\n",
      "Mean Absolute Error for User 495 = 2.935\n",
      "Mean Absolute Error for User 496 = 3.931\n",
      "Mean Absolute Error for User 497 = 5.951\n",
      "Mean Absolute Error for User 498 = 3.880\n",
      "Mean Absolute Error for User 499 = 3.512\n",
      "Mean Absolute Error for User 500 = 5.469\n",
      "Mean Absolute Error for User 501 = 3.771\n",
      "Mean Absolute Error for User 502 = 4.520\n",
      "Mean Absolute Error for User 503 = 5.215\n",
      "Mean Absolute Error for User 504 = 5.257\n",
      "Mean Absolute Error for User 505 = 2.462\n",
      "Mean Absolute Error for User 506 = 5.136\n",
      "Mean Absolute Error for User 507 = 4.303\n",
      "Mean Absolute Error for User 508 = 6.826\n",
      "Mean Absolute Error for User 509 = 2.040\n",
      "Mean Absolute Error for User 510 = 4.842\n",
      "Mean Absolute Error for User 511 = 2.413\n",
      "Mean Absolute Error for User 512 = 1.842\n",
      "Mean Absolute Error for User 513 = 7.411\n",
      "Mean Absolute Error for User 514 = 3.994\n",
      "Mean Absolute Error for User 515 = 4.155\n",
      "Mean Absolute Error for User 516 = 4.485\n",
      "Mean Absolute Error for User 517 = 5.001\n",
      "Mean Absolute Error for User 518 = 1.841\n",
      "Mean Absolute Error for User 519 = 3.566\n",
      "Mean Absolute Error for User 520 = 3.500\n",
      "Mean Absolute Error for User 521 = 4.868\n",
      "Mean Absolute Error for User 522 = 4.426\n",
      "Mean Absolute Error for User 523 = 2.352\n",
      "Mean Absolute Error for User 524 = 2.939\n",
      "Mean Absolute Error for User 525 = 1.564\n",
      "Mean Absolute Error for User 526 = 2.903\n",
      "Mean Absolute Error for User 527 = 3.542\n",
      "Mean Absolute Error for User 528 = 3.111\n",
      "Mean Absolute Error for User 529 = 4.462\n",
      "Mean Absolute Error for User 530 = 3.213\n",
      "Mean Absolute Error for User 531 = 4.808\n",
      "Mean Absolute Error for User 532 = 2.864\n",
      "Mean Absolute Error for User 533 = 4.948\n",
      "Mean Absolute Error for User 534 = 3.881\n",
      "Mean Absolute Error for User 535 = 5.874\n",
      "Mean Absolute Error for User 536 = 1.758\n",
      "Mean Absolute Error for User 537 = 2.424\n",
      "Mean Absolute Error for User 538 = 1.450\n",
      "Mean Absolute Error for User 539 = 3.484\n",
      "Mean Absolute Error for User 540 = 2.745\n",
      "Mean Absolute Error for User 541 = 6.738\n",
      "Mean Absolute Error for User 542 = 6.127\n",
      "Mean Absolute Error for User 543 = 2.339\n",
      "Mean Absolute Error for User 544 = 4.479\n",
      "Mean Absolute Error for User 545 = 2.574\n",
      "Mean Absolute Error for User 546 = 6.812\n",
      "Mean Absolute Error for User 547 = 2.519\n",
      "Mean Absolute Error for User 548 = 4.332\n",
      "Mean Absolute Error for User 549 = 2.731\n",
      "Mean Absolute Error for User 550 = 3.250\n",
      "Mean Absolute Error for User 551 = 3.846\n",
      "Mean Absolute Error for User 552 = 2.219\n",
      "Mean Absolute Error for User 553 = 3.550\n",
      "Mean Absolute Error for User 554 = 4.761\n",
      "Mean Absolute Error for User 555 = 3.474\n",
      "Mean Absolute Error for User 556 = 3.840\n",
      "Mean Absolute Error for User 557 = 1.518\n",
      "Mean Absolute Error for User 558 = 3.853\n",
      "Mean Absolute Error for User 559 = 2.546\n",
      "Mean Absolute Error for User 560 = 2.627\n",
      "Mean Absolute Error for User 561 = 3.863\n",
      "Mean Absolute Error for User 562 = 2.763\n",
      "Mean Absolute Error for User 563 = 1.893\n",
      "Mean Absolute Error for User 564 = 3.640\n",
      "Mean Absolute Error for User 565 = 3.252\n",
      "Mean Absolute Error for User 566 = 3.627\n",
      "Mean Absolute Error for User 567 = 4.627\n",
      "Mean Absolute Error for User 568 = 0.392\n",
      "Mean Absolute Error for User 569 = 2.947\n",
      "Mean Absolute Error for User 570 = 1.954\n",
      "Mean Absolute Error for User 571 = 3.214\n",
      "Mean Absolute Error for User 572 = 3.898\n",
      "Mean Absolute Error for User 573 = 3.310\n",
      "Mean Absolute Error for User 574 = 6.932\n",
      "Mean Absolute Error for User 575 = 5.116\n",
      "Mean Absolute Error for User 576 = 4.514\n",
      "Mean Absolute Error for User 577 = 3.301\n",
      "Mean Absolute Error for User 578 = 3.049\n",
      "Mean Absolute Error for User 579 = 2.672\n",
      "Mean Absolute Error for User 580 = 3.685\n",
      "Mean Absolute Error for User 581 = 3.064\n",
      "Mean Absolute Error for User 582 = 4.427\n",
      "Mean Absolute Error for User 583 = 3.569\n",
      "Mean Absolute Error for User 584 = 4.317\n",
      "Mean Absolute Error for User 585 = 2.206\n",
      "Mean Absolute Error for User 586 = 4.248\n",
      "Mean Absolute Error for User 587 = 3.888\n",
      "Mean Absolute Error for User 588 = 2.211\n",
      "Mean Absolute Error for User 589 = 0.551\n",
      "Mean Absolute Error for User 590 = 3.177\n",
      "Mean Absolute Error for User 591 = 3.546\n",
      "Mean Absolute Error for User 592 = 7.899\n",
      "Mean Absolute Error for User 593 = 3.770\n",
      "Mean Absolute Error for User 594 = 0.919\n",
      "Mean Absolute Error for User 595 = 2.000\n",
      "Mean Absolute Error for User 596 = 2.468\n",
      "Mean Absolute Error for User 597 = 8.107\n",
      "Mean Absolute Error for User 598 = 3.778\n",
      "Mean Absolute Error for User 599 = 5.854\n",
      "Mean Absolute Error for User 600 = 8.843\n",
      "Mean Absolute Error for User 601 = 3.282\n",
      "Mean Absolute Error for User 602 = 4.842\n",
      "Mean Absolute Error for User 603 = 4.372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 604 = 3.663\n",
      "Mean Absolute Error for User 605 = 3.918\n",
      "Mean Absolute Error for User 606 = 1.563\n",
      "Mean Absolute Error for User 607 = 3.788\n",
      "Mean Absolute Error for User 608 = 1.407\n",
      "Mean Absolute Error for User 609 = 1.164\n",
      "Mean Absolute Error for User 610 = 1.141\n",
      "Mean Absolute Error for User 611 = 2.711\n",
      "Mean Absolute Error for User 612 = 6.043\n",
      "Mean Absolute Error for User 613 = 4.503\n",
      "Mean Absolute Error for User 614 = 2.046\n",
      "Mean Absolute Error for User 615 = 1.907\n",
      "Mean Absolute Error for User 616 = 4.421\n",
      "Mean Absolute Error for User 617 = 3.802\n",
      "Mean Absolute Error for User 618 = 3.229\n",
      "Mean Absolute Error for User 619 = 2.411\n",
      "Mean Absolute Error for User 620 = 1.522\n",
      "Mean Absolute Error for User 621 = 2.920\n",
      "Mean Absolute Error for User 622 = 3.723\n",
      "Mean Absolute Error for User 623 = 3.245\n",
      "Mean Absolute Error for User 624 = 2.582\n",
      "Mean Absolute Error for User 625 = 3.151\n",
      "Mean Absolute Error for User 626 = 2.401\n",
      "Mean Absolute Error for User 627 = 1.488\n",
      "Mean Absolute Error for User 628 = 4.846\n",
      "Mean Absolute Error for User 629 = 4.597\n",
      "Mean Absolute Error for User 630 = 1.763\n",
      "Mean Absolute Error for User 631 = 4.804\n",
      "Mean Absolute Error for User 632 = 4.038\n",
      "Mean Absolute Error for User 633 = 5.230\n",
      "Mean Absolute Error for User 634 = 4.208\n",
      "Mean Absolute Error for User 635 = 3.879\n",
      "Mean Absolute Error for User 636 = 4.496\n",
      "Mean Absolute Error for User 637 = 6.834\n",
      "Mean Absolute Error for User 638 = 2.388\n",
      "Mean Absolute Error for User 639 = 1.624\n",
      "Mean Absolute Error for User 640 = 3.846\n",
      "Mean Absolute Error for User 641 = 3.477\n",
      "Mean Absolute Error for User 642 = 3.194\n",
      "Mean Absolute Error for User 643 = 3.086\n",
      "Mean Absolute Error for User 644 = 3.963\n",
      "Mean Absolute Error for User 645 = 3.864\n",
      "Mean Absolute Error for User 646 = 2.703\n",
      "Mean Absolute Error for User 647 = 3.504\n",
      "Mean Absolute Error for User 648 = 2.201\n",
      "Mean Absolute Error for User 649 = 2.339\n",
      "Mean Absolute Error for User 650 = 4.514\n",
      "Mean Absolute Error for User 651 = 3.105\n",
      "Mean Absolute Error for User 652 = 3.061\n",
      "Mean Absolute Error for User 653 = 4.415\n",
      "Mean Absolute Error for User 654 = 2.109\n",
      "Mean Absolute Error for User 655 = 3.800\n",
      "Mean Absolute Error for User 656 = 4.961\n",
      "Mean Absolute Error for User 657 = 2.177\n",
      "Mean Absolute Error for User 658 = 3.684\n",
      "Mean Absolute Error for User 659 = 2.768\n",
      "Mean Absolute Error for User 660 = 1.043\n",
      "Mean Absolute Error for User 661 = 2.622\n",
      "Mean Absolute Error for User 662 = 3.860\n",
      "Mean Absolute Error for User 663 = 2.302\n",
      "Mean Absolute Error for User 664 = 4.787\n",
      "Mean Absolute Error for User 665 = 5.263\n",
      "Mean Absolute Error for User 666 = 3.758\n",
      "Mean Absolute Error for User 667 = 2.468\n",
      "Mean Absolute Error for User 668 = 2.678\n",
      "Mean Absolute Error for User 669 = 4.721\n",
      "Mean Absolute Error for User 670 = 2.624\n",
      "Mean Absolute Error for User 671 = 4.594\n",
      "Mean Absolute Error for User 672 = 2.002\n",
      "Mean Absolute Error for User 673 = 3.512\n",
      "Mean Absolute Error for User 674 = 3.305\n",
      "Mean Absolute Error for User 675 = 3.890\n",
      "Mean Absolute Error for User 676 = 1.050\n",
      "Mean Absolute Error for User 677 = 2.666\n",
      "Mean Absolute Error for User 678 = 5.143\n",
      "Mean Absolute Error for User 679 = 5.967\n",
      "Mean Absolute Error for User 680 = 2.607\n",
      "Mean Absolute Error for User 681 = 4.972\n",
      "Mean Absolute Error for User 682 = 5.199\n",
      "Mean Absolute Error for User 683 = 0.741\n",
      "Mean Absolute Error for User 684 = 3.351\n",
      "Mean Absolute Error for User 685 = 7.097\n",
      "Mean Absolute Error for User 686 = 2.903\n",
      "Mean Absolute Error for User 687 = 2.591\n",
      "Mean Absolute Error for User 688 = 2.691\n",
      "Mean Absolute Error for User 689 = 3.325\n",
      "Mean Absolute Error for User 690 = 3.330\n",
      "Mean Absolute Error for User 691 = 6.842\n",
      "Mean Absolute Error for User 692 = 2.520\n",
      "Mean Absolute Error for User 693 = 2.652\n",
      "Mean Absolute Error for User 694 = 5.372\n",
      "Mean Absolute Error for User 695 = 4.622\n",
      "Mean Absolute Error for User 696 = 4.221\n",
      "Mean Absolute Error for User 697 = 3.152\n",
      "Mean Absolute Error for User 698 = 3.859\n",
      "Mean Absolute Error for User 699 = 2.987\n",
      "Mean Absolute Error for User 700 = 5.591\n",
      "Mean Absolute Error for User 701 = 3.658\n",
      "Mean Absolute Error for User 702 = 3.974\n",
      "Mean Absolute Error for User 703 = 3.951\n",
      "Mean Absolute Error for User 704 = 2.497\n",
      "Mean Absolute Error for User 705 = 3.682\n",
      "Mean Absolute Error for User 706 = 5.469\n",
      "Mean Absolute Error for User 707 = 5.982\n",
      "Mean Absolute Error for User 708 = 5.598\n",
      "Mean Absolute Error for User 709 = 3.484\n",
      "Mean Absolute Error for User 710 = 3.002\n",
      "Mean Absolute Error for User 711 = 3.806\n",
      "Mean Absolute Error for User 712 = 4.946\n",
      "Mean Absolute Error for User 713 = 5.073\n",
      "Mean Absolute Error for User 714 = 3.764\n",
      "Mean Absolute Error for User 715 = 4.089\n",
      "Mean Absolute Error for User 716 = 4.784\n",
      "Mean Absolute Error for User 717 = 2.932\n",
      "Mean Absolute Error for User 718 = 2.594\n",
      "Mean Absolute Error for User 719 = 3.985\n",
      "Mean Absolute Error for User 720 = 3.582\n",
      "Mean Absolute Error for User 721 = 3.730\n",
      "Mean Absolute Error for User 722 = 3.182\n",
      "Mean Absolute Error for User 723 = 2.258\n",
      "Mean Absolute Error for User 724 = 6.159\n",
      "Mean Absolute Error for User 725 = 4.534\n",
      "Mean Absolute Error for User 726 = 2.990\n",
      "Mean Absolute Error for User 727 = 1.580\n",
      "Mean Absolute Error for User 728 = 5.620\n",
      "Mean Absolute Error for User 729 = 2.470\n",
      "Mean Absolute Error for User 730 = 4.585\n",
      "Mean Absolute Error for User 731 = 3.890\n",
      "Mean Absolute Error for User 732 = 1.788\n",
      "Mean Absolute Error for User 733 = 0.824\n",
      "Mean Absolute Error for User 734 = 4.334\n",
      "Mean Absolute Error for User 735 = 4.266\n",
      "Mean Absolute Error for User 736 = 4.613\n",
      "Mean Absolute Error for User 737 = 4.331\n",
      "Mean Absolute Error for User 738 = 4.722\n",
      "Mean Absolute Error for User 739 = 2.876\n",
      "Mean Absolute Error for User 740 = 1.401\n",
      "Mean Absolute Error for User 741 = 2.288\n",
      "Mean Absolute Error for User 742 = 4.743\n",
      "Mean Absolute Error for User 743 = 2.666\n",
      "Mean Absolute Error for User 744 = 3.345\n",
      "Mean Absolute Error for User 745 = 5.092\n",
      "Mean Absolute Error for User 746 = 4.696\n",
      "Mean Absolute Error for User 747 = 3.735\n",
      "Mean Absolute Error for User 748 = 3.950\n",
      "Mean Absolute Error for User 749 = 5.743\n",
      "Mean Absolute Error for User 750 = 3.987\n",
      "Mean Absolute Error for User 751 = 5.008\n",
      "Mean Absolute Error for User 752 = 4.357\n",
      "Mean Absolute Error for User 753 = 4.198\n",
      "Mean Absolute Error for User 754 = 3.664\n",
      "Mean Absolute Error for User 755 = 2.632\n",
      "Mean Absolute Error for User 756 = 4.633\n",
      "Mean Absolute Error for User 757 = 5.040\n",
      "Mean Absolute Error for User 758 = 1.763\n",
      "Mean Absolute Error for User 759 = 3.946\n",
      "Mean Absolute Error for User 760 = 2.486\n",
      "Mean Absolute Error for User 761 = 3.094\n",
      "Mean Absolute Error for User 762 = 2.475\n",
      "Mean Absolute Error for User 763 = 3.928\n",
      "Mean Absolute Error for User 764 = 1.164\n",
      "Mean Absolute Error for User 765 = 6.033\n",
      "Mean Absolute Error for User 766 = 1.071\n",
      "Mean Absolute Error for User 767 = 2.131\n",
      "Mean Absolute Error for User 768 = 3.556\n",
      "Mean Absolute Error for User 769 = 4.377\n",
      "Mean Absolute Error for User 770 = 2.536\n",
      "Mean Absolute Error for User 771 = 7.216\n",
      "Mean Absolute Error for User 772 = 2.380\n",
      "Mean Absolute Error for User 773 = 2.099\n",
      "Mean Absolute Error for User 774 = 4.232\n",
      "Mean Absolute Error for User 775 = 4.813\n",
      "Mean Absolute Error for User 776 = 3.325\n",
      "Mean Absolute Error for User 777 = 2.763\n",
      "Mean Absolute Error for User 778 = 4.031\n",
      "Mean Absolute Error for User 779 = 2.839\n",
      "Mean Absolute Error for User 780 = 2.311\n",
      "Mean Absolute Error for User 781 = 4.789\n",
      "Mean Absolute Error for User 782 = 3.878\n",
      "Mean Absolute Error for User 783 = 4.714\n",
      "Mean Absolute Error for User 784 = 1.938\n",
      "Mean Absolute Error for User 785 = 2.978\n",
      "Mean Absolute Error for User 786 = 3.878\n",
      "Mean Absolute Error for User 787 = 2.553\n",
      "Mean Absolute Error for User 788 = 4.102\n",
      "Mean Absolute Error for User 789 = 3.782\n",
      "Mean Absolute Error for User 790 = 5.340\n",
      "Mean Absolute Error for User 791 = 1.546\n",
      "Mean Absolute Error for User 792 = 4.217\n",
      "Mean Absolute Error for User 793 = 2.591\n",
      "Mean Absolute Error for User 794 = 4.350\n",
      "Mean Absolute Error for User 795 = 3.681\n",
      "Mean Absolute Error for User 796 = 2.720\n",
      "Mean Absolute Error for User 797 = 3.876\n",
      "Mean Absolute Error for User 798 = 3.046\n",
      "Mean Absolute Error for User 799 = 2.499\n",
      "Mean Absolute Error for User 800 = 2.413\n",
      "Mean Absolute Error for User 801 = 3.473\n",
      "Mean Absolute Error for User 802 = 3.169\n",
      "Mean Absolute Error for User 803 = 6.047\n",
      "Mean Absolute Error for User 804 = 1.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 805 = 4.903\n",
      "Mean Absolute Error for User 806 = 5.345\n",
      "Mean Absolute Error for User 807 = 2.471\n",
      "Mean Absolute Error for User 808 = 3.536\n",
      "Mean Absolute Error for User 809 = 3.564\n",
      "Mean Absolute Error for User 810 = 3.634\n",
      "Mean Absolute Error for User 811 = 3.763\n",
      "Mean Absolute Error for User 812 = 5.292\n",
      "Mean Absolute Error for User 813 = 4.893\n",
      "Mean Absolute Error for User 814 = 2.064\n",
      "Mean Absolute Error for User 815 = 2.445\n",
      "Mean Absolute Error for User 816 = 5.783\n",
      "Mean Absolute Error for User 817 = 2.419\n",
      "Mean Absolute Error for User 818 = 4.646\n",
      "Mean Absolute Error for User 819 = 5.657\n",
      "Mean Absolute Error for User 820 = 2.591\n",
      "Mean Absolute Error for User 821 = 1.955\n",
      "Mean Absolute Error for User 822 = 2.906\n",
      "Mean Absolute Error for User 823 = 1.434\n",
      "Mean Absolute Error for User 824 = 4.162\n",
      "Mean Absolute Error for User 825 = 3.469\n",
      "Mean Absolute Error for User 826 = 4.481\n",
      "Mean Absolute Error for User 827 = 3.111\n",
      "Mean Absolute Error for User 828 = 2.073\n",
      "Mean Absolute Error for User 829 = 4.534\n",
      "Mean Absolute Error for User 830 = 3.173\n",
      "Mean Absolute Error for User 831 = 3.613\n",
      "Mean Absolute Error for User 832 = 3.731\n",
      "Mean Absolute Error for User 833 = 5.261\n",
      "Mean Absolute Error for User 834 = 1.072\n",
      "Mean Absolute Error for User 835 = 3.133\n",
      "Mean Absolute Error for User 836 = 3.873\n",
      "Mean Absolute Error for User 837 = 2.717\n",
      "Mean Absolute Error for User 838 = 4.903\n",
      "Mean Absolute Error for User 839 = 4.159\n",
      "Mean Absolute Error for User 840 = 6.209\n",
      "Mean Absolute Error for User 841 = 3.184\n",
      "Mean Absolute Error for User 842 = 4.504\n",
      "Mean Absolute Error for User 843 = 3.059\n",
      "Mean Absolute Error for User 844 = 4.723\n",
      "Mean Absolute Error for User 845 = 1.652\n",
      "Mean Absolute Error for User 846 = 2.422\n",
      "Mean Absolute Error for User 847 = 2.734\n",
      "Mean Absolute Error for User 848 = 2.500\n",
      "Mean Absolute Error for User 849 = 1.339\n",
      "Mean Absolute Error for User 850 = 3.051\n",
      "Mean Absolute Error for User 851 = 2.882\n",
      "Mean Absolute Error for User 852 = 7.508\n",
      "Mean Absolute Error for User 853 = 4.203\n",
      "Mean Absolute Error for User 854 = 4.636\n",
      "Mean Absolute Error for User 855 = 3.312\n",
      "Mean Absolute Error for User 856 = 3.352\n",
      "Mean Absolute Error for User 857 = 3.598\n",
      "Mean Absolute Error for User 858 = 4.656\n",
      "Mean Absolute Error for User 859 = 5.195\n",
      "Mean Absolute Error for User 860 = 2.533\n",
      "Mean Absolute Error for User 861 = 2.508\n",
      "Mean Absolute Error for User 862 = 2.371\n",
      "Mean Absolute Error for User 863 = 3.038\n",
      "Mean Absolute Error for User 864 = 2.594\n",
      "Mean Absolute Error for User 865 = 5.699\n",
      "Mean Absolute Error for User 866 = 3.697\n",
      "Mean Absolute Error for User 867 = 2.156\n",
      "Mean Absolute Error for User 868 = 2.677\n",
      "Mean Absolute Error for User 869 = 5.870\n",
      "Mean Absolute Error for User 870 = 4.969\n",
      "Mean Absolute Error for User 871 = 2.852\n",
      "Mean Absolute Error for User 872 = 3.764\n",
      "Mean Absolute Error for User 873 = 3.128\n",
      "Mean Absolute Error for User 874 = 2.936\n",
      "Mean Absolute Error for User 875 = 2.387\n",
      "Mean Absolute Error for User 876 = 5.907\n",
      "Mean Absolute Error for User 877 = 2.791\n",
      "Mean Absolute Error for User 878 = 1.583\n",
      "Mean Absolute Error for User 879 = 4.240\n",
      "Mean Absolute Error for User 880 = 7.000\n",
      "Mean Absolute Error for User 881 = 2.669\n",
      "Mean Absolute Error for User 882 = 2.351\n",
      "Mean Absolute Error for User 883 = 3.613\n",
      "Mean Absolute Error for User 884 = 3.842\n",
      "Mean Absolute Error for User 885 = 2.582\n",
      "Mean Absolute Error for User 886 = 5.503\n",
      "Mean Absolute Error for User 887 = 3.009\n",
      "Mean Absolute Error for User 888 = 2.819\n",
      "Mean Absolute Error for User 889 = 2.803\n",
      "Mean Absolute Error for User 890 = 5.711\n",
      "Mean Absolute Error for User 891 = 3.070\n",
      "Mean Absolute Error for User 892 = 4.570\n",
      "Mean Absolute Error for User 893 = 2.703\n",
      "Mean Absolute Error for User 894 = 3.453\n",
      "Mean Absolute Error for User 895 = 3.310\n",
      "Mean Absolute Error for User 896 = 1.586\n",
      "Mean Absolute Error for User 897 = 4.721\n",
      "Mean Absolute Error for User 898 = 1.400\n",
      "Mean Absolute Error for User 899 = 5.151\n",
      "Mean Absolute Error for User 900 = 4.077\n",
      "Mean Absolute Error for User 901 = 6.751\n",
      "Mean Absolute Error for User 902 = 3.308\n",
      "Mean Absolute Error for User 903 = 3.559\n",
      "Mean Absolute Error for User 904 = 3.349\n",
      "Mean Absolute Error for User 905 = 2.736\n",
      "Mean Absolute Error for User 906 = 2.736\n",
      "Mean Absolute Error for User 907 = 2.731\n",
      "Mean Absolute Error for User 908 = 2.385\n",
      "Mean Absolute Error for User 909 = 2.352\n",
      "Mean Absolute Error for User 910 = 2.189\n",
      "Mean Absolute Error for User 911 = 2.817\n",
      "Mean Absolute Error for User 912 = 2.122\n",
      "Mean Absolute Error for User 913 = 4.542\n",
      "Mean Absolute Error for User 914 = 5.388\n",
      "Mean Absolute Error for User 915 = 4.831\n",
      "Mean Absolute Error for User 916 = 5.980\n",
      "Mean Absolute Error for User 917 = 4.172\n",
      "Mean Absolute Error for User 918 = 1.936\n",
      "Mean Absolute Error for User 919 = 3.325\n",
      "Mean Absolute Error for User 920 = 3.616\n",
      "Mean Absolute Error for User 921 = 3.108\n",
      "Mean Absolute Error for User 922 = 6.544\n",
      "Mean Absolute Error for User 923 = 2.512\n",
      "Mean Absolute Error for User 924 = 3.940\n",
      "Mean Absolute Error for User 925 = 4.237\n",
      "Mean Absolute Error for User 926 = 2.702\n",
      "Mean Absolute Error for User 927 = 2.128\n",
      "Mean Absolute Error for User 928 = 2.953\n",
      "Mean Absolute Error for User 929 = 5.379\n",
      "Mean Absolute Error for User 930 = 4.009\n",
      "Mean Absolute Error for User 931 = 3.763\n",
      "Mean Absolute Error for User 932 = 4.986\n",
      "Mean Absolute Error for User 933 = 3.356\n",
      "Mean Absolute Error for User 934 = 2.898\n",
      "Mean Absolute Error for User 935 = 2.069\n",
      "Mean Absolute Error for User 936 = 3.458\n",
      "Mean Absolute Error for User 937 = 3.713\n",
      "Mean Absolute Error for User 938 = 1.752\n",
      "Mean Absolute Error for User 939 = 2.132\n",
      "Mean Absolute Error for User 940 = 3.377\n",
      "Mean Absolute Error for User 941 = 5.650\n",
      "Mean Absolute Error for User 942 = 3.011\n",
      "Mean Absolute Error for User 943 = 1.928\n",
      "Mean Absolute Error for User 944 = 2.206\n",
      "Mean Absolute Error for User 945 = 3.712\n",
      "Mean Absolute Error for User 946 = 6.536\n",
      "Mean Absolute Error for User 947 = 1.818\n",
      "Mean Absolute Error for User 948 = 2.615\n",
      "Mean Absolute Error for User 949 = 4.774\n",
      "Mean Absolute Error for User 950 = 4.263\n",
      "Mean Absolute Error for User 951 = 3.602\n",
      "Mean Absolute Error for User 952 = 3.198\n",
      "Mean Absolute Error for User 953 = 3.108\n",
      "Mean Absolute Error for User 954 = 2.861\n",
      "Mean Absolute Error for User 955 = 3.166\n",
      "Mean Absolute Error for User 956 = 0.588\n",
      "Mean Absolute Error for User 957 = 4.299\n",
      "Mean Absolute Error for User 958 = 4.671\n",
      "Mean Absolute Error for User 959 = 6.001\n",
      "Mean Absolute Error for User 960 = 2.479\n",
      "Mean Absolute Error for User 961 = 6.231\n",
      "Mean Absolute Error for User 962 = 5.036\n",
      "Mean Absolute Error for User 963 = 2.769\n",
      "Mean Absolute Error for User 964 = 2.276\n",
      "Mean Absolute Error for User 965 = 1.628\n",
      "Mean Absolute Error for User 966 = 3.191\n",
      "Mean Absolute Error for User 967 = 0.739\n",
      "Mean Absolute Error for User 968 = 3.532\n",
      "Mean Absolute Error for User 969 = 2.108\n",
      "Mean Absolute Error for User 970 = 4.181\n",
      "Mean Absolute Error for User 971 = 0.953\n",
      "Mean Absolute Error for User 972 = 2.153\n",
      "Mean Absolute Error for User 973 = 3.235\n",
      "Mean Absolute Error for User 974 = 4.752\n",
      "Mean Absolute Error for User 975 = 3.877\n",
      "Mean Absolute Error for User 976 = 2.410\n",
      "Mean Absolute Error for User 977 = 2.530\n",
      "Mean Absolute Error for User 978 = 2.017\n",
      "Mean Absolute Error for User 979 = 3.502\n",
      "Mean Absolute Error for User 980 = 2.109\n",
      "Mean Absolute Error for User 981 = 2.708\n",
      "Mean Absolute Error for User 982 = 4.792\n",
      "Mean Absolute Error for User 983 = 4.415\n",
      "Mean Absolute Error for User 984 = 2.875\n",
      "Mean Absolute Error for User 985 = 5.257\n",
      "Mean Absolute Error for User 986 = 1.760\n",
      "Mean Absolute Error for User 987 = 2.825\n",
      "Mean Absolute Error for User 988 = 2.507\n",
      "Mean Absolute Error for User 989 = 4.614\n",
      "Mean Absolute Error for User 990 = 3.237\n",
      "Mean Absolute Error for User 991 = 3.463\n",
      "Mean Absolute Error for User 992 = 2.359\n",
      "Mean Absolute Error for User 993 = 2.774\n",
      "Mean Absolute Error for User 994 = 2.666\n",
      "Mean Absolute Error for User 995 = 3.114\n",
      "Mean Absolute Error for User 996 = 4.347\n",
      "Mean Absolute Error for User 997 = 2.439\n",
      "Mean Absolute Error for User 998 = 1.654\n",
      "Mean Absolute Error for User 999 = 1.645\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Mean Absolute Error = 3.697\n"
     ]
    }
   ],
   "source": [
    "test(dataMat, 0.2, \"standEst\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 0 = 5.611\n",
      "Mean Absolute Error for User 1 = 4.380\n",
      "Mean Absolute Error for User 2 = 4.022\n",
      "Mean Absolute Error for User 3 = 4.333\n",
      "Mean Absolute Error for User 4 = 3.327\n",
      "Mean Absolute Error for User 5 = 2.582\n",
      "Mean Absolute Error for User 6 = 2.212\n",
      "Mean Absolute Error for User 7 = 4.454\n",
      "Mean Absolute Error for User 8 = 2.768\n",
      "Mean Absolute Error for User 9 = 1.979\n",
      "Mean Absolute Error for User 10 = 3.081\n",
      "Mean Absolute Error for User 11 = 2.498\n",
      "Mean Absolute Error for User 12 = 2.082\n",
      "Mean Absolute Error for User 13 = 4.546\n",
      "Mean Absolute Error for User 14 = 5.975\n",
      "Mean Absolute Error for User 15 = 3.332\n",
      "Mean Absolute Error for User 16 = 4.175\n",
      "Mean Absolute Error for User 17 = 2.192\n",
      "Mean Absolute Error for User 18 = 1.696\n",
      "Mean Absolute Error for User 19 = 5.833\n",
      "Mean Absolute Error for User 20 = 2.593\n",
      "Mean Absolute Error for User 21 = 2.621\n",
      "Mean Absolute Error for User 22 = 4.531\n",
      "Mean Absolute Error for User 23 = 4.372\n",
      "Mean Absolute Error for User 24 = 2.456\n",
      "Mean Absolute Error for User 25 = 2.873\n",
      "Mean Absolute Error for User 26 = 3.909\n",
      "Mean Absolute Error for User 27 = 3.686\n",
      "Mean Absolute Error for User 28 = 2.806\n",
      "Mean Absolute Error for User 29 = 3.162\n",
      "Mean Absolute Error for User 30 = 3.016\n",
      "Mean Absolute Error for User 31 = 3.324\n",
      "Mean Absolute Error for User 32 = 2.765\n",
      "Mean Absolute Error for User 33 = 2.978\n",
      "Mean Absolute Error for User 34 = 2.644\n",
      "Mean Absolute Error for User 35 = 5.815\n",
      "Mean Absolute Error for User 36 = 2.596\n",
      "Mean Absolute Error for User 37 = 1.052\n",
      "Mean Absolute Error for User 38 = 0.121\n",
      "Mean Absolute Error for User 39 = 3.507\n",
      "Mean Absolute Error for User 40 = 4.485\n",
      "Mean Absolute Error for User 41 = 3.911\n",
      "Mean Absolute Error for User 42 = 4.225\n",
      "Mean Absolute Error for User 43 = 3.809\n",
      "Mean Absolute Error for User 44 = 2.362\n",
      "Mean Absolute Error for User 45 = 5.561\n",
      "Mean Absolute Error for User 46 = 2.185\n",
      "Mean Absolute Error for User 47 = 5.238\n",
      "Mean Absolute Error for User 48 = 2.391\n",
      "Mean Absolute Error for User 49 = 5.899\n",
      "Mean Absolute Error for User 50 = 3.032\n",
      "Mean Absolute Error for User 51 = 2.705\n",
      "Mean Absolute Error for User 52 = 4.921\n",
      "Mean Absolute Error for User 53 = 5.440\n",
      "Mean Absolute Error for User 54 = 5.099\n",
      "Mean Absolute Error for User 55 = 1.611\n",
      "Mean Absolute Error for User 56 = 4.392\n",
      "Mean Absolute Error for User 57 = 1.648\n",
      "Mean Absolute Error for User 58 = 5.018\n",
      "Mean Absolute Error for User 59 = 5.719\n",
      "Mean Absolute Error for User 60 = 2.507\n",
      "Mean Absolute Error for User 61 = 3.498\n",
      "Mean Absolute Error for User 62 = 5.239\n",
      "Mean Absolute Error for User 63 = 2.628\n",
      "Mean Absolute Error for User 64 = 1.600\n",
      "Mean Absolute Error for User 65 = 5.689\n",
      "Mean Absolute Error for User 66 = 1.213\n",
      "Mean Absolute Error for User 67 = 2.076\n",
      "Mean Absolute Error for User 68 = 4.958\n",
      "Mean Absolute Error for User 69 = 1.703\n",
      "Mean Absolute Error for User 70 = 3.245\n",
      "Mean Absolute Error for User 71 = 2.829\n",
      "Mean Absolute Error for User 72 = 2.409\n",
      "Mean Absolute Error for User 73 = 3.130\n",
      "Mean Absolute Error for User 74 = 3.444\n",
      "Mean Absolute Error for User 75 = 4.012\n",
      "Mean Absolute Error for User 76 = 7.246\n",
      "Mean Absolute Error for User 77 = 3.405\n",
      "Mean Absolute Error for User 78 = 3.758\n",
      "Mean Absolute Error for User 79 = 2.210\n",
      "Mean Absolute Error for User 80 = 2.760\n",
      "Mean Absolute Error for User 81 = 5.032\n",
      "Mean Absolute Error for User 82 = 6.366\n",
      "Mean Absolute Error for User 83 = 4.089\n",
      "Mean Absolute Error for User 84 = 2.398\n",
      "Mean Absolute Error for User 85 = 3.008\n",
      "Mean Absolute Error for User 86 = 2.863\n",
      "Mean Absolute Error for User 87 = 3.650\n",
      "Mean Absolute Error for User 88 = 4.652\n",
      "Mean Absolute Error for User 89 = 4.102\n",
      "Mean Absolute Error for User 90 = 3.629\n",
      "Mean Absolute Error for User 91 = 3.790\n",
      "Mean Absolute Error for User 92 = 5.555\n",
      "Mean Absolute Error for User 93 = 2.526\n",
      "Mean Absolute Error for User 94 = 3.298\n",
      "Mean Absolute Error for User 95 = 4.413\n",
      "Mean Absolute Error for User 96 = 5.523\n",
      "Mean Absolute Error for User 97 = 7.200\n",
      "Mean Absolute Error for User 98 = 2.310\n",
      "Mean Absolute Error for User 99 = 1.708\n",
      "Mean Absolute Error for User 100 = 3.194\n",
      "Mean Absolute Error for User 101 = 3.605\n",
      "Mean Absolute Error for User 102 = 2.476\n",
      "Mean Absolute Error for User 103 = 2.316\n",
      "Mean Absolute Error for User 104 = 4.504\n",
      "Mean Absolute Error for User 105 = 5.554\n",
      "Mean Absolute Error for User 106 = 2.739\n",
      "Mean Absolute Error for User 107 = 1.226\n",
      "Mean Absolute Error for User 108 = 2.843\n",
      "Mean Absolute Error for User 109 = 2.379\n",
      "Mean Absolute Error for User 110 = 2.229\n",
      "Mean Absolute Error for User 111 = 2.203\n",
      "Mean Absolute Error for User 112 = 4.567\n",
      "Mean Absolute Error for User 113 = 5.066\n",
      "Mean Absolute Error for User 114 = 3.299\n",
      "Mean Absolute Error for User 115 = 5.168\n",
      "Mean Absolute Error for User 116 = 2.206\n",
      "Mean Absolute Error for User 117 = 2.941\n",
      "Mean Absolute Error for User 118 = 6.807\n",
      "Mean Absolute Error for User 119 = 6.734\n",
      "Mean Absolute Error for User 120 = 3.967\n",
      "Mean Absolute Error for User 121 = 1.628\n",
      "Mean Absolute Error for User 122 = 6.835\n",
      "Mean Absolute Error for User 123 = 5.484\n",
      "Mean Absolute Error for User 124 = 3.899\n",
      "Mean Absolute Error for User 125 = 4.694\n",
      "Mean Absolute Error for User 126 = 3.258\n",
      "Mean Absolute Error for User 127 = 4.065\n",
      "Mean Absolute Error for User 128 = 2.769\n",
      "Mean Absolute Error for User 129 = 4.227\n",
      "Mean Absolute Error for User 130 = 3.895\n",
      "Mean Absolute Error for User 131 = 3.016\n",
      "Mean Absolute Error for User 132 = 5.246\n",
      "Mean Absolute Error for User 133 = 5.405\n",
      "Mean Absolute Error for User 134 = 5.676\n",
      "Mean Absolute Error for User 135 = 7.412\n",
      "Mean Absolute Error for User 136 = 3.195\n",
      "Mean Absolute Error for User 137 = 4.741\n",
      "Mean Absolute Error for User 138 = 2.594\n",
      "Mean Absolute Error for User 139 = 4.812\n",
      "Mean Absolute Error for User 140 = 3.154\n",
      "Mean Absolute Error for User 141 = 5.066\n",
      "Mean Absolute Error for User 142 = 4.636\n",
      "Mean Absolute Error for User 143 = 2.301\n",
      "Mean Absolute Error for User 144 = 2.405\n",
      "Mean Absolute Error for User 145 = 3.582\n",
      "Mean Absolute Error for User 146 = 3.040\n",
      "Mean Absolute Error for User 147 = 1.631\n",
      "Mean Absolute Error for User 148 = 3.171\n",
      "Mean Absolute Error for User 149 = 4.032\n",
      "Mean Absolute Error for User 150 = 5.810\n",
      "Mean Absolute Error for User 151 = 5.833\n",
      "Mean Absolute Error for User 152 = 3.148\n",
      "Mean Absolute Error for User 153 = 4.951\n",
      "Mean Absolute Error for User 154 = 3.408\n",
      "Mean Absolute Error for User 155 = 3.505\n",
      "Mean Absolute Error for User 156 = 3.315\n",
      "Mean Absolute Error for User 157 = 3.217\n",
      "Mean Absolute Error for User 158 = 4.088\n",
      "Mean Absolute Error for User 159 = 4.700\n",
      "Mean Absolute Error for User 160 = 3.573\n",
      "Mean Absolute Error for User 161 = 2.819\n",
      "Mean Absolute Error for User 162 = 5.067\n",
      "Mean Absolute Error for User 163 = 2.359\n",
      "Mean Absolute Error for User 164 = 6.364\n",
      "Mean Absolute Error for User 165 = 4.877\n",
      "Mean Absolute Error for User 166 = 5.136\n",
      "Mean Absolute Error for User 167 = 2.781\n",
      "Mean Absolute Error for User 168 = 3.050\n",
      "Mean Absolute Error for User 169 = 4.830\n",
      "Mean Absolute Error for User 170 = 7.640\n",
      "Mean Absolute Error for User 171 = 2.250\n",
      "Mean Absolute Error for User 172 = 3.038\n",
      "Mean Absolute Error for User 173 = 1.555\n",
      "Mean Absolute Error for User 174 = 2.915\n",
      "Mean Absolute Error for User 175 = 7.316\n",
      "Mean Absolute Error for User 176 = 2.826\n",
      "Mean Absolute Error for User 177 = 4.534\n",
      "Mean Absolute Error for User 178 = 2.023\n",
      "Mean Absolute Error for User 179 = 2.794\n",
      "Mean Absolute Error for User 180 = 2.053\n",
      "Mean Absolute Error for User 181 = 4.619\n",
      "Mean Absolute Error for User 182 = 1.857\n",
      "Mean Absolute Error for User 183 = 3.821\n",
      "Mean Absolute Error for User 184 = 5.062\n",
      "Mean Absolute Error for User 185 = 4.024\n",
      "Mean Absolute Error for User 186 = 2.431\n",
      "Mean Absolute Error for User 187 = 1.899\n",
      "Mean Absolute Error for User 188 = 3.514\n",
      "Mean Absolute Error for User 189 = 3.006\n",
      "Mean Absolute Error for User 190 = 2.767\n",
      "Mean Absolute Error for User 191 = 3.171\n",
      "Mean Absolute Error for User 192 = 3.172\n",
      "Mean Absolute Error for User 193 = 5.053\n",
      "Mean Absolute Error for User 194 = 4.043\n",
      "Mean Absolute Error for User 195 = 2.371\n",
      "Mean Absolute Error for User 196 = 2.748\n",
      "Mean Absolute Error for User 197 = 2.584\n",
      "Mean Absolute Error for User 198 = 5.177\n",
      "Mean Absolute Error for User 199 = 5.094\n",
      "Mean Absolute Error for User 200 = 6.035\n",
      "Mean Absolute Error for User 201 = 5.526\n",
      "Mean Absolute Error for User 202 = 3.779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 203 = 3.715\n",
      "Mean Absolute Error for User 204 = 4.169\n",
      "Mean Absolute Error for User 205 = 4.239\n",
      "Mean Absolute Error for User 206 = 6.179\n",
      "Mean Absolute Error for User 207 = 2.197\n",
      "Mean Absolute Error for User 208 = 3.553\n",
      "Mean Absolute Error for User 209 = 2.017\n",
      "Mean Absolute Error for User 210 = 3.687\n",
      "Mean Absolute Error for User 211 = 5.255\n",
      "Mean Absolute Error for User 212 = 4.921\n",
      "Mean Absolute Error for User 213 = 2.320\n",
      "Mean Absolute Error for User 214 = 4.071\n",
      "Mean Absolute Error for User 215 = 2.097\n",
      "Mean Absolute Error for User 216 = 2.358\n",
      "Mean Absolute Error for User 217 = 4.984\n",
      "Mean Absolute Error for User 218 = 5.353\n",
      "Mean Absolute Error for User 219 = 3.398\n",
      "Mean Absolute Error for User 220 = 4.110\n",
      "Mean Absolute Error for User 221 = 1.873\n",
      "Mean Absolute Error for User 222 = 2.815\n",
      "Mean Absolute Error for User 223 = 1.317\n",
      "Mean Absolute Error for User 224 = 1.414\n",
      "Mean Absolute Error for User 225 = 6.229\n",
      "Mean Absolute Error for User 226 = 2.606\n",
      "Mean Absolute Error for User 227 = 4.047\n",
      "Mean Absolute Error for User 228 = 2.376\n",
      "Mean Absolute Error for User 229 = 5.546\n",
      "Mean Absolute Error for User 230 = 7.123\n",
      "Mean Absolute Error for User 231 = 3.467\n",
      "Mean Absolute Error for User 232 = 3.125\n",
      "Mean Absolute Error for User 233 = 1.926\n",
      "Mean Absolute Error for User 234 = 4.382\n",
      "Mean Absolute Error for User 235 = 2.138\n",
      "Mean Absolute Error for User 236 = 5.245\n",
      "Mean Absolute Error for User 237 = 4.854\n",
      "Mean Absolute Error for User 238 = 3.525\n",
      "Mean Absolute Error for User 239 = 6.950\n",
      "Mean Absolute Error for User 240 = 2.972\n",
      "Mean Absolute Error for User 241 = 1.292\n",
      "Mean Absolute Error for User 242 = 1.651\n",
      "Mean Absolute Error for User 243 = 3.571\n",
      "Mean Absolute Error for User 244 = 2.258\n",
      "Mean Absolute Error for User 245 = 3.987\n",
      "Mean Absolute Error for User 246 = 6.017\n",
      "Mean Absolute Error for User 247 = 3.557\n",
      "Mean Absolute Error for User 248 = 3.298\n",
      "Mean Absolute Error for User 249 = 2.069\n",
      "Mean Absolute Error for User 250 = 3.461\n",
      "Mean Absolute Error for User 251 = 3.660\n",
      "Mean Absolute Error for User 252 = 2.120\n",
      "Mean Absolute Error for User 253 = 2.553\n",
      "Mean Absolute Error for User 254 = 4.440\n",
      "Mean Absolute Error for User 255 = 2.530\n",
      "Mean Absolute Error for User 256 = 3.672\n",
      "Mean Absolute Error for User 257 = 4.031\n",
      "Mean Absolute Error for User 258 = 2.091\n",
      "Mean Absolute Error for User 259 = 6.041\n",
      "Mean Absolute Error for User 260 = 8.037\n",
      "Mean Absolute Error for User 261 = 5.056\n",
      "Mean Absolute Error for User 262 = 2.649\n",
      "Mean Absolute Error for User 263 = 6.471\n",
      "Mean Absolute Error for User 264 = 4.383\n",
      "Mean Absolute Error for User 265 = 3.084\n",
      "Mean Absolute Error for User 266 = 2.143\n",
      "Mean Absolute Error for User 267 = 5.471\n",
      "Mean Absolute Error for User 268 = 3.879\n",
      "Mean Absolute Error for User 269 = 5.263\n",
      "Mean Absolute Error for User 270 = 3.201\n",
      "Mean Absolute Error for User 271 = 3.433\n",
      "Mean Absolute Error for User 272 = 3.128\n",
      "Mean Absolute Error for User 273 = 2.668\n",
      "Mean Absolute Error for User 274 = 3.267\n",
      "Mean Absolute Error for User 275 = 6.280\n",
      "Mean Absolute Error for User 276 = 3.598\n",
      "Mean Absolute Error for User 277 = 2.919\n",
      "Mean Absolute Error for User 278 = 5.311\n",
      "Mean Absolute Error for User 279 = 4.171\n",
      "Mean Absolute Error for User 280 = 5.458\n",
      "Mean Absolute Error for User 281 = 3.350\n",
      "Mean Absolute Error for User 282 = 2.353\n",
      "Mean Absolute Error for User 283 = 2.921\n",
      "Mean Absolute Error for User 284 = 1.185\n",
      "Mean Absolute Error for User 285 = 4.643\n",
      "Mean Absolute Error for User 286 = 2.350\n",
      "Mean Absolute Error for User 287 = 2.604\n",
      "Mean Absolute Error for User 288 = 5.886\n",
      "Mean Absolute Error for User 289 = 3.840\n",
      "Mean Absolute Error for User 290 = 4.322\n",
      "Mean Absolute Error for User 291 = 1.850\n",
      "Mean Absolute Error for User 292 = 5.180\n",
      "Mean Absolute Error for User 293 = 1.465\n",
      "Mean Absolute Error for User 294 = 6.245\n",
      "Mean Absolute Error for User 295 = 2.274\n",
      "Mean Absolute Error for User 296 = 4.849\n",
      "Mean Absolute Error for User 297 = 4.377\n",
      "Mean Absolute Error for User 298 = 4.590\n",
      "Mean Absolute Error for User 299 = 3.234\n",
      "Mean Absolute Error for User 300 = 2.545\n",
      "Mean Absolute Error for User 301 = 2.941\n",
      "Mean Absolute Error for User 302 = 3.499\n",
      "Mean Absolute Error for User 303 = 5.854\n",
      "Mean Absolute Error for User 304 = 3.222\n",
      "Mean Absolute Error for User 305 = 3.858\n",
      "Mean Absolute Error for User 306 = 4.331\n",
      "Mean Absolute Error for User 307 = 3.605\n",
      "Mean Absolute Error for User 308 = 2.405\n",
      "Mean Absolute Error for User 309 = 3.356\n",
      "Mean Absolute Error for User 310 = 3.392\n",
      "Mean Absolute Error for User 311 = 2.457\n",
      "Mean Absolute Error for User 312 = 1.166\n",
      "Mean Absolute Error for User 313 = 3.191\n",
      "Mean Absolute Error for User 314 = 3.079\n",
      "Mean Absolute Error for User 315 = 4.181\n",
      "Mean Absolute Error for User 316 = 5.781\n",
      "Mean Absolute Error for User 317 = 0.915\n",
      "Mean Absolute Error for User 318 = 5.380\n",
      "Mean Absolute Error for User 319 = 8.366\n",
      "Mean Absolute Error for User 320 = 5.910\n",
      "Mean Absolute Error for User 321 = 2.500\n",
      "Mean Absolute Error for User 322 = 3.461\n",
      "Mean Absolute Error for User 323 = 3.313\n",
      "Mean Absolute Error for User 324 = 5.057\n",
      "Mean Absolute Error for User 325 = 4.504\n",
      "Mean Absolute Error for User 326 = 3.917\n",
      "Mean Absolute Error for User 327 = 5.148\n",
      "Mean Absolute Error for User 328 = 4.347\n",
      "Mean Absolute Error for User 329 = 3.546\n",
      "Mean Absolute Error for User 330 = 2.423\n",
      "Mean Absolute Error for User 331 = 2.064\n",
      "Mean Absolute Error for User 332 = 2.774\n",
      "Mean Absolute Error for User 333 = 3.889\n",
      "Mean Absolute Error for User 334 = 3.886\n",
      "Mean Absolute Error for User 335 = 4.270\n",
      "Mean Absolute Error for User 336 = 2.625\n",
      "Mean Absolute Error for User 337 = 5.572\n",
      "Mean Absolute Error for User 338 = 1.981\n",
      "Mean Absolute Error for User 339 = 3.449\n",
      "Mean Absolute Error for User 340 = 2.806\n",
      "Mean Absolute Error for User 341 = 1.183\n",
      "Mean Absolute Error for User 342 = 3.698\n",
      "Mean Absolute Error for User 343 = 3.305\n",
      "Mean Absolute Error for User 344 = 3.723\n",
      "Mean Absolute Error for User 345 = 4.985\n",
      "Mean Absolute Error for User 346 = 5.139\n",
      "Mean Absolute Error for User 347 = 1.727\n",
      "Mean Absolute Error for User 348 = 3.079\n",
      "Mean Absolute Error for User 349 = 5.422\n",
      "Mean Absolute Error for User 350 = 1.887\n",
      "Mean Absolute Error for User 351 = 1.478\n",
      "Mean Absolute Error for User 352 = 5.515\n",
      "Mean Absolute Error for User 353 = 3.899\n",
      "Mean Absolute Error for User 354 = 4.867\n",
      "Mean Absolute Error for User 355 = 2.832\n",
      "Mean Absolute Error for User 356 = 5.719\n",
      "Mean Absolute Error for User 357 = 1.605\n",
      "Mean Absolute Error for User 358 = 2.596\n",
      "Mean Absolute Error for User 359 = 2.834\n",
      "Mean Absolute Error for User 360 = 3.757\n",
      "Mean Absolute Error for User 361 = 2.179\n",
      "Mean Absolute Error for User 362 = 3.413\n",
      "Mean Absolute Error for User 363 = 5.298\n",
      "Mean Absolute Error for User 364 = 3.566\n",
      "Mean Absolute Error for User 365 = 3.260\n",
      "Mean Absolute Error for User 366 = 4.198\n",
      "Mean Absolute Error for User 367 = 1.904\n",
      "Mean Absolute Error for User 368 = 3.086\n",
      "Mean Absolute Error for User 369 = 3.633\n",
      "Mean Absolute Error for User 370 = 5.304\n",
      "Mean Absolute Error for User 371 = 2.946\n",
      "Mean Absolute Error for User 372 = 4.799\n",
      "Mean Absolute Error for User 373 = 3.821\n",
      "Mean Absolute Error for User 374 = 4.054\n",
      "Mean Absolute Error for User 375 = 3.932\n",
      "Mean Absolute Error for User 376 = 2.107\n",
      "Mean Absolute Error for User 377 = 3.072\n",
      "Mean Absolute Error for User 378 = 5.368\n",
      "Mean Absolute Error for User 379 = 0.982\n",
      "Mean Absolute Error for User 380 = 2.486\n",
      "Mean Absolute Error for User 381 = 6.198\n",
      "Mean Absolute Error for User 382 = 6.030\n",
      "Mean Absolute Error for User 383 = 3.481\n",
      "Mean Absolute Error for User 384 = 2.842\n",
      "Mean Absolute Error for User 385 = 2.727\n",
      "Mean Absolute Error for User 386 = 3.717\n",
      "Mean Absolute Error for User 387 = 4.262\n",
      "Mean Absolute Error for User 388 = 5.280\n",
      "Mean Absolute Error for User 389 = 5.087\n",
      "Mean Absolute Error for User 390 = 4.442\n",
      "Mean Absolute Error for User 391 = 3.687\n",
      "Mean Absolute Error for User 392 = 6.514\n",
      "Mean Absolute Error for User 393 = 3.323\n",
      "Mean Absolute Error for User 394 = 3.675\n",
      "Mean Absolute Error for User 395 = 3.344\n",
      "Mean Absolute Error for User 396 = 2.166\n",
      "Mean Absolute Error for User 397 = 2.567\n",
      "Mean Absolute Error for User 398 = 3.685\n",
      "Mean Absolute Error for User 399 = 2.201\n",
      "Mean Absolute Error for User 400 = 1.679\n",
      "Mean Absolute Error for User 401 = 2.767\n",
      "Mean Absolute Error for User 402 = 3.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 403 = 2.132\n",
      "Mean Absolute Error for User 404 = 2.017\n",
      "Mean Absolute Error for User 405 = 5.480\n",
      "Mean Absolute Error for User 406 = 6.206\n",
      "Mean Absolute Error for User 407 = 3.050\n",
      "Mean Absolute Error for User 408 = 2.211\n",
      "Mean Absolute Error for User 409 = 3.317\n",
      "Mean Absolute Error for User 410 = 4.139\n",
      "Mean Absolute Error for User 411 = 3.010\n",
      "Mean Absolute Error for User 412 = 1.914\n",
      "Mean Absolute Error for User 413 = 4.074\n",
      "Mean Absolute Error for User 414 = 3.254\n",
      "Mean Absolute Error for User 415 = 6.436\n",
      "Mean Absolute Error for User 416 = 4.688\n",
      "Mean Absolute Error for User 417 = 3.194\n",
      "Mean Absolute Error for User 418 = 4.164\n",
      "Mean Absolute Error for User 419 = 5.449\n",
      "Mean Absolute Error for User 420 = 2.890\n",
      "Mean Absolute Error for User 421 = 1.590\n",
      "Mean Absolute Error for User 422 = 2.690\n",
      "Mean Absolute Error for User 423 = 5.186\n",
      "Mean Absolute Error for User 424 = 3.626\n",
      "Mean Absolute Error for User 425 = 4.433\n",
      "Mean Absolute Error for User 426 = 3.965\n",
      "Mean Absolute Error for User 427 = 7.025\n",
      "Mean Absolute Error for User 428 = 0.898\n",
      "Mean Absolute Error for User 429 = 4.896\n",
      "Mean Absolute Error for User 430 = 5.633\n",
      "Mean Absolute Error for User 431 = 2.748\n",
      "Mean Absolute Error for User 432 = 3.434\n",
      "Mean Absolute Error for User 433 = 2.308\n",
      "Mean Absolute Error for User 434 = 4.159\n",
      "Mean Absolute Error for User 435 = 1.739\n",
      "Mean Absolute Error for User 436 = 2.362\n",
      "Mean Absolute Error for User 437 = 3.290\n",
      "Mean Absolute Error for User 438 = 6.003\n",
      "Mean Absolute Error for User 439 = 7.115\n",
      "Mean Absolute Error for User 440 = 5.056\n",
      "Mean Absolute Error for User 441 = 3.705\n",
      "Mean Absolute Error for User 442 = 1.644\n",
      "Mean Absolute Error for User 443 = 4.090\n",
      "Mean Absolute Error for User 444 = 1.916\n",
      "Mean Absolute Error for User 445 = 3.558\n",
      "Mean Absolute Error for User 446 = 5.821\n",
      "Mean Absolute Error for User 447 = 3.564\n",
      "Mean Absolute Error for User 448 = 2.592\n",
      "Mean Absolute Error for User 449 = 1.881\n",
      "Mean Absolute Error for User 450 = 2.983\n",
      "Mean Absolute Error for User 451 = 3.125\n",
      "Mean Absolute Error for User 452 = 3.237\n",
      "Mean Absolute Error for User 453 = 5.516\n",
      "Mean Absolute Error for User 454 = 1.921\n",
      "Mean Absolute Error for User 455 = 6.835\n",
      "Mean Absolute Error for User 456 = 2.954\n",
      "Mean Absolute Error for User 457 = 3.338\n",
      "Mean Absolute Error for User 458 = 5.293\n",
      "Mean Absolute Error for User 459 = 3.845\n",
      "Mean Absolute Error for User 460 = 5.118\n",
      "Mean Absolute Error for User 461 = 1.028\n",
      "Mean Absolute Error for User 462 = 6.289\n",
      "Mean Absolute Error for User 463 = 2.257\n",
      "Mean Absolute Error for User 464 = 2.045\n",
      "Mean Absolute Error for User 465 = 4.990\n",
      "Mean Absolute Error for User 466 = 2.833\n",
      "Mean Absolute Error for User 467 = 0.932\n",
      "Mean Absolute Error for User 468 = 2.785\n",
      "Mean Absolute Error for User 469 = 2.919\n",
      "Mean Absolute Error for User 470 = 7.832\n",
      "Mean Absolute Error for User 471 = 3.064\n",
      "Mean Absolute Error for User 472 = 3.673\n",
      "Mean Absolute Error for User 473 = 3.062\n",
      "Mean Absolute Error for User 474 = 5.027\n",
      "Mean Absolute Error for User 475 = 0.844\n",
      "Mean Absolute Error for User 476 = 6.247\n",
      "Mean Absolute Error for User 477 = 1.768\n",
      "Mean Absolute Error for User 478 = 2.386\n",
      "Mean Absolute Error for User 479 = 5.017\n",
      "Mean Absolute Error for User 480 = 3.850\n",
      "Mean Absolute Error for User 481 = 1.416\n",
      "Mean Absolute Error for User 482 = 3.211\n",
      "Mean Absolute Error for User 483 = 1.269\n",
      "Mean Absolute Error for User 484 = 3.398\n",
      "Mean Absolute Error for User 485 = 1.590\n",
      "Mean Absolute Error for User 486 = 5.171\n",
      "Mean Absolute Error for User 487 = 3.352\n",
      "Mean Absolute Error for User 488 = 4.827\n",
      "Mean Absolute Error for User 489 = 2.759\n",
      "Mean Absolute Error for User 490 = 3.012\n",
      "Mean Absolute Error for User 491 = 4.404\n",
      "Mean Absolute Error for User 492 = 3.899\n",
      "Mean Absolute Error for User 493 = 4.709\n",
      "Mean Absolute Error for User 494 = 3.608\n",
      "Mean Absolute Error for User 495 = 4.014\n",
      "Mean Absolute Error for User 496 = 3.052\n",
      "Mean Absolute Error for User 497 = 4.533\n",
      "Mean Absolute Error for User 498 = 3.001\n",
      "Mean Absolute Error for User 499 = 3.013\n",
      "Mean Absolute Error for User 500 = 6.520\n",
      "Mean Absolute Error for User 501 = 3.129\n",
      "Mean Absolute Error for User 502 = 5.073\n",
      "Mean Absolute Error for User 503 = 6.104\n",
      "Mean Absolute Error for User 504 = 5.237\n",
      "Mean Absolute Error for User 505 = 2.846\n",
      "Mean Absolute Error for User 506 = 4.088\n",
      "Mean Absolute Error for User 507 = 3.585\n",
      "Mean Absolute Error for User 508 = 6.385\n",
      "Mean Absolute Error for User 509 = 2.229\n",
      "Mean Absolute Error for User 510 = 3.638\n",
      "Mean Absolute Error for User 511 = 3.393\n",
      "Mean Absolute Error for User 512 = 2.050\n",
      "Mean Absolute Error for User 513 = 6.318\n",
      "Mean Absolute Error for User 514 = 3.929\n",
      "Mean Absolute Error for User 515 = 4.220\n",
      "Mean Absolute Error for User 516 = 3.531\n",
      "Mean Absolute Error for User 517 = 6.633\n",
      "Mean Absolute Error for User 518 = 1.896\n",
      "Mean Absolute Error for User 519 = 4.933\n",
      "Mean Absolute Error for User 520 = 3.238\n",
      "Mean Absolute Error for User 521 = 6.546\n",
      "Mean Absolute Error for User 522 = 4.394\n",
      "Mean Absolute Error for User 523 = 3.178\n",
      "Mean Absolute Error for User 524 = 2.722\n",
      "Mean Absolute Error for User 525 = 1.412\n",
      "Mean Absolute Error for User 526 = 3.675\n",
      "Mean Absolute Error for User 527 = 2.483\n",
      "Mean Absolute Error for User 528 = 3.503\n",
      "Mean Absolute Error for User 529 = 2.885\n",
      "Mean Absolute Error for User 530 = 2.346\n",
      "Mean Absolute Error for User 531 = 4.938\n",
      "Mean Absolute Error for User 532 = 5.218\n",
      "Mean Absolute Error for User 533 = 5.809\n",
      "Mean Absolute Error for User 534 = 3.568\n",
      "Mean Absolute Error for User 535 = 6.795\n",
      "Mean Absolute Error for User 536 = 1.717\n",
      "Mean Absolute Error for User 537 = 2.491\n",
      "Mean Absolute Error for User 538 = 1.918\n",
      "Mean Absolute Error for User 539 = 3.902\n",
      "Mean Absolute Error for User 540 = 2.794\n",
      "Mean Absolute Error for User 541 = 4.930\n",
      "Mean Absolute Error for User 542 = 4.866\n",
      "Mean Absolute Error for User 543 = 2.453\n",
      "Mean Absolute Error for User 544 = 2.410\n",
      "Mean Absolute Error for User 545 = 3.233\n",
      "Mean Absolute Error for User 546 = 6.219\n",
      "Mean Absolute Error for User 547 = 1.752\n",
      "Mean Absolute Error for User 548 = 4.297\n",
      "Mean Absolute Error for User 549 = 2.602\n",
      "Mean Absolute Error for User 550 = 2.658\n",
      "Mean Absolute Error for User 551 = 2.938\n",
      "Mean Absolute Error for User 552 = 1.825\n",
      "Mean Absolute Error for User 553 = 3.570\n",
      "Mean Absolute Error for User 554 = 4.830\n",
      "Mean Absolute Error for User 555 = 2.346\n",
      "Mean Absolute Error for User 556 = 2.881\n",
      "Mean Absolute Error for User 557 = 1.668\n",
      "Mean Absolute Error for User 558 = 2.676\n",
      "Mean Absolute Error for User 559 = 2.676\n",
      "Mean Absolute Error for User 560 = 5.741\n",
      "Mean Absolute Error for User 561 = 3.784\n",
      "Mean Absolute Error for User 562 = 3.198\n",
      "Mean Absolute Error for User 563 = 1.451\n",
      "Mean Absolute Error for User 564 = 4.963\n",
      "Mean Absolute Error for User 565 = 3.630\n",
      "Mean Absolute Error for User 566 = 1.944\n",
      "Mean Absolute Error for User 567 = 5.028\n",
      "Mean Absolute Error for User 568 = 1.152\n",
      "Mean Absolute Error for User 569 = 4.273\n",
      "Mean Absolute Error for User 570 = 2.025\n",
      "Mean Absolute Error for User 571 = 4.278\n",
      "Mean Absolute Error for User 572 = 3.752\n",
      "Mean Absolute Error for User 573 = 2.692\n",
      "Mean Absolute Error for User 574 = 5.504\n",
      "Mean Absolute Error for User 575 = 4.063\n",
      "Mean Absolute Error for User 576 = 5.837\n",
      "Mean Absolute Error for User 577 = 4.565\n",
      "Mean Absolute Error for User 578 = 4.171\n",
      "Mean Absolute Error for User 579 = 2.617\n",
      "Mean Absolute Error for User 580 = 4.054\n",
      "Mean Absolute Error for User 581 = 2.241\n",
      "Mean Absolute Error for User 582 = 3.372\n",
      "Mean Absolute Error for User 583 = 4.042\n",
      "Mean Absolute Error for User 584 = 4.695\n",
      "Mean Absolute Error for User 585 = 2.410\n",
      "Mean Absolute Error for User 586 = 2.774\n",
      "Mean Absolute Error for User 587 = 3.120\n",
      "Mean Absolute Error for User 588 = 4.272\n",
      "Mean Absolute Error for User 589 = 1.331\n",
      "Mean Absolute Error for User 590 = 4.319\n",
      "Mean Absolute Error for User 591 = 3.466\n",
      "Mean Absolute Error for User 592 = 7.318\n",
      "Mean Absolute Error for User 593 = 4.003\n",
      "Mean Absolute Error for User 594 = 0.586\n",
      "Mean Absolute Error for User 595 = 2.622\n",
      "Mean Absolute Error for User 596 = 1.541\n",
      "Mean Absolute Error for User 597 = 5.672\n",
      "Mean Absolute Error for User 598 = 2.150\n",
      "Mean Absolute Error for User 599 = 5.388\n",
      "Mean Absolute Error for User 600 = 7.958\n",
      "Mean Absolute Error for User 601 = 2.202\n",
      "Mean Absolute Error for User 602 = 3.990\n",
      "Mean Absolute Error for User 603 = 3.765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 604 = 4.928\n",
      "Mean Absolute Error for User 605 = 4.119\n",
      "Mean Absolute Error for User 606 = 1.310\n",
      "Mean Absolute Error for User 607 = 3.708\n",
      "Mean Absolute Error for User 608 = 1.893\n",
      "Mean Absolute Error for User 609 = 1.976\n",
      "Mean Absolute Error for User 610 = 1.675\n",
      "Mean Absolute Error for User 611 = 1.082\n",
      "Mean Absolute Error for User 612 = 5.609\n",
      "Mean Absolute Error for User 613 = 6.323\n",
      "Mean Absolute Error for User 614 = 2.270\n",
      "Mean Absolute Error for User 615 = 2.504\n",
      "Mean Absolute Error for User 616 = 4.614\n",
      "Mean Absolute Error for User 617 = 3.958\n",
      "Mean Absolute Error for User 618 = 4.197\n",
      "Mean Absolute Error for User 619 = 3.832\n",
      "Mean Absolute Error for User 620 = 1.630\n",
      "Mean Absolute Error for User 621 = 3.109\n",
      "Mean Absolute Error for User 622 = 3.801\n",
      "Mean Absolute Error for User 623 = 3.548\n",
      "Mean Absolute Error for User 624 = 1.984\n",
      "Mean Absolute Error for User 625 = 4.373\n",
      "Mean Absolute Error for User 626 = 4.398\n",
      "Mean Absolute Error for User 627 = 2.287\n",
      "Mean Absolute Error for User 628 = 4.127\n",
      "Mean Absolute Error for User 629 = 5.102\n",
      "Mean Absolute Error for User 630 = 1.558\n",
      "Mean Absolute Error for User 631 = 4.372\n",
      "Mean Absolute Error for User 632 = 2.756\n",
      "Mean Absolute Error for User 633 = 5.642\n",
      "Mean Absolute Error for User 634 = 5.407\n",
      "Mean Absolute Error for User 635 = 3.095\n",
      "Mean Absolute Error for User 636 = 3.899\n",
      "Mean Absolute Error for User 637 = 7.645\n",
      "Mean Absolute Error for User 638 = 2.446\n",
      "Mean Absolute Error for User 639 = 2.718\n",
      "Mean Absolute Error for User 640 = 3.032\n",
      "Mean Absolute Error for User 641 = 3.945\n",
      "Mean Absolute Error for User 642 = 6.291\n",
      "Mean Absolute Error for User 643 = 2.715\n",
      "Mean Absolute Error for User 644 = 3.470\n",
      "Mean Absolute Error for User 645 = 3.526\n",
      "Mean Absolute Error for User 646 = 2.168\n",
      "Mean Absolute Error for User 647 = 5.276\n",
      "Mean Absolute Error for User 648 = 2.780\n",
      "Mean Absolute Error for User 649 = 2.970\n",
      "Mean Absolute Error for User 650 = 4.677\n",
      "Mean Absolute Error for User 651 = 3.300\n",
      "Mean Absolute Error for User 652 = 2.692\n",
      "Mean Absolute Error for User 653 = 3.163\n",
      "Mean Absolute Error for User 654 = 2.726\n",
      "Mean Absolute Error for User 655 = 3.389\n",
      "Mean Absolute Error for User 656 = 4.491\n",
      "Mean Absolute Error for User 657 = 2.341\n",
      "Mean Absolute Error for User 658 = 6.448\n",
      "Mean Absolute Error for User 659 = 2.573\n",
      "Mean Absolute Error for User 660 = 2.609\n",
      "Mean Absolute Error for User 661 = 2.523\n",
      "Mean Absolute Error for User 662 = 5.056\n",
      "Mean Absolute Error for User 663 = 2.383\n",
      "Mean Absolute Error for User 664 = 3.986\n",
      "Mean Absolute Error for User 665 = 3.628\n",
      "Mean Absolute Error for User 666 = 3.575\n",
      "Mean Absolute Error for User 667 = 1.849\n",
      "Mean Absolute Error for User 668 = 2.806\n",
      "Mean Absolute Error for User 669 = 3.887\n",
      "Mean Absolute Error for User 670 = 2.730\n",
      "Mean Absolute Error for User 671 = 2.063\n",
      "Mean Absolute Error for User 672 = 2.447\n",
      "Mean Absolute Error for User 673 = 3.470\n",
      "Mean Absolute Error for User 674 = 3.693\n",
      "Mean Absolute Error for User 675 = 3.027\n",
      "Mean Absolute Error for User 676 = 2.834\n",
      "Mean Absolute Error for User 677 = 2.408\n",
      "Mean Absolute Error for User 678 = 4.831\n",
      "Mean Absolute Error for User 679 = 6.449\n",
      "Mean Absolute Error for User 680 = 0.862\n",
      "Mean Absolute Error for User 681 = 4.197\n",
      "Mean Absolute Error for User 682 = 6.309\n",
      "Mean Absolute Error for User 683 = 1.186\n",
      "Mean Absolute Error for User 684 = 3.445\n",
      "Mean Absolute Error for User 685 = 4.732\n",
      "Mean Absolute Error for User 686 = 3.090\n",
      "Mean Absolute Error for User 687 = 2.217\n",
      "Mean Absolute Error for User 688 = 2.250\n",
      "Mean Absolute Error for User 689 = 1.718\n",
      "Mean Absolute Error for User 690 = 3.373\n",
      "Mean Absolute Error for User 691 = 6.581\n",
      "Mean Absolute Error for User 692 = 2.428\n",
      "Mean Absolute Error for User 693 = 1.780\n",
      "Mean Absolute Error for User 694 = 4.602\n",
      "Mean Absolute Error for User 695 = 5.497\n",
      "Mean Absolute Error for User 696 = 4.946\n",
      "Mean Absolute Error for User 697 = 3.343\n",
      "Mean Absolute Error for User 698 = 4.596\n",
      "Mean Absolute Error for User 699 = 2.436\n",
      "Mean Absolute Error for User 700 = 6.527\n",
      "Mean Absolute Error for User 701 = 3.014\n",
      "Mean Absolute Error for User 702 = 3.155\n",
      "Mean Absolute Error for User 703 = 4.720\n",
      "Mean Absolute Error for User 704 = 3.097\n",
      "Mean Absolute Error for User 705 = 2.424\n",
      "Mean Absolute Error for User 706 = 7.205\n",
      "Mean Absolute Error for User 707 = 5.453\n",
      "Mean Absolute Error for User 708 = 6.222\n",
      "Mean Absolute Error for User 709 = 2.122\n",
      "Mean Absolute Error for User 710 = 3.044\n",
      "Mean Absolute Error for User 711 = 4.063\n",
      "Mean Absolute Error for User 712 = 3.213\n",
      "Mean Absolute Error for User 713 = 2.438\n",
      "Mean Absolute Error for User 714 = 2.170\n",
      "Mean Absolute Error for User 715 = 3.530\n",
      "Mean Absolute Error for User 716 = 5.391\n",
      "Mean Absolute Error for User 717 = 3.260\n",
      "Mean Absolute Error for User 718 = 2.441\n",
      "Mean Absolute Error for User 719 = 5.375\n",
      "Mean Absolute Error for User 720 = 4.336\n",
      "Mean Absolute Error for User 721 = 2.877\n",
      "Mean Absolute Error for User 722 = 2.747\n",
      "Mean Absolute Error for User 723 = 2.241\n",
      "Mean Absolute Error for User 724 = 4.756\n",
      "Mean Absolute Error for User 725 = 4.274\n",
      "Mean Absolute Error for User 726 = 3.345\n",
      "Mean Absolute Error for User 727 = 3.654\n",
      "Mean Absolute Error for User 728 = 5.557\n",
      "Mean Absolute Error for User 729 = 3.828\n",
      "Mean Absolute Error for User 730 = 3.771\n",
      "Mean Absolute Error for User 731 = 3.132\n",
      "Mean Absolute Error for User 732 = 2.038\n",
      "Mean Absolute Error for User 733 = 0.815\n",
      "Mean Absolute Error for User 734 = 3.735\n",
      "Mean Absolute Error for User 735 = 5.598\n",
      "Mean Absolute Error for User 736 = 3.230\n",
      "Mean Absolute Error for User 737 = 3.473\n",
      "Mean Absolute Error for User 738 = 4.593\n",
      "Mean Absolute Error for User 739 = 2.250\n",
      "Mean Absolute Error for User 740 = 2.266\n",
      "Mean Absolute Error for User 741 = 1.860\n",
      "Mean Absolute Error for User 742 = 4.082\n",
      "Mean Absolute Error for User 743 = 3.266\n",
      "Mean Absolute Error for User 744 = 4.954\n",
      "Mean Absolute Error for User 745 = 4.106\n",
      "Mean Absolute Error for User 746 = 4.119\n",
      "Mean Absolute Error for User 747 = 3.459\n",
      "Mean Absolute Error for User 748 = 4.176\n",
      "Mean Absolute Error for User 749 = 2.697\n",
      "Mean Absolute Error for User 750 = 4.099\n",
      "Mean Absolute Error for User 751 = 3.285\n",
      "Mean Absolute Error for User 752 = 5.258\n",
      "Mean Absolute Error for User 753 = 4.086\n",
      "Mean Absolute Error for User 754 = 2.908\n",
      "Mean Absolute Error for User 755 = 3.091\n",
      "Mean Absolute Error for User 756 = 2.523\n",
      "Mean Absolute Error for User 757 = 4.897\n",
      "Mean Absolute Error for User 758 = 1.936\n",
      "Mean Absolute Error for User 759 = 3.489\n",
      "Mean Absolute Error for User 760 = 3.321\n",
      "Mean Absolute Error for User 761 = 5.251\n",
      "Mean Absolute Error for User 762 = 4.267\n",
      "Mean Absolute Error for User 763 = 3.905\n",
      "Mean Absolute Error for User 764 = 1.959\n",
      "Mean Absolute Error for User 765 = 4.862\n",
      "Mean Absolute Error for User 766 = 1.128\n",
      "Mean Absolute Error for User 767 = 3.792\n",
      "Mean Absolute Error for User 768 = 3.783\n",
      "Mean Absolute Error for User 769 = 4.527\n",
      "Mean Absolute Error for User 770 = 2.484\n",
      "Mean Absolute Error for User 771 = 5.553\n",
      "Mean Absolute Error for User 772 = 2.229\n",
      "Mean Absolute Error for User 773 = 4.714\n",
      "Mean Absolute Error for User 774 = 3.137\n",
      "Mean Absolute Error for User 775 = 3.590\n",
      "Mean Absolute Error for User 776 = 2.775\n",
      "Mean Absolute Error for User 777 = 3.968\n",
      "Mean Absolute Error for User 778 = 3.307\n",
      "Mean Absolute Error for User 779 = 3.584\n",
      "Mean Absolute Error for User 780 = 2.263\n",
      "Mean Absolute Error for User 781 = 3.497\n",
      "Mean Absolute Error for User 782 = 2.132\n",
      "Mean Absolute Error for User 783 = 3.647\n",
      "Mean Absolute Error for User 784 = 1.680\n",
      "Mean Absolute Error for User 785 = 1.994\n",
      "Mean Absolute Error for User 786 = 3.902\n",
      "Mean Absolute Error for User 787 = 2.927\n",
      "Mean Absolute Error for User 788 = 3.101\n",
      "Mean Absolute Error for User 789 = 6.181\n",
      "Mean Absolute Error for User 790 = 3.011\n",
      "Mean Absolute Error for User 791 = 2.380\n",
      "Mean Absolute Error for User 792 = 5.586\n",
      "Mean Absolute Error for User 793 = 3.242\n",
      "Mean Absolute Error for User 794 = 4.993\n",
      "Mean Absolute Error for User 795 = 4.363\n",
      "Mean Absolute Error for User 796 = 5.110\n",
      "Mean Absolute Error for User 797 = 4.841\n",
      "Mean Absolute Error for User 798 = 4.002\n",
      "Mean Absolute Error for User 799 = 2.569\n",
      "Mean Absolute Error for User 800 = 2.626\n",
      "Mean Absolute Error for User 801 = 2.881\n",
      "Mean Absolute Error for User 802 = 2.963\n",
      "Mean Absolute Error for User 803 = 4.092\n",
      "Mean Absolute Error for User 804 = 0.776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for User 805 = 4.202\n",
      "Mean Absolute Error for User 806 = 5.035\n",
      "Mean Absolute Error for User 807 = 2.882\n",
      "Mean Absolute Error for User 808 = 3.906\n",
      "Mean Absolute Error for User 809 = 4.315\n",
      "Mean Absolute Error for User 810 = 3.875\n",
      "Mean Absolute Error for User 811 = 2.991\n",
      "Mean Absolute Error for User 812 = 5.328\n",
      "Mean Absolute Error for User 813 = 4.231\n",
      "Mean Absolute Error for User 814 = 2.050\n",
      "Mean Absolute Error for User 815 = 2.622\n",
      "Mean Absolute Error for User 816 = 4.338\n",
      "Mean Absolute Error for User 817 = 2.750\n",
      "Mean Absolute Error for User 818 = 3.275\n",
      "Mean Absolute Error for User 819 = 6.246\n",
      "Mean Absolute Error for User 820 = 1.596\n",
      "Mean Absolute Error for User 821 = 1.557\n",
      "Mean Absolute Error for User 822 = 1.899\n",
      "Mean Absolute Error for User 823 = 1.332\n",
      "Mean Absolute Error for User 824 = 2.778\n",
      "Mean Absolute Error for User 825 = 4.300\n",
      "Mean Absolute Error for User 826 = 4.771\n",
      "Mean Absolute Error for User 827 = 3.723\n",
      "Mean Absolute Error for User 828 = 2.697\n",
      "Mean Absolute Error for User 829 = 5.256\n",
      "Mean Absolute Error for User 830 = 3.744\n",
      "Mean Absolute Error for User 831 = 4.167\n",
      "Mean Absolute Error for User 832 = 2.580\n",
      "Mean Absolute Error for User 833 = 5.670\n",
      "Mean Absolute Error for User 834 = 2.278\n",
      "Mean Absolute Error for User 835 = 3.560\n",
      "Mean Absolute Error for User 836 = 4.123\n",
      "Mean Absolute Error for User 837 = 2.530\n",
      "Mean Absolute Error for User 838 = 5.347\n",
      "Mean Absolute Error for User 839 = 3.512\n",
      "Mean Absolute Error for User 840 = 4.796\n",
      "Mean Absolute Error for User 841 = 4.652\n",
      "Mean Absolute Error for User 842 = 3.123\n",
      "Mean Absolute Error for User 843 = 2.496\n",
      "Mean Absolute Error for User 844 = 4.273\n",
      "Mean Absolute Error for User 845 = 1.748\n",
      "Mean Absolute Error for User 846 = 3.543\n",
      "Mean Absolute Error for User 847 = 2.862\n",
      "Mean Absolute Error for User 848 = 3.628\n",
      "Mean Absolute Error for User 849 = 2.753\n",
      "Mean Absolute Error for User 850 = 5.522\n",
      "Mean Absolute Error for User 851 = 3.637\n",
      "Mean Absolute Error for User 852 = 7.428\n",
      "Mean Absolute Error for User 853 = 3.535\n",
      "Mean Absolute Error for User 854 = 3.569\n",
      "Mean Absolute Error for User 855 = 2.677\n",
      "Mean Absolute Error for User 856 = 2.828\n",
      "Mean Absolute Error for User 857 = 5.078\n",
      "Mean Absolute Error for User 858 = 6.050\n",
      "Mean Absolute Error for User 859 = 3.773\n",
      "Mean Absolute Error for User 860 = 2.540\n",
      "Mean Absolute Error for User 861 = 3.091\n",
      "Mean Absolute Error for User 862 = 1.679\n",
      "Mean Absolute Error for User 863 = 2.106\n",
      "Mean Absolute Error for User 864 = 3.009\n",
      "Mean Absolute Error for User 865 = 3.394\n",
      "Mean Absolute Error for User 866 = 3.303\n",
      "Mean Absolute Error for User 867 = 2.343\n",
      "Mean Absolute Error for User 868 = 2.592\n",
      "Mean Absolute Error for User 869 = 3.632\n",
      "Mean Absolute Error for User 870 = 4.861\n",
      "Mean Absolute Error for User 871 = 2.893\n",
      "Mean Absolute Error for User 872 = 1.790\n",
      "Mean Absolute Error for User 873 = 3.512\n",
      "Mean Absolute Error for User 874 = 3.734\n",
      "Mean Absolute Error for User 875 = 2.631\n",
      "Mean Absolute Error for User 876 = 4.851\n",
      "Mean Absolute Error for User 877 = 3.886\n",
      "Mean Absolute Error for User 878 = 2.566\n",
      "Mean Absolute Error for User 879 = 3.411\n",
      "Mean Absolute Error for User 880 = 6.438\n",
      "Mean Absolute Error for User 881 = 3.452\n",
      "Mean Absolute Error for User 882 = 1.303\n",
      "Mean Absolute Error for User 883 = 2.524\n",
      "Mean Absolute Error for User 884 = 4.556\n",
      "Mean Absolute Error for User 885 = 2.305\n",
      "Mean Absolute Error for User 886 = 4.207\n",
      "Mean Absolute Error for User 887 = 2.828\n",
      "Mean Absolute Error for User 888 = 3.096\n",
      "Mean Absolute Error for User 889 = 2.784\n",
      "Mean Absolute Error for User 890 = 5.813\n",
      "Mean Absolute Error for User 891 = 3.364\n",
      "Mean Absolute Error for User 892 = 2.719\n",
      "Mean Absolute Error for User 893 = 3.670\n",
      "Mean Absolute Error for User 894 = 3.700\n",
      "Mean Absolute Error for User 895 = 3.254\n",
      "Mean Absolute Error for User 896 = 2.833\n",
      "Mean Absolute Error for User 897 = 3.990\n",
      "Mean Absolute Error for User 898 = 2.469\n",
      "Mean Absolute Error for User 899 = 3.800\n",
      "Mean Absolute Error for User 900 = 4.903\n",
      "Mean Absolute Error for User 901 = 5.607\n",
      "Mean Absolute Error for User 902 = 5.029\n",
      "Mean Absolute Error for User 903 = 2.097\n",
      "Mean Absolute Error for User 904 = 4.703\n",
      "Mean Absolute Error for User 905 = 2.978\n",
      "Mean Absolute Error for User 906 = 2.282\n",
      "Mean Absolute Error for User 907 = 4.122\n",
      "Mean Absolute Error for User 908 = 1.776\n",
      "Mean Absolute Error for User 909 = 2.753\n",
      "Mean Absolute Error for User 910 = 1.836\n",
      "Mean Absolute Error for User 911 = 2.933\n",
      "Mean Absolute Error for User 912 = 2.038\n",
      "Mean Absolute Error for User 913 = 3.033\n",
      "Mean Absolute Error for User 914 = 5.642\n",
      "Mean Absolute Error for User 915 = 4.837\n",
      "Mean Absolute Error for User 916 = 4.238\n",
      "Mean Absolute Error for User 917 = 3.419\n",
      "Mean Absolute Error for User 918 = 1.644\n",
      "Mean Absolute Error for User 919 = 2.943\n",
      "Mean Absolute Error for User 920 = 2.824\n",
      "Mean Absolute Error for User 921 = 4.828\n",
      "Mean Absolute Error for User 922 = 6.549\n",
      "Mean Absolute Error for User 923 = 2.576\n",
      "Mean Absolute Error for User 924 = 4.261\n",
      "Mean Absolute Error for User 925 = 4.875\n",
      "Mean Absolute Error for User 926 = 3.031\n",
      "Mean Absolute Error for User 927 = 2.773\n",
      "Mean Absolute Error for User 928 = 1.787\n",
      "Mean Absolute Error for User 929 = 5.630\n",
      "Mean Absolute Error for User 930 = 4.145\n",
      "Mean Absolute Error for User 931 = 2.595\n",
      "Mean Absolute Error for User 932 = 3.369\n",
      "Mean Absolute Error for User 933 = 2.723\n",
      "Mean Absolute Error for User 934 = 2.481\n",
      "Mean Absolute Error for User 935 = 2.389\n",
      "Mean Absolute Error for User 936 = 2.723\n",
      "Mean Absolute Error for User 937 = 2.478\n",
      "Mean Absolute Error for User 938 = 1.410\n",
      "Mean Absolute Error for User 939 = 1.829\n",
      "Mean Absolute Error for User 940 = 3.040\n",
      "Mean Absolute Error for User 941 = 6.092\n",
      "Mean Absolute Error for User 942 = 2.933\n",
      "Mean Absolute Error for User 943 = 1.905\n",
      "Mean Absolute Error for User 944 = 3.125\n",
      "Mean Absolute Error for User 945 = 3.157\n",
      "Mean Absolute Error for User 946 = 6.560\n",
      "Mean Absolute Error for User 947 = 2.012\n",
      "Mean Absolute Error for User 948 = 2.396\n",
      "Mean Absolute Error for User 949 = 3.705\n",
      "Mean Absolute Error for User 950 = 4.681\n",
      "Mean Absolute Error for User 951 = 3.585\n",
      "Mean Absolute Error for User 952 = 4.801\n",
      "Mean Absolute Error for User 953 = 3.388\n",
      "Mean Absolute Error for User 954 = 2.153\n",
      "Mean Absolute Error for User 955 = 4.417\n",
      "Mean Absolute Error for User 956 = 0.461\n",
      "Mean Absolute Error for User 957 = 5.365\n",
      "Mean Absolute Error for User 958 = 4.836\n",
      "Mean Absolute Error for User 959 = 6.124\n",
      "Mean Absolute Error for User 960 = 2.968\n",
      "Mean Absolute Error for User 961 = 5.746\n",
      "Mean Absolute Error for User 962 = 4.225\n",
      "Mean Absolute Error for User 963 = 2.716\n",
      "Mean Absolute Error for User 964 = 3.530\n",
      "Mean Absolute Error for User 965 = 2.260\n",
      "Mean Absolute Error for User 966 = 3.846\n",
      "Mean Absolute Error for User 967 = 1.363\n",
      "Mean Absolute Error for User 968 = 4.316\n",
      "Mean Absolute Error for User 969 = 2.531\n",
      "Mean Absolute Error for User 970 = 3.255\n",
      "Mean Absolute Error for User 971 = 1.313\n",
      "Mean Absolute Error for User 972 = 2.974\n",
      "Mean Absolute Error for User 973 = 4.217\n",
      "Mean Absolute Error for User 974 = 4.023\n",
      "Mean Absolute Error for User 975 = 4.005\n",
      "Mean Absolute Error for User 976 = 3.367\n",
      "Mean Absolute Error for User 977 = 2.129\n",
      "Mean Absolute Error for User 978 = 1.982\n",
      "Mean Absolute Error for User 979 = 3.051\n",
      "Mean Absolute Error for User 980 = 1.954\n",
      "Mean Absolute Error for User 981 = 1.780\n",
      "Mean Absolute Error for User 982 = 4.357\n",
      "Mean Absolute Error for User 983 = 4.570\n",
      "Mean Absolute Error for User 984 = 2.806\n",
      "Mean Absolute Error for User 985 = 6.450\n",
      "Mean Absolute Error for User 986 = 2.423\n",
      "Mean Absolute Error for User 987 = 2.901\n",
      "Mean Absolute Error for User 988 = 2.028\n",
      "Mean Absolute Error for User 989 = 4.664\n",
      "Mean Absolute Error for User 990 = 3.039\n",
      "Mean Absolute Error for User 991 = 4.514\n",
      "Mean Absolute Error for User 992 = 3.264\n",
      "Mean Absolute Error for User 993 = 3.016\n",
      "Mean Absolute Error for User 994 = 2.893\n",
      "Mean Absolute Error for User 995 = 3.107\n",
      "Mean Absolute Error for User 996 = 3.080\n",
      "Mean Absolute Error for User 997 = 1.487\n",
      "Mean Absolute Error for User 998 = 1.930\n",
      "Mean Absolute Error for User 999 = 2.348\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Mean Absolute Error = 3.628\n"
     ]
    }
   ],
   "source": [
    "test(dataMat, 0.2, \"svdEst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a new function \"print_most_similar_jokes\" which takes the joke ratings data, a query joke id, a parameter k for the number similar jokes, and a similarity metric function, and prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings. [Note: For hints on how to accomplish this task, please see comments at the end of the provided module as well as comments for the provided stub function.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Joke number: 1 similarity: 0.7197405302080393\n",
      "The Chukcha (Russian Eskimo) phones up the Russian Parliament Building.  A guard answers. Chukcha:  \"What is required to become Parliament member?\"Guard:  \"What are you an idiot?\"Chukcha:  \"Is it required?\"\n",
      "\n",
      "\n",
      "Joke number: 2 similarity: 0.6614635821246622\n",
      "What do you get when you run over a parakeet with a lawnmower? Shredded tweet.\n",
      "\n",
      "\n",
      "Joke number: 3 similarity: 0.7879384178265028\n",
      "Did you hear that Clinton has announced there is a new national bird?  The spread eagle.\n",
      "\n",
      "\n",
      "Joke number: 4 similarity: 0.8127096111748633\n",
      "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n",
      "\n",
      "\n",
      "Joke number: 5 similarity: 0.5511943323181557\n",
      "What do you call an American in the finals of the world cup?\"Hey Beer Man!\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "rating1 = pd.read_csv(\"modified_jester_data.csv\", header=None).values\n",
    "data = rating1.T\n",
    "jokeA = np.array(joke.iloc[:,1])\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsonSim):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    knn_ = le.fit_transform(jokes)\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(dataMat, knn_)\n",
    "    f, t = knn.kneighbors([dataMat[queryJoke]], n_neighbors = k+1) \n",
    "    t = t.tolist()\n",
    "    t = [[x for x in y if x != queryJoke] for y in t]\n",
    "    g = jokes[t]\n",
    "    for i in range(len(g)):\n",
    "        print(\"\\n\")\n",
    "        print(\"Joke number:\", i+1 ,\"similarity:\" ,metric(dataMat[queryJoke], dataMat[i]))\n",
    "        print(g[i])\n",
    "print_most_similar_jokes(data,jokeA, 29, 5, metric = pearsonSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Extra Credit]: Develop your own item-based collaborative filtering recommender that uses a model-based approach (separating the training and the prediction tasks). In the training component, item-item similarities for all pairs of items are computed and stored in an appropriate data structure. Your training function should be able to use different similarity functions (passed as a parameter) including Cosine Similarity or Pearson Correlation. The prediction (or estimation) function should take as parameters a target user, an item, a value of k, and the similarities data structure and return the predicted rating on the target item for the target user. The predicted rating should be based on the weighted average of the target user's ratings on k most similar items to the target item. You should test the prediction accuracy of your estimation function (using a cross-validation similar to part b, above) and provide a plot of cross-validation accuracies across a range of values of k. Using the best value of k, demonstrate the functionality of your recommender by generating recommendations for several anecdotal users (similar to part a, above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.60</td>\n",
       "      <td>15.90</td>\n",
       "      <td>7.55</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.00</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.95</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.64</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.57</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.83</td>\n",
       "      <td>10.71</td>\n",
       "      <td>16.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>6.34</td>\n",
       "      <td>12.07</td>\n",
       "      <td>16.58</td>\n",
       "      <td>5.13</td>\n",
       "      <td>9.79</td>\n",
       "      <td>10.03</td>\n",
       "      <td>3.62</td>\n",
       "      <td>13.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.63</td>\n",
       "      <td>14.93</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>7.60</td>\n",
       "      <td>15.56</td>\n",
       "      <td>...</td>\n",
       "      <td>12.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.95</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.60</td>\n",
       "      <td>10.13</td>\n",
       "      <td>20.08</td>\n",
       "      <td>18.18</td>\n",
       "      <td>15.51</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.93</td>\n",
       "      <td>15.42</td>\n",
       "      <td>14.01</td>\n",
       "      <td>...</td>\n",
       "      <td>16.05</td>\n",
       "      <td>14.45</td>\n",
       "      <td>10.81</td>\n",
       "      <td>14.83</td>\n",
       "      <td>14.40</td>\n",
       "      <td>7.80</td>\n",
       "      <td>15.76</td>\n",
       "      <td>11.24</td>\n",
       "      <td>3.38</td>\n",
       "      <td>14.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5    6     7     8     9    ...   990   991  \\\n",
       "78 0.00 19.64  0.00  0.00  0.00  1.15 0.00  1.87 10.71  0.00  ...  0.00  0.00   \n",
       "97 0.00 11.34  0.00  0.00 17.55  1.92 0.00 10.95 10.71  0.00  ...  0.00  0.00   \n",
       "29 3.57 17.99  0.00  0.00 15.61  4.06 0.00  6.83 10.71 16.15  ...  0.00  9.59   \n",
       "84 0.00 19.98  0.00 11.63 14.93 10.71 0.00  1.10  7.60 15.56  ... 12.21  0.00   \n",
       "38 2.60 10.13 20.08 18.18 15.51  5.61 0.00  4.93 15.42 14.01  ... 16.05 14.45   \n",
       "\n",
       "     992   993   994  995   996   997  998   999  \n",
       "78  0.00 17.60 15.90 7.55  4.06  0.00 7.21  0.00  \n",
       "97  0.00  0.00 14.64 5.47  0.00  0.00 3.77  0.00  \n",
       "29  6.34 12.07 16.58 5.13  9.79 10.03 3.62 13.67  \n",
       "84  0.00  0.00 15.95 7.89  0.00  0.00 4.16  0.00  \n",
       "38 10.81 14.83 14.40 7.80 15.76 11.24 3.38 14.25  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = pd.read_csv(\"modified_jester_data.csv\", header=None)\n",
    "joke= pd.read_csv(\"jokes.csv\", header=None)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(rating.T,joke, test_size=0.2, random_state=33)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>Q: Ever wonder why the IRS calls it Form 1040?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Age and Womanhood1. Between the ages of 13 and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Q: What's the difference between a Lawyer and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>Q: How many Presidents does it take to screw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>What is the difference between men and women:A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Q: If a person who speaks three languages is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>Once upon a time two brooms fell in love and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>A radio conversation of a US naval ship with C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Q. Did you hear about the dyslexic devil worsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>What's the difference between a used tire and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1\n",
       "78  78  Q: Ever wonder why the IRS calls it Form 1040?...\n",
       "97  97  Age and Womanhood1. Between the ages of 13 and...\n",
       "29  29  Q: What's the difference between a Lawyer and ...\n",
       "84  84  Q: How many Presidents does it take to screw i...\n",
       "38  38  What is the difference between men and women:A...\n",
       "..  ..                                                ...\n",
       "18  18  Q: If a person who speaks three languages is c...\n",
       "66  66  Once upon a time two brooms fell in love and d...\n",
       "88  88  A radio conversation of a US naval ship with C...\n",
       "7    7  Q. Did you hear about the dyslexic devil worsh...\n",
       "20  20  What's the difference between a used tire and ...\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction (or estimation) function should take as parameters a target user, an item, a value of k, and the similarities data structure and return the predicted rating on the target item for the target user. The predicted rating should be based on the weighted average of the target user's ratings on k most similar items to the target item. You should test the prediction accuracy of your estimation function (using a cross-validation similar to part b, above) and provide a plot of cross-validation accuracies across a range of values of k. Using the best value of k, demonstrate the functionality of your recommender by generating recommendations for several anecdotal users (similar to part a, above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
